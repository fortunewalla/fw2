[
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#introduction",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "",
    "text": "The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)"
  },
  {
    "objectID": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "href": "posts/cs06-choiceofmovingaverageorexponential/index.html#visualization-of-raw-data",
    "title": "Choice of moving average or exponential smoothing for a particular product profile",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data\n\n\nForecasting with 5-point moving average & simple exponential smoothing\nHere we use α=1/3 for our exponential smoothing model\n\n\n\nAccuracy of the models\nTo estimate the accuracy of the models, we first compare the Mean Absolute Deviation (MAD) for each of the models.\n\n\n\n\n5MA\nSES\n\n\n\n\nMAD\n453.7429\n432.0553\n\n\n\nAlso comparing the error cumulatively gives a better picture of the accuracy of each of the models.\n\n\n\nConclusion\nFor this particular product, the forecasts from June to December show that SES is performing better than 5MA. SES outperforms 5MA for 5 months while 5MA outperforms SES for only 2 months."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html",
    "href": "posts/10-businessopportunities/index.html",
    "title": "Business opportunities from the analysis of customer data",
    "section": "",
    "text": "DATA SCIENCE SERVICES\nBusiness opportunities from the analysis of customer data\nData science in the enterprise"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#data-as-a-commodity",
    "href": "posts/10-businessopportunities/index.html#data-as-a-commodity",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.1 Data as a commodity",
    "text": "1.1 Data as a commodity\nThe whole notion of data and the use of data in organizations have changed. Especially for smaller organizations & those that are not software companies.\nThe paradigm change that has occurred is that\n\nData is now a commodity.\nValue creation from data.\nCreating new data products from existing data.\n\n\n“A data application acquires its value from the data itself, and creates more data as a result. It’s not just an application with data; it’s a data product. Data science enables the creation of data products.” 1\n\nSo far all organizations were collecting data & analyzing it to solve specific business/research problems.\n\nHere often only a part of the data was evaluated for specific business or research purposes.\nAlso people did not think about insights from the rest of the related but uncombined data?"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#what-has-caused-this-shift-in-the-value-of-data",
    "href": "posts/10-businessopportunities/index.html#what-has-caused-this-shift-in-the-value-of-data",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.2 What has caused this shift in the value of data?",
    "text": "1.2 What has caused this shift in the value of data?\n\nMy analogy would be DSP & IC revolution\nEvents after Sep-11\nS.M.A.C -&gt; Social Mobile Analytics Cloud\n\nThis is where most companies missing the paradigm & the opportunities. Most companies especially in the industrial & technical sphere feel that they are not a data company but a hardware, embedded, biomedical, engineering instrumentation company."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#data-science-a-brave-new-world-2",
    "href": "posts/10-businessopportunities/index.html#data-science-a-brave-new-world-2",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.3 Data Science: a brave new world 2",
    "text": "1.3 Data Science: a brave new world 2\n“Whether it is called data mining, predictive analytics, sense making, or knowledge discovery, the rapid development and increased availability of advanced computational techniques have changed our world in many ways.\nThere are very few, if any, electronic transactions that are not monitored, collected, aggregated, analyzed, and modeled. Data are collected about everything, from our financial activities to our shopping habits. Even casino gambling is being analyzed and modeled in an effort to characterize, predict, or modify behavior.”\n\n1.3.1 Relation between the subjects 3\n\n\n\n1.3.2 Statistical pattern recognition 4\nStatistical pattern recognition is a term used to cover all stages of an investigation from problem formulation and data collection through to discrimination and classification, assessment of results and interpretation.\nIt developed significantly in the 1960s. It was very much an interdisciplinary subject. Approaches for analyzing such data include those for signal processing, filtering, data summarization, dimension reduction, variable selection, regression and classification.\nThe large number of applications ranging from the classical ones such as automatic character recognition and medical diagnosis to the more recent ones in data mining (such as credit scoring, consumer sales analysis and credit card transaction analysis) have attracted considerable research effort.\nWithin these areas significant progress has been made. These developments include, for example, kernel-based methods (including support vector machines) and Bayesian computational methods.\nThe term machine learning describes the study of machines that can adapt to their environment and learn from example. The machine learning emphasis is perhaps more on computationally intensive methods and less on a statistical approach.\nTwo complementary approaches to discrimination\n\nA decision theory approach based on calculation of probability density functions\nThe use of Bayes theorem and a discriminant function approach. (Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership. 5)\n\nTwo approaches to classification\n\nBoth supervised (using class information to design a classifier – i.e. discrimination)\nUnsupervised (allocating to groups without class information – i.e. clustering).\n\nA practical example of pattern recognition that is familiar to many people is classifying email messages (as spam/not spam) based upon message header, content and sender.\n\n\n1.3.3 Knowledge Discovery in Databases (KDD)\n\nFrom Data to Knowledge:6\n\nIn short, KDA is a sequence of six steps, one of which is the data mining step concerned with building the data model. …from understanding of data and preprocessing to deployment of the results.\n\n\nOverview of KDD 7\nHistorically, the notion of finding useful patterns in data has been given a variety of names, including data mining, knowledge extraction, information discovery, information harvesting, data archaeology, and data pattern processing.\nThe phrase knowledge discovery in databases was coined at the first KDD workshop in 1989 (Piatetsky-Shapiro 1991) to emphasize that knowledge is the end product of a data-driven discovery.\nThe interdisciplinary nature of KDD has evolved, and continues to evolve, from the intersection of research fields such as machine learning, pattern recognition, databases, and statistics.\nData mining is the application of specific algorithms for extracting patterns from data. The datamining component of KDD currently relies heavily on known techniques from machine learning, pattern recognition, and statistics to find patterns from data in the data-mining step of the KDD process.\nThe term data mining has mostly been used by statisticians, data analysts, and the management information systems (MIS) communities. It has also gained popularity in the database field.\nBlind application of data-mining methods (rightly criticized as data dredging in the statistical literature) can be a dangerous activity, easily leading to the discovery of meaningless and invalid patterns.\nA natural question is: How is KDD different from pattern recognition or machine learning (and related fields)?\nThe answer is that these fields provide some of the data-mining methods that are used in the datamining step of the KDD process, including how the data are stored and accessed, how algorithms can be scaled to massive data sets ultimate and still run efficiently, how results can be interpreted and visualized, and how the overall man-machine interaction can usefully be modeled and supported.\nThe KDD process can be viewed as a multidisciplinary activity that encompasses techniques beyond the scope of any one particular discipline such as machine learning. Thus, for example, neural networks, although a powerful modeling tool, are relatively difficult to understand compared to decision trees.\nKDD also emphasizes scaling and robustness properties of modeling algorithms for large noisy data sets.\nKnowledge discovery from data is fundamentally a statistical endeavor. Statistics provides a language and framework for quantifying the uncertainty that results when one tries to infer general patterns from a particular sample of an overall population.\nThe term data mining has had negative connotations in statistics since the1960s when computerbased data analysis techniques were first introduced. The concern arose because if one searches long enough in any data set (even randomly generated data), one can find patterns that appear to be statistically significant but, in fact, are not.\nKDD can also be viewed as encompassing a broader view of modeling than statistics. KDD aims to provide tools to automate (to the degree possible) the entire process of data analysis and the statistician’s “art” of hypothesis selection.\nEspecially database techniques for gaining efficient data access, grouping and ordering operations when accessing data, and optimizing queries constitute the basics for scaling algorithms to larger data sets.\nMost data-mining algorithms from statistics, pattern recognition, and machine learning assume data are in the main memory and pay no attention to how the algorithm breaks down if only limited views of the data are possible.\n\n\n\n1.3.4 Data mining 8\nThe aim of data mining is to make sense of large amounts of mostly unsupervised data, in some domain.\nBusinesses are the largest group of DM users, since they routinely collect massive amounts of data and have a vested interest in making sense of the data. Their goal is to make their companies more competitive and profitable.\nIn the above definition, the first key term is to make sense, which has different meanings depending on the user’s experience. Probably the most important requirement is that the discovered new knowledge needs to be understandable to data owners who want to use it to some advantage. The most convenient outcome by far would be knowledge or a model of the data that can be described in easy-to-understand terms, say, via production rules such as:\nIF abnormality (obstruction) in coronary arteries\nTHEN coronary artery disease\nIn the example, the input data may be images of the heart and accompanying arteries. If the images are diagnosed by cardiologists as being normal or abnormal (with obstructed arteries), then such data are known as learning/training data. Some DM techniques generate models of the data in terms of production rules, and cardiologists may then analyze these and either accept or reject them (in case the rules do not agree with their domain knowledge).\nNote, however, that cardiologists may not have used, or even known, some of the rules generated by DM techniques, even if the rules are correct (as determined by cardiologists after deeper examination), or as shown by a data miner to be performing well on new unseen data, known as test data.\nWe then come to the second requirement; the generated model needs to be valid. If, in our example, all the generated rules were already known to cardiologists, these rules would be considered trivial and of no interest, although the generation of the already-known rules validates the generated models and the DM methodology. However, in the latter case, the project results would be considered a failure by the cardiologists (data owners).\nThus, we come to the third requirement associated with making sense, namely, that the discovered knowledge must be novel. Let us suppose that the new knowledge about how to diagnose a patient had been discovered not in terms of production rules but by a different type of data model, say, a neural network. In this case, the new knowledge may or may not be acceptable to the cardiologists, since a neural network is a “black box” model that, in general, cannot be understood by humans. A trained neural network, however, might still be acceptable if it were proven to work well on hundreds of new cases. To illustrate the latter case, assume that the purpose of DM was to automate the analysis (prescreening) of heart images before a cardiologist would see a patient; in that case, a neural network model would be acceptable. We thus associate with the term making sense the fourth requirement, by requesting that the discovered knowledge be useful. This usefulness must hold true regardless of the type of model used (in our example, it was rules vs. neural networks).\nThe other key term in the definition is large amounts of data. DM is not about analyzing small data sets that can be easily dealt with using many standard techniques, or even manually. To give the reader a sense of the scale of data being collected that are good candidates for DM, let us look at the following examples:\n\nAT&T handles over 300 million calls daily to serve about 100 million customers and stores the information in a multiterabyte database.\nWal-Mart, in all its stores taken together handles about 21 million transactions a day, and stores the information in a database of about a dozen terabytes.\nNASA generates several gigabytes of data per hour through its Earth Observing System.\nOil companies like Mobil Oil store hundreds of terabytes of data about different aspects of oil exploration.\nThe Sloan Digital Sky Survey project will collect observational data of about 40 terabytes.\nModern biology creates, in projects like the human genome and proteome, data measured in terabytes and petabytes. Although no data are publiclyd available,\nHomeland Security in the U.S.A. is collecting petabytes of data on its own and other countries’ citizens.\n\nIt is clear that none of the above databases can be analyzed by humans or even by the best algorithms (in terms of speed and memory requirements); these large amounts of data necessarily require the use of DM techniques to reduce the data in terms of both quantity and dimensionality.\nThe third key term in the above definition is mostly unsupervised data. It is much easier, and less expensive, to collect unsupervised data than supervised data. The reason is that with supervised data we must have known inputs corresponding to known outputs, as determined by domain experts. In our example, “input” images correspond to the “output” diagnosis of coronary artery disease (determined by cardiologists – a costly and error-prone process).\nSo what can be done if only unsupervised data are collected? To deal with the problem, one of the most difficult in DM, we need to use algorithms that are able to find “natural” groupings/clusters, relationships, and associations in the data. For example, if clusters can be found, they can possibly be labeled by domain experts. If we are able to do both, our unsupervised data becomes supervised, resulting in a much easier problem to deal with. Finding natural groupings or relationships in the data, however, is very difficult and remains an open research problem. Clustering is exacerbated by the fact that most clustering algorithms require the user a priori to specify (guess) the number of clusters in the data.\nSimilarly, the association-rule mining algorithms require the user to specify parameters that allow the generation of an appropriate number of high-quality associations. Another scenario exists when the available data are semisupervised, meaning that there are a few known training data pairs along with thousands of unsupervised data points. In our cardiology example, this situation would correspond to having thousands of images without diagnosis and only a few images that have been diagnosed. The question then becomes: Can these few data points help in the process of making sense of the entire data set?\nFortunately, there exist techniques of semi-supervised learning which take advantage of these few training data points.\nBy far the easiest scenario in DM is when all data points are fully supervised, since the majority of existing DM techniques are quite good at dealing with such data, with the possible exception of their scalability. A DM algorithm that works well on both small and large data is called scalable, but, unfortunately, few are.\nThe final key term in the definition is domain. The success of DM projects depends heavily on access to domain knowledge, and thus it is crucial for data miners to work very closely with domain experts/data owners. Discovering new knowledge from data is a process that is highly interactive (with domain experts) and iterative. We cannot simply take a successful DM system, built for some domain, and apply it to another domain and expect good results.\nAnother hundreds of available DM algorithms, such as clustering or machine learning, only small numbers of them are scalable to large data.\nHow does Data Mining Differ from Other Approaches?\nData mining came into existence in response to technological advances in many diverse disciplines. For instance, over the years computer engineering contributed significantly to the development of more powerful computers in terms of both speed and memory; computer science and mathematics continued to develop more and more efficient database architectures and search algorithms; and the combination of these disciplines helped to develop the World Wide Web.\nAll the data in the world are of no value without mechanisms to efficiently and effectively extract information and knowledge from them. Early pioneers such as U. Fayyad, H. Mannila, G. PiatetskyShapiro, G. Djorgovski, W. Frawley, P. Smith, and others recognized this urgent need, and the data mining field was born.\nData mining is not just an “umbrella” term coined for the purpose of making sense of data. The major distinguishing characteristic of DM is that it is data driven, as opposed to other methods that are often model driven.\nIn statistics, researchers frequently deal with the problem of finding the smallest data size that gives sufficiently confident estimates. In DM, we deal with the opposite problem, namely, data size is large and we are interested in building a data model that is small (not too complex) but still describes the data well.\nFinding a good model of the data, which at the same time is easy to understand, is at the heart of DM. We need to keep in mind, however, that none of the generated models will be complete (using all the relevant variables/attributes of the data), and that almost always we will look for a compromise between model completeness and model. This approach is in accordance with Occam’s razor:\n\nsimpler models are preferred over more complex ones.\n\nThe users should understand that the application of even a very good tool to one’s data will most often not result in the generation of valuable knowledge for the data owner after simply clicking “run”.\n“Since its genesis in the mid 1990s, data mining has been thought of as encompassing two tasks:\n\nusing data to test some pre-determined hypothesis, or\nusing data to determine the hypothesis in the first place.\n\nThe full automation of both these tasks – hypothesising and then testing – leads to what is known as automated discovery or machine learning.” 9\n\n\n1.3.5 Machine learning:\nThe notion of performance improving with experience is central to most, if not all, forms of machine learning. We will use the following general definition:\n\nMachine learning is the systematic study of algorithms and systems that improve their knowledge or performance with experience. 10\n\n“The goal of machine learning is to teach machines to carry out tasks by providing them with a couple of examples (how to do or not do a task) & let an algorithm come up with the best rule set. The pairs would be your training data, and the resulting rule set (also called model) could then be applied to future data that we have not yet seen.” 11\n“It’s only over the past decade or so that the inherent multi-disciplinarity of machine learning has been recognized. It merges ideas from neuroscience and biology, statistics, mathematics, and physics to make machines learn. …..\nAnother thing that has driven the change in direction in machine learning research is data mining which looks at the extraction of useful information from massive data sets, and which requires efficient algorithms, putting more of the emphasis back onto computer science” 12\n\n\n1.3.6 Predictive analytics 13\nData mining is the discovery of hidden patterns of data through machine learning — and sophisticated algorithms are the mining tools.\nPredictive analytics is the process of refining that data resource, using business knowledge to extract hidden value from those newly discovered patterns.\nData mining + business knowledge = predictive analytics =&gt; value\n\nTwo broad, identifiable branches to predictive analytics 14\n■ Decision analytics has to do with classifying (mainly) people into segments of interest to the analyst. This branch of analytics depends heavily on multivariate statistical analyses, such as cluster analysis and multidimensional scaling. Decision analytics also uses a method called logistic regression to deal with the special problems created by dependent variables that are binary or nominal, such as buys versus doesn’t buy and survives versus doesn’t survive.\n■ Predictive analytics deals with forecasting, and often employs techniques that have been used for decades. Exponential smoothing (also termed exponentially weighted moving averages or EMWA) is one such technique, as is autoregression. Box-Jenkins analysis dates to the middle of the twentieth century and comprises the moving average and regression approaches to forecasting.\nOf course, these two broad branches aren’t mutually exclusive. There’s not a clear dividing line between situations in which you would use one and not the other, although that’s often the case. But you can certainly find yourself asking questions such as these:\n\nI’ve classified my current database of prospects into likely buyers and likely non-buyers, according to demographics such as age, income, ZIP Code, and education level. Can I create a credible quarterly forecast of purchase volume if I apply the same classification criteria to a data set consisting of past prospects?\nI’ve extracted two principal components from a set of variables that measure the weekly performance of several product lines over the past two years. How do I forecast the performance of the products for the next quarter using the principal components as the outcome measures?\n\nSo, there can be overlap between decision analytics and predictive analytics. But not always— sometimes all you want to do is forecast, say, product revenue without first doing any classification or multivariate analysis. But at times you believe there’s a need to forecast the behavior of segments or of components that aren’t directly measurable. It’s in that sort of situation that the two broad branches, decision and predictive analytics, nourish one another.\n\n\nPredictive analytic techniques 15\n\nPredictive analytics is the process of using a set of sophisticated analytic tools to develop models and estimations of what the environment will do in the future. In addition to the preceding definition of predictive analytics, Gartner Research Director Gareth Herschel says this: “Predictive analytics helps connect data to effective action by drawing reliable conclusions about current conditions and future events.”\n\n\nPredictive analytics is data-driven16\nLearning how to predict from data is sometimes called machine learning—but, it turns out, this is mostly an academic term you find used within research labs, conference papers, and university courses .In commercial, industrial, and government applications—in the real-world usage of machine learning to predict—it’s called something else:\nPredictive analytics (PA)— Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\nBuilt upon computer science and statistics and bolstered by devoted conferences and university degree programs, PA has emerged as its own discipline. But, beyond a field of science, PA is a movement that exerts a forceful impact. Millions of decisions a day determine whom to call, mail, approve, test, diagnose, warn, investigate, incarcerate, set up on a date, and medicate. PA is the means to drive per-person decisions empirically, as guided by data. By answering this mountain of smaller questions, PA may in fact answer the biggest question of all:\nHow can we improve the effectiveness of all these massive functions across government, healthcare, business, nonprofit, and law enforcement work?\nIn this way, PA is a completely different animal from forecasting. Forecasting makes aggregate predictions on a macroscopic level.\nWhereas forecasting estimates the total number of ice cream cones to be purchased next month in Nebraska, predictive technology tells you which individual Nebraskans are most likely to be seen with cone in hand.\nPA leads within the growing trend to make decisions more “data driven,” relying less on one’s “gut” and more on hard, empirical evidence.\nEnter this fact-based domain and you’ll be attacked by buzzwords, including analytics, big data, business intelligence, and data science. While PA fits underneath each of these umbrellas, these evocative terms refer more to the culture and general skill sets of technologists who do an assortment of creative, innovative things with data, rather than alluding to any specific technology or method.\nThese areas are broad; in some cases, they refer simply to standard Excel reports—that is, to things that are important and require a great deal of craft, but may not rely on science or sophisticated math. And so they are more subjectively defined.\nAnother term, data mining, is often used as a synonym for PA, but, as an evocative metaphor depicting “digging around” through data in one fashion or another, it is\n\n\nManaging risk is a critical aspect of Decision Management Systems. 17\nThe first real commercial use of predictive analytics was to manage credit risk by predicting the likelihood that a consumer would miss a payment in the immediate future.\nSuddenly there is all this data about who uses what power when. Using predictive analytics to find trends, segments with specific behaviors, and to predict how people might react to specific price changes will become the norm.\nThe process of building mathematical optimization models has similarities with its predictive analytics counterpart, but there are a few major differences worth highlighting to avoid confusion:\n■ Although predictive models are generated by applying an algorithm to a data set, an optimization model is formulated by hand to represent a business problem by defining the decision variables, the objective, and the constraints.\n■ Although the scope and input to a predictive model is often relatively small (such as information about a customer), the scope of an optimization model is usually a complex transaction or a set of transactions.\n■ Predictive analytic models generally require access to large amounts of historical data that can be used to train the model. Optimization models can be run against historical data but do not require it.\n■ Although invoking a predictive model in a Decision Service is relatively fast and simple—it simply involves evaluating a formula or interpreting a decision tree—solving an optimization model can consume significant time and memory, depending on the complexity of the model and size of the data. The optimization model must search a large set of possible actions to determine the one that best fits the constraints and goals.\nOptimization is well established in supply chain problem domains where it is often used to define which products to make on which machines in a factory to maximize the value of products produced given restricted access to the various machines needed to make the products.\nSimilarly, your airplane seat, rental car, and hotel room are all likely to be priced using optimization technology.\n\n\n\nStatistical hypothesis testing 18\nA preliminary study may suggest that customers in the Northeast have a churn rate of 22.5%, whereas the nationwide average churn rate is only 15%. This may be just a chance fluctuation since the churn rate is not constant; it varies over regions and over time, so differences are to be expected. But the Northeast rate is one and a half times the U.S. average, which seems unusually high.\n\nWhat is the chance that this pattern or phenomenon is due to random variation? Statistical hypothesis testing is used to answer such questions.\n\n\n\nData science 19\nThe statistician William S. Cleveland defined data science as an interdisciplinary field larger than statistics itself.\n\nWe define data science as managing the process that can transform hypotheses and data into actionable predictions.\n\nTypical predictive analytic goals include predicting who will win an election, what products will sell well together, which loans will default, or which advertisements will be clicked on.\nThe data scientist is responsible for acquiring the data, managing the data, choosing the modeling technique, writing the code, and verifying the results.\n\nData science and its relationship to Big Data and data-driven decision making 20\n\nCompanies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz.\n\nWe can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important\n\n\nto understand its relationships to these other important and closely related concepts,\nto begin to understand what are the fundamental principles underlying data science.\n\n\nAt a high level, data science is a set of fundamental principles that support and guide the principled extraction of information and knowledge from data.\nData science is viewed as the connective tissue between data-processing technologies (including those for ‘‘big data’’) and data-driven decision making.\nProbably the broadest business applications are in marketing for tasks such as targeted marketing, online advertising, and recommendations for cross-selling. Data science also is applied for general customer relationship management to analyze customer behavior in order to manage attrition and maximize expected customer value. The finance industry uses data science for credit scoring and trading and in operations via fraud detection and workforce management\nA data-science perspective provides practitioners with structure and principles, which give the data scientist a framework to systematically treat problems of extracting useful knowledge from data.\nBacked by the trillions of bytes’ worth of shopper history that is stored in Wal-Mart’s data warehouse, she felt that the company could ‘‘start predicting what’s going to happen, when Hurricane Charley struck, several weeks earlier, instead of waiting for it to happen,’’ as she put it.\nThe New York Times reported that: ‘‘.the experts mined the data and found that the stores would indeed need certain products— and not just the usual flashlights. ‘We didn’t know in the past that strawberry Pop-Tarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,’ Ms. Dillman said in a recent interview.’ And the pre-hurricane top-selling item was beer.’’’2\nHow should MegaTelCo decide on the set of customers to target to best reduce churn for a particular incentive budget?\nOne standard deviation higher on the DDD scale is associated with a 4–6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.\nOur two example case studies illustrate two different sorts of decisions: (1) decisions for which ‘‘discoveries’’ need to be made within data, and (2) decisions that repeat, especially at massive scale, and so decision making can benefit from even small increases in accuracy based on data analysis. The Wal-Mart example above illustrates a type-1 problem. Linda Dillman would like to discover knowledge that will help Wal-Mart prepare for Hurricane Frances’s imminent arrival. Our churn example illustrates a type-2 DDD problem.\nA large telecommunications company may have hundreds of millions of customers, each a candidate for defection. Tens of millions of customers have contracts expiring each month, so each one of them has an increased likelihood of defection in the near future. If we can improve our ability to estimate, for a given customer, how profitable it would be for us to focus on her, we can potentially reap large benefits by applying this ability to the millions of customers in the population. This same logic applies to many of the areas where we have seen the most intense application of data science and data mining:\nDirect marketing, online advertising, credit scoring, financial trading, help-desk management, fraud detection, search ranking, product recommendation, and so on.\nThe use of big data technologies correlates with significant additional productivity growth. Specifically, one standard deviation higher utilization of big data technologies is associated with 1–3% higher productivity than the average firm; one standard deviation lower in terms of big data utilization is associated with 1–3% lower productivity. This leads to potentially very large productivity differences between the firms at the extremes.\nSimilarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the forefront, providing data-driven recommendations from massive data. There are other examples as well. Online advertisers must process extremely large volumes of data (billions of ad impressions per day is not unusual) and maintain a very high throughput (real-time bidding systems make decisions in tens of milliseconds).\nManagers and line employees in other functional areas will only get the best from the company’s datascience resources if they have some basic understanding of the fundamental principles.\nFacebook, Twitter, Yahoo, Google, Amazon along with many other ‘‘Digital 100’’ companies,5 have high valuations due primarily to data assets they are committed to capturing or creating.\nFundamentals concepts of data science:\n\nExtracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages. The Cross-Industry Standard Process for Data Mining (CRISP-DM)\nEvaluating data-science results requires careful consideration of the context in which they will be used: For our churn-management example, how exactly are we going to use the patterns that are extracted from historical data? More generally, does the pattern lead to better decisions than some reasonable alternative?\nThe relationship between the business problem and the analytics solution often can be decomposed into tractable subproblems via the framework of analyzing expected value. We have many specific tools for estimating probabilities and values from data. For our churn example, should the value of the customer be taken into account in addition to the likelihood of leaving?\nInformation technology can be used to find informative data items from within a large body of data: In our churn example, a quantity of interest is the likelihood that a particular customer will leave after her contract expires. Before the contract expires, this would be an unknown quantity. However, there may be known data items (usage, service history, how many friends have canceled contracts) that correlate with our quantity of interest. This fundamental concept underlies a vast number of techniques for statistical analysis, predictive modeling, and other data mining.\nEntities that are similar with respect to known features or attributes often are similar with respect to unknown features or attributes.\nIf you look too hard at a set of data, you will find something—but it might not generalize beyond the data you’re observing. This is referred to as ‘‘overfitting’’ a dataset.\nTo draw causal conclusions, one must pay very close attention to the presence of confounding factors, possibly unseen ones.\n\nFor example, it is common to see job advertisements mentioning data-mining techniques (random forests, support vector machines), specific application areas (recommendation systems, ad placement optimization), alongside popular software tools for processing big data (SQL, Hadoop, MongoDB)."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#where-is-the-concept-of-data-science-heading",
    "href": "posts/10-businessopportunities/index.html#where-is-the-concept-of-data-science-heading",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.4 Where is the concept of data science heading?",
    "text": "1.4 Where is the concept of data science heading?\n“Google is not really a search company. It’s a machine-learning company,” says Matthew Zeiler, the CEO of visual search startup Clarifai, who worked on Google Brain during a pair of internships. He says that all of Google’s most-important projects—autonomous cars, advertising, Google Maps—stand to gain from this type of research. “Everything in the company is really driven by machine learning.” 21\n\nMachine learning applications for data center efficiency optimization 22\n“The sheer number of possible equipment combinations and their setpoint values makes it difficult to determine where the optimal efficiency lies,” Gao writes in the white paper on his initiative. “In a live DC, it is possible to meet the target setpoints through many possible combinations of hardware (mechanical and electrical equipment) and software (control strategies and setpoints). Testing each and every feature combination to maximize efficiency would be unfeasible given time constraints, frequent fluctuations in the IT load and weather conditions, as well as the need to maintain a stable DC environment.”\n“Google’s Search engine has tried to approximate human intelligence by rapidly analyzing enormous amounts of data, but people like LeCun (from Facebook) aim to build massive “neutral networks” that actually mimic the way the brain works.”[^23]\n“Vern Brownell, D-Wave’s CEO, talks about the company’s ongoing efforts to prove the potential of its hardware and its plans for the future … third category is in the broad area of machine learning, which we think is one of the most exciting things going on in computer science today.”23\n“In 2000, physicist Alexei Kitaev (then at Microsoft Research) proposed that a mysterious quasiparticle known as a Majorana could be used in quantum information processing, showing that Majoranas located at opposite ends of a quantum wire could effectively create a topologically protected qubit. Five years later, Das Sarma – along with Freedman and Chetan Nayak, Station Q’s other main leader – co-authored a paper suggesting an experimental proposal for creating a topologically protected qubit using something called the “fractional quantum Hall” system (twodimensional electron gas in a strong magnetic field) along with a similar quasiparticle. These important discoveries pointed to a promising new direction for protecting qubits, and therefore getting them to behave. After all, qubits working together in harmony is fundamental to getting them to compute.” 24\n“Viv is not the only company competing for a share of those billions. The field of artificial intelligence has become the scene of a frantic corporate arms race, with Internet giants snapping up AI startups and talent. …Their goal is to build a new generation of AI that can process massive troves of data to predict and fulfill our desires.” 25\nGoogle 3D phone/Amazon 3D phone. IBM Neural chip."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#some-of-the-current-trends.",
    "href": "posts/10-businessopportunities/index.html#some-of-the-current-trends.",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.5 Some of the current trends.",
    "text": "1.5 Some of the current trends.\n\nJuly 2009 - IBM to buy SPSS(statistics software vendor) for $1.2 Billion to gain analytics\nMar 2011 - Via Science Acquires Dataspora, a pioneer in predictive analytics to leverage proprietary machine learning platform, REFS™, beyond healthcare and financial services.\nOct 2013 - SAP buys KXEN(statistics software vendor) to further predictive analytics\nFeb 2014 – Workday acquires HR predictive analytics company Identified\nMar 2014 - Dell acquires StatSoft (statistics software vendor) and the list of predictive platform vendors gets even shorter… by Simon Arkell, CEO, Predixion Software\nJun 2014 - Nokia’s HERE mapping service continued its buying spree with today’s acquisition of real-time predictive analytics firm Medio Systems\nJul 2014 - Twitter acquires image search firm Madbits (uses deep learning techniques to understand the content of an image)"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#what-would-typical-application-areas-of-data-science-be",
    "href": "posts/10-businessopportunities/index.html#what-would-typical-application-areas-of-data-science-be",
    "title": "Business opportunities from the analysis of customer data",
    "section": "1.6 What would typical application areas of data science be?",
    "text": "1.6 What would typical application areas of data science be?\n\nMicrosoft Machine Learning software\nOSISoft is working with Carnegie Mellon University on real time fault detection and the diagnosis of energy output variations across campus buildings. Machine learning is helping to mitigate issues in real time and to predictably optimize energy usage and cost.\nGitHub: sebastianbk/BreastCancerNeuralNetwork Implementation of a Neural Network in .NET using the Diagnostic Wisconsin Breast Cancer Database. After completing the example with the Breast Cancer data set by coding it myself, I thought of using Azure Machine Learning to do the same job.\nKaggle, the leading platform for predictive modeling competitions:\n\nUPenn and Mayo Clinic’s Seizure Detection Challenge: Detect seizures in intracranial EEG recordings\nThe Heritage Provider Network (HPN): The goal of the prize is to develop a predictive algorithm that can identify patients who will be admitted to a hospital within the next year, using historical claims data.\nDunnhumby, a U.K. firm that does analytics for supermarket chains, was looking to build a model to predict when supermarket shoppers will next visit the store and how much they will spend.\nBoehringerIngelheimBioSciences: Predict a biological response of molecules from their chemical properties as optimally as this data allows, relate molecular information, to an actual biological response.\nJetpac is an online travel magazine startup: Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as ‘good’.\nAllState Insurance: The goal of the Claim Prediction Challenge was to predict bodily injury liability, based solely on the characteristics of the insured vehicle.\n\n\n\nIBM Life Sciences\nRicardo Machado, (IBM, Brazil) published many papers on neural networks and a predictive expert system named Next. The power of this system stemmed from its ability to use “knowledge graphs” obtained from interviews with medical experts to form the basis of a model capable of altering these graphs when presented with data, thus transforming them into an artificial neural network. Next was successfully used to diagnose and classify kidney diseases.\nBeatriz Leao (IBM, Brazil), developed a system called HYCONES, which also combined symbolic knowledge and neural networks. It was able to successfully detect and classify congenital heart diseases. The results of the work were published in M.D. Computing in 1994.\n\n\nStanford Machine Learning Projects (CS229)\n\nCharacterizing and diagnosing hypertrophic cardiomyopathy from ECG data.\nElectrical energy modeling in Y2E2 building based on distributed sensors information.\nPredicting semantic features from CT images of liver lesions using deep learning.\nMachine learning classification of kidney and lung cancer types.\nGaussian process based image segmentation and object detection in pathology slides.\nListen to your heart: stress prediction using consumer heart rate sensors."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#top-ten-algorithms-in-data-mining-27",
    "href": "posts/10-businessopportunities/index.html#top-ten-algorithms-in-data-mining-27",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.1 Top ten algorithms in data mining 26",
    "text": "2.1 Top ten algorithms in data mining 26\n\nC4.5\nK-Means\nSVM: Support Vector Machines\nApriori\nExpecation Maximization\nPageRank\nAdaBoost\nkNN: k-Nearest Neighbors.\nNaïve Bayes\nCART: Classification and Regression Trees"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#microsoft-sql-server-analysis-services-ssas-algorithms",
    "href": "posts/10-businessopportunities/index.html#microsoft-sql-server-analysis-services-ssas-algorithms",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms",
    "text": "2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms\n\n\n\n\n\n\n\nExamples of tasks\nMicrosoft algorithms to use\n\n\n\n\nPredicting a discrete attribute - Flag the customers in a prospective buyers list as good or poor prospects. - Calculate the probability that a server will fail within the next 6 months.- Categorize patient outcomes and explore related factors.\n* Decision Trees Algorithm * Naive Bayes Algorithm * Clustering Algorithm * Neural Network Algorithm\n\n\nPredicting a continuous attribute - Forecast next year’s sales. - Predict site visitors given past historical and seasonal trends. - Generate a risk score given demographics.\n* Decision Trees Algorithm * Time Series Algorithm * Linear Regression Algorithm\n\n\nPredicting a sequence - Perform clickstream analysis of a company’s Web site. - Analyze the factors leading to server failure. - Capture and analyze sequences of activities during outpatient visits, to formulate best practices around common activities.\n* Sequence Clustering Algorithm\n\n\nFinding groups of common items in transactions - Use market basket analysis to determine product placement. - Suggest additional products to a customer for purchase. - Analyze survey data from visitors to an event, to find which activities or booths were correlated, to plan future activities.\n* Association Algorithm * Decision Trees Algorithm\n\n\nFinding groups of similar items - Create patient risk profiles groups based on attributes such as demographics and behaviors. - Analyze users by browsing and buying patterns. - Identify servers that have similar usage characteristics.\n* Clustering Algorithm * Sequence Clustering Algorithm"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#madlib-algorithms",
    "href": "posts/10-businessopportunities/index.html#madlib-algorithms",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.3 MADlib algorithms",
    "text": "2.3 MADlib algorithms\nKey philosophies driving the architecture of MADlib are:\n\nOperate on the data locally—in database. Do not move it between multiple runtime environments unnecessarily.\nUtilize best of breed database engines, but separate the machine learning logic from database specific implementation details.\nLeverage MPP Share nothing technology, such as the Pivotal Greenplum Database, to provide parallelism and scalability.\nOpen implementation maintaining active ties into ongoing academic research.\n\n\n\n\n\n\n\n\nTechnique\nExample Use Cases\n\n\n\n\nLinear Regression\ncan be used to model a linear relationship of a scalar dependent variable to one or more explanatory independent variables.\n\n\nLatent Dirichlet Allocation\nis a topic modeling function used to identify recurring themes in a large document corpus.\n\n\nSummary Function\nprovides summary statistics for any data table. These statistics include statistics such as: number of distinct values, number of missing values, mean, variance, min, max, most frequent values, quantiles, etc.\n\n\nLogistic Regression\ncan be used to predict a binary outcome of a dependent variable from one or more explanatory independent variables.\n\n\nElastic Net Regularization\nis a regularization technique that can be implemented for either linear or logistic regression to help build a more robust model in the event of large numbers of explanatory independent variables.\n\n\nPrincipal Component Analysis\nis a dimensional reduction technique that can be used to transform a high dimensional space into a lower dimensional space.\n\n\nApriori\nis a technique for evaluating frequent item-sets, which allows analysis of what events tend to occur together. For instance what items customers frequently purchase in a single transaction.\n\n\nk-Means Clustering\nis a clustering method used to identify regions of similarity within a dataset. It can be used for many types of analysis including customer segmentation analysis."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#oracle-advanced-analytics",
    "href": "posts/10-businessopportunities/index.html#oracle-advanced-analytics",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.4 Oracle advanced analytics",
    "text": "2.4 Oracle advanced analytics\n\n\n\nTechnique\nApplicability\nAlgorithms\n\n\n\n\n./media/image9.jpg Classification\nMost commonly used technique for predicting a specific outcome such as response / no-response, high / medium / low-value customer, likely to buy / not buy.\n* Logistic Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Naive Bayes —Fast, simple, commonly applicable * Support Vector Machine—Next generation, supports text and wide data. Decision Tree —Popular, provides human-readable rules.\n\n\n./media/image10.jpg Regression\nTechnique for predicting a continuous numerical outcome such as customer lifetime value, house value, process yield rates.\n* Multiple Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Support Vector Machine\n\n\n./media/image11.jpg Attribute Importance\nRanks attributes according to strength of relationship with target attribute. Use cases include finding factors most associated with customers who respond to an offer, factors most associated with healthy patients.\n* Minimum Description – Considers each attribute as a simple predictive model of the target classLength—\n\n\n./media/image12.jpg Anomaly Detection\nIdentifies unusual or suspicious cases based on deviation from the norm. Common examples include health care fraud, expense report fraud, and tax compliance.\n* One-Class Support Vector Machine — Trains on “normal” cases to flag unusual cases\n\n\n./media/image13.jpg Clustering\nUseful for exploring data and finding natural groupings. Members of a cluster are more like each other than they are like members of a different cluster. Common examples include finding new customer segments, and life sciences discovery.\n* Enhanced K-Means—Supports text mining, hierarchical clustering, distance based * Orthogonal Partitioning Clustering— Hierarchical clustering, density based. * Expectation Maximization—Clustering technique that performs well in mixed data (dense and sparse) data mining problems.\n\n\n./media/image14.jpg Association\nFinds rules associated with frequently co-occurring items, used for market basket analysis, cross-sell, root cause analysis. Useful for product bundling, instore placement, and defect analysis.\n* Apriori—Industry standard for market basket analysis\n\n\n./media/image15.jpg Feature Selection and Extraction\nProduces new attributes as linear combination of existing attributes. Applicable for text data, latent semantic analysis, data compression, data decomposition and projection, and pattern recognition.\nNon-negative Matrix Factorization— Next generation, maps the original data into the new set of attributes. * Principal Components Analysis (PCA)— creates new fewer composite attributes that represent all the attributes. * Singular Vector Decomposition— established feature extraction method that has a wide range of applications."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#apache-mahout-for-hadoop",
    "href": "posts/10-businessopportunities/index.html#apache-mahout-for-hadoop",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.5 Apache Mahout for Hadoop",
    "text": "2.5 Apache Mahout for Hadoop\nThe Apache Mahout project’s goal is to build a scalable machine learning library. There is some degree of overlap with big data analytics within a Hadoop\nMahout includes algorithms for clustering, classification and collaborative filtering. You can also find:\n\nMatrix factorization based recommenders\nK-Means, Fuzzy K-Means clustering\nLatent Dirichlet Allocation\nSingular Value Decomposition\nLogistic regression classifier\n(Complementary) Naive Bayes classifier\nRandom forest classifier"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#microsoft-azure-cloud-machine-learning-studio",
    "href": "posts/10-businessopportunities/index.html#microsoft-azure-cloud-machine-learning-studio",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.6 Microsoft Azure cloud machine learning studio",
    "text": "2.6 Microsoft Azure cloud machine learning studio\nThere is a pool of VMs running machine learning algorithms using an orchestration engine, freeing the data scientist from moving data and moving to different services.\nThe ML Studio is targeting the emerging data scientists. You can train 10 models in minutes, not days. You can put a predictive model into production in minutes, not weeks or months. Some customers are reporting a 10X-100X in reduction in cost relative to SAS.\nEmployees can create their own workspaces, giving re-use and cross-teaming and sharing models with others.\nThe predictive models can be shared as a service across an enterprise leverage Azure as the public cloud back-end. For example, you can write JSON-based back ends that leverage your predictive models, allowing you to build decision making dashboards for your business.\nMachine Learning algorithms are built to continually improve over time by leverage training sets. Training sets make it possible to continually improve the robustness of your predictive model.\nThe good news is that R is easily integrated into ML Studio. Right now, R is dominant in machine learning space."
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#google-prediction-api",
    "href": "posts/10-businessopportunities/index.html#google-prediction-api",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.7 Google Prediction API",
    "text": "2.7 Google Prediction API\nGoogle’s cloud-based machine learning tools can help analyze your data to add the following features to your applications:\n\nCustomer sentiment analysis\n\nSpam detection\n\nMessage routing decisions\n\nUpsell opportunity analysis\n\nDocument and email classification\n\nDiagnostics\n\nChurn analysis\n\nSuspicious activity identification\n\nRecommendation systems\n\nAnd much more…"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#implementation-options",
    "href": "posts/10-businessopportunities/index.html#implementation-options",
    "title": "Business opportunities from the analysis of customer data",
    "section": "2.8 Implementation options",
    "text": "2.8 Implementation options\n\n2.8.1 Hardware\nOffline: Stand-Alone machine, private cloud.\nOnline: shared hosting, VPS, Virtual Machine\nHosting:1000/month\nVPS: 2000-5000/month\nCloud: AzureML, Google Predictive API, Amazon, Oracle DM, ADAPA Cloud, FICO Cloud, SAS Cloud, Mathematica Cloud\n\n\n2.8.2 Software\nOS: Windows. Linux/UNIX:\nDatabases: SQL Server, Oracle DM, MySQL, MariaDB, Hadoop, NOSQL\nClient End: Excel Add-ins\nAnalysis Packages: MS Excel, SAS, SPSS [ibm], Statistica [dell], STATA, KXEN [sap]\nMADLIB ~ SQL library for databases\nLanguages: sql, python, perl, awk, sed, vba, R, C#, F#, julia\nGraphics/Visualization: 2d/3d ggplot2\n\nPaid: SAS (best), IBM SPSS, Statistica, Stata, MS Excel (most versatile), MS SQL Server Analysis Services (Built-in Data mining Algorithms implemented as SQL based queries & GUI tools), Oracle Data Mining (ODM), Predixion Tools.\nOSS: WekaGUI (Machine Learning), KNIME, R, Python etc….\n\n\n\n2.8.3 Standards\nCRISP-DM (Cross Industry Standard Process for Data Mining): Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment\nSEMMA (Sample, Explore, Modify, Model and Assess) by SAS Inc.\nPMML (Predictive Model Markup Language) – can exchange predictive models across software using XML. Zemetis, The Data Mining Group (dmg.org)\nCONCLUSION"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#why-even-bother",
    "href": "posts/10-businessopportunities/index.html#why-even-bother",
    "title": "Business opportunities from the analysis of customer data",
    "section": "3.1 Why even bother?",
    "text": "3.1 Why even bother?\n\nCAPABILITY"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#future-trends",
    "href": "posts/10-businessopportunities/index.html#future-trends",
    "title": "Business opportunities from the analysis of customer data",
    "section": "3.2 Future trends?",
    "text": "3.2 Future trends?\n\nThe big players of the internet are investing in deep learning, AI & quantum computing to deal with overflowing data.\nAll technologies considered hi-tech or cutting-edge are becoming commercialized for normal businesses.\nNovell Computers or IBM Personal computers were very specialized services. But now are commoditized.\nNow we have a situation where data sets are getting larger and the software to do advanced analysis & create prediction models from the vast amounts of data is getting cheaper (SAS).\nSmaller firms, individuals & startups are getting a level playing field in terms of cost of operations, renting cloud, machines, people (expertise is flowing from research labs to engineering & design depts. of companies.) etc…\n“20-25% of outsourcing contracts now relating to Cloud; IT services companies that don’t invest in Cloud may be making a mistake, say experts”27"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#business-opportunities-from-the-analysis-of-customer-data.",
    "href": "posts/10-businessopportunities/index.html#business-opportunities-from-the-analysis-of-customer-data.",
    "title": "Business opportunities from the analysis of customer data",
    "section": "3.3 Business opportunities from the analysis of customer data.",
    "text": "3.3 Business opportunities from the analysis of customer data.\nThey include:\n\nBasic reporting & analysis\nAdvanced/premium/value-added services\nEngineering, quality control of production, manufacturing.\nWear/Tear, Performance of sensors / devices. • Optimization of resources for customers\nSell raw data to bigger firms.\nSell value added data to bigger firms.\nSell data services to bigger firms.\nGet contracts from bigger firms.\nCreate a marketplace for sensors with advanced analysis in the commercial, industrial or consumer space.\nIntellectual property creation\n\nFor developing countries, it is a wide open market. They have basic electronic infrastructure. Most of the infrastructure in not “smart” or even standardized for advanced data analysis & automated solutions. Less competition especially in the industrial, hardware, electronics, and engineering domain.\nData science in the enterprise\n© 2014 by data science services"
  },
  {
    "objectID": "posts/10-businessopportunities/index.html#footnotes",
    "href": "posts/10-businessopportunities/index.html#footnotes",
    "title": "Business opportunities from the analysis of customer data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIKE LOUKIDES What is data science? The future belongs to the companies and people that turn data into products. (June, 2010) radar.oreilly.com/2010/06/what-is-data-science.html↩︎\nCOLLEEN MCCUE Data Mining and Predictive Analysis (2007) Elsevier Inc1.↩︎\nF. VAN DER HEIJDEN Classification, parameter estimation and state estimation: an engineering approach using MATLAB (2004) John Wiley & Sons Ltd.↩︎\nANDREW R. WEBB AND KEITH D. COPSEY Statistical Pattern Recognition, Third Edition (2011) John Wiley & Sons, Ltd↩︎\nS.B. GREEN, N. J. SALKIND & T. M. AKEY Using SPSS for Windows and Macintosh: Analyzing and understanding data (2008) Prentice Hall. New Jersey↩︎\nKRZYSZTOF J. CIOS Data Mining: A Knowledge Discovery Approach (2007) Springer Science+Business Media, LLC↩︎\nUSAMA FAYYAD, GREGORY PIATETSKY-SHAPIRO AND PADHRAIC SMYTH “From Data Mining to Knowledge Discovery in Databases” AI Magazine (1996), vol. 17 No.3, pp.37—54↩︎\nKRZYSZTOF J. CIOS Data Mining: A Knowledge Discovery Approach (2007) Springer Science+Business Media, LLC↩︎\nMOHAMED MEDHAT GABER (Editor) Scientific Data Mining and Knowledge Discovery: Principles and Foundations (2010) Springer-Verlag Berlin Heidelberg↩︎\nPETER FLACH Machine learning: The Art and Science of Algorithms that Make Sense of Data (2012) Cambridge University Press, New York↩︎\nWILLI RICHERT Building Machine Learning Systems with Python (2013) Packt Publishing↩︎\nSTEPHEN MARSLAND Machine Learning: An Algorithmic Perspective (2009) CRC Press↩︎\nANASSE BARI Predictive Analytics for Dummies (2013) Wiley↩︎\nCONRAD CARLBERG Predictive Analytics with Microsoft Excel (2013) QUE↩︎\nMICHAEL WESSLER Predictive Analytics For Dummies, Alteryx Special Edition (2014) Wiley↩︎\nERIC SIEGEL Predictive Analytics: The Power to Predict who will click, buy, lie or die (2013)↩︎\nJAMES TAYLOR Decision Management Systems - A Practical Guide to Using Business Rules and Predictive Analytics (2012) IBM Press↩︎\nFOSTER PROVOST AND TOM FAWCETT Data Science for Business (2013) O’Reilly Media, Inc↩︎\nNINA ZUMEL Practical Data Science with R (2014) Manning Publications↩︎\nFOSTER PROVOST AND TOM FAWCETT “Data Science and its Relationship to Big Data and Data-Driven Decision Making” Big Data (March 2013), 1(1): 51-59.↩︎\nROBERT MCMILLAN “Inside the Artificial Brain That’s Remaking the Google Empire” Wired Magazine (07.16.14) www.wired.com/2014/07/google_brain/↩︎\nwww.datacenterknowledge.com/archives/2014/05/28/google-using-machine-learning-boost-data-center-efficiency/2/↩︎\nRACHEL COURTLAND “D-Wave Aims to Bring Quantum Computing to the Cloud” (9 Apr 2014) IEEE Spectrum http://spectrum.ieee.org/podcast/computing/hardware/dwave-aims-to-bring-quantum-computing-to-the-cloud↩︎\nwww.microsoft.com/en-us/news/stories/stationq/index.html↩︎\nSTEVEN LEVY “Siri’s Inventors Are Building a Radical New AI That Does Anything You Ask” Wired Magazine (08.12.14) www.wired.com/2014/08/viv/↩︎\nXINDONG WU AND VIPIN KUMAR (eds.) The Top Ten Algorithms in Data Mining (2009) Chapman and Hall/CRC Press↩︎\nITIKA SHARMA PUNIT “Cloud: The reality that enterprises cannot escape” Business Standard Newspaper Bangalore Edition (July 7, 2014)↩︎"
  },
  {
    "objectID": "posts/03-thebestwaytostudyforsas/index.html",
    "href": "posts/03-thebestwaytostudyforsas/index.html",
    "title": "The best options to study for SAS Certified Base Programmer for SAS 9 Credential",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:59 am\nIf SAS software will be a part of your career then get certified by passing the SAS Base Programming Exam from SAS Institute. It is designed to test your knowledge in writing SAS programs to access & manage data to perform queries and analyses. It also creates a good impression to your client / company.\nhttp://support.sas.com/certify/creds/bp.html\nOption 1: If you can afford it or have your company/institution sponsor/subsidize you, then the two courses offered by the SAS Institute is the best way to learn the Base SAS system & programming.\nSAS Programming 1: Essentials http://support.sas.com/edu/schedules.html?id=277&ctry=US\nSAS Programming 2: Data Manipulation Techniques http://support.sas.com/edu/schedules.html?id=278&ctry=US\nIt is a highly structured course with code examples, exercises & practice questions that will thoroughly teach you the basic concepts of SAS software & Base SAS programming.\nOption 2: The next best option is to purchase the following book\nBase SAS Programmer Certification 3rd edition\nhttp://www.amazon.com/SAS-Certification-Prep-Guide-Programming/dp/1607649241/ref=sr_1_1?ie=UTF8\nIt is a condensed version of the full course & hence is the best option for those that cannot afford the full course. Use it with the Base SAS software available at your academic institution or company & practice all the example code. You can also buy access to SAS OnDemand to use SAS software through the Internet to practice for the exam.\nOption 3: Join an institute that offer training in SAS & statistical analysis. They usually frame their SAS course syllabus & material from the official courses & have computer labs with access to Base SAS software. The advantage here is having an instructor who can give you feedback on your progress & clear your doubts & queries regarding the exam.\nPreparation: You can be ready with 4-5 months of preparation if you follow any of the above options. Modifying existing code & using different procedures / keywords to achieve the same result is best way to understand how SAS works. It requires time & practice as SAS software is vast & comprehensive.\nSAS Institute also offers a practice exam that you can purchase to test yourself before attempting the actual exam.\nIt should be understood that in addition to all this, there is an abundant amount of material on the Web to help you learn Base SAS programming & prepare for the certification credential exam. All the best!"
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "",
    "text": "Created on Sunday, May 18th, 2014 at 3:24 am\nOne of the important skills in SAS is being able to connect & query a local or remote database, conduct data analysis in SAS & write the new information back to the database. SAS provides access to a variety of databases as well as different ways of doing so. In this article, access to Microsoft SQL Server (MSSQL) using SAS/ACCESS ODBC interface is explained. If you are using SAS at an institute or organization, your Systems Administrator would have setup everything. The steps in this article might vary depending on the software configuration."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#summary",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "Summary:",
    "text": "Summary:\nThe SAS/ACCESS interface component is used to connect to various kinds of databases. Three main ways of connecting SAS with MSSQL are using SAS/ACCESS interface for MS SQL Server, SAS/ACCESS interface for OLE DB & SAS/ACCESS interface for ODBC. SAS allows data retrieval/storage using both LIBNAME statement and direct SQL Pass-Through statement method.\nWhat is shown above is very elementary code just to get the process started. Students learning data science/analytics should try to use different combinations of SAS methods & SAS interfaces to retrieve data from a database, manipulate it using SAS/SQL & write the processed data sets back to the database."
  },
  {
    "objectID": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "href": "posts/09-connectquerymssqlsasinterfaceodbc/index.html#references",
    "title": "Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC",
    "section": "References:",
    "text": "References:\n\nSAS/ACCESS 9.2 for Relational Databases Reference, Fourth Edition. http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#titlepage.htm\n\nSAS 9.2 SQL Procedure User’s Guide. http://support.sas.com/documentation/cdl/en/sqlproc/62086/HTML/default/viewer.htm#titlepage.htm"
  },
  {
    "objectID": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "href": "posts/cs01-suitabilityofstationarydemandmodels/index.html",
    "title": "Suitability of stationary demand models for forecasting",
    "section": "",
    "text": "Objective\nStudy the suitability of stationary demand models for forecasting sales at the Shah Alam Palm Oil Company.\n\n\nIntroduction\nPalm oil is harvested from the fruit of oil palm trees and is widely used as a cooking oil throughout Africa, Southeast Asia, and parts of Brazil. It is becoming widely used throughout the world as it is a lower cost alternative to other vegetable oils and has other attractive properties.\nThe Shah Alam Palm Oil Company (SAPOC) harvests, processes, and sells palm oil throughout the region. As a demand analyst, you are asked to review the sales volume (in pounds) of you premium palm oil by one of your customers, a local grocery store in the region.\n\n\nVisualization of the raw data\n\nQ1. What is the trend over the last three years?\nThere appears to be a positive trend. From the graph there is an increase in the demand of palm oil by about 24 lbs per month for the last three years.\n\nQ2. Does there appear to be any seasonality in the demand pattern?\nYes. If we plot the data by the months for each of the years, there seems to be seasonality to the demand. Demand is low from January to May. It picks up from June to August and then again from October to December.\n\nQ3. What is the forecast for demand in January 2015?\n\n\n\n\n\n\n\n\nMethod\nForecast for Jan 2015 (lbs)\nActual value in Dec 2014\n\n\n\n\nNaïve Model\n1512\n1512\n\n\nCumulative Model\n957.9444\n1512\n\n\n12 Period Moving Average\n1173.667\n1512\n\n\n\nQ4. What is the root mean square error (RMSE) for a next period forecast for these three years of demand?\n\n\n\nMethod\nRMSE\n\n\n\n\n\nNaïve Model\n383.7282\n\n\n\nCumulative\n419.8851\n\n\n\n12 Period Moving Average\n423.33\n\n\n\n\nQ5. Which of these three models is most appropriate for forecasting the January 2015 demand?\nNone. As shown above, the palm oil data shows a positive trend & seasonality during the years. The Naïve model forecast for Jan 2015 is clearly different from the previous trends. While the cumulative & naïve models are quite calm indicating they are forecasting demand closer to the average of the data. The main reason for this discrepancies is that the above three models assume a stationary demand that is very close to the level of the mean."
  },
  {
    "objectID": "posts/cs03-comparisonnaivecumulative/index.html",
    "href": "posts/cs03-comparisonnaivecumulative/index.html",
    "title": "Improving the naïve model forecast using cumulative period model",
    "section": "",
    "text": "Objective:\nComparing error metrics by switching naïve model to cumulative model.\nIntroduction:\nYou have been hired by General Miles, a company that produces healthy gluten‐free breakfast cereal bars. The last market introduction happened a year ago and your manager thinks there might be an issue in the forecasting methodology. They are currently using a simple Naive forecasting model and you think there might be some room for improvement.\nYour boss provides you with the sales for the last 12 months and the forecasts for the last 11 months. No data is available to forecast the first month as the product was totally new to the market at the time.\nVisualization of the raw data\n\nComparison of the forecasting models\n\nComparison of Model Error Metrics\n\n\n\nError Metric\nRMSE\nMAPE\n\n\n\n\nNaïve Model\n80.45665\n0.155729\n\n\nCumulative Model\n131.226\n0.269029\n\n\n\nConclusion:\nThe cumulative model in this case is worse than the Naïve model since the RMSE and the MAPE values are greater. It does not react quickly enough to adapt to the high variability in sales for this new gluten free cereal bar."
  },
  {
    "objectID": "posts/07-softwarechoicesremotedb/index.html",
    "href": "posts/07-softwarechoicesremotedb/index.html",
    "title": "Software choices to implement a remote database client/server network setup",
    "section": "",
    "text": "Posted on Thursday, May 8th, 2014 at 2:25 am\nInteracting with a database on a network from statistical/reporting software like MS Excel, MS Access, SAS, R, Python, etc… is a great way to learn how data might be retrieved, analysed & stored remotely. Web scripting languages like ASPX, PHP also allow remote database interactions for data analysis using Web based applications.\nHowever, a local database with local software using only individual local files pretty much provides the same experience as using them on a network. But the aim here is to simulate a corporate/research environment where all the software & data resources are spread throughout the network. The aim is not to become an expert in the technologies but to know just enough to use them to do data science/analytics.\nHence the philosophy is to use minimum hardware/software resources to be able to study & learn quickly and efficiently. The problem is that each person has a different configuration of client/server and hardware/software components & usually one person’s solution will not necessarily work for the other. The choices available to you will be different from the choices presented here.\nServer decision process:\n\nServer database: MS SQL Server (MSSQL) vs. Mysql Database (mysql)\nDecision: MSSQL\nReason: MSSQL has built-in data mining through SQL Server Analysis Services (SSAS). Also MSSQL2008 offers more data mining options when compared to MSSQL2005. There is more information online about MSSQL data mining features than about mysql data mining features. Furthermore, MS Excel can perform data mining directly on spreadsheets using SSAS via an add-in\nServer OS: Windows XP (winxp) vs. Windows Vista/7 (win7)\n\nDecision: win7\n\nReason: win7 has similar features to Windows Server 2008 & handles networking better than winxp. Winxp would be sufficient but one needs to install additional software components such as dotnet packages, VC++ run-time packages, deal with permissions/networking issues etc…\nOnce you have the server & software setup for networking & remote access, almost any hardware/software combination can be used as a client.\nClient decision process:\n\nClient OS: Windows vs. GNU/Linux\n\nDecision: Windows\nReason: MS Excel and MS Access are used widely in the business/research world. Hence Windows XP was chosen. Also MS Excel supports many add-ins for statistical analysis, data mining & visualization. Although software such as R, Python, WEKA etc… would integrate better with GNU/Linux.\n\nClient web server: MS Internet Information Services (IIS) vs. Apache Web Server (apache).\n\nDecision: IIS\n\nReason: A student of data science/analytics does not need advanced features of a web server. A basic version of IIS comes built-in with winxp. I am guessing it is enough for the purpose of learning. Apache is robust but requires more configuration than IIS.\n\nClient web language: ASPX vs. PHP\n\nDecision:PHP\n\nReason: PHP is easy to configure for IIS & light on system resources. ASPX would be the preferred choice for use with IIS and MSSQL, but requires more configuration & systems resources for Visual Studio 2010 development environment. Students can look at the PHPStats project https://github.com/mcordingley/PHPStats for using statistical functions to do data analysis on the web.\n\nOS for Web Server: Client OS vs. Server OS\n\nDecision: Client OS\n\nReason: Decision to install IIS/PHP on Client OS was primarily done for two reasons i) To isolate the database from any instability that might result from the Web application environment ii) To simulate an “Internet” where the database & web server reside on different machines.\n\nMisc. client software: MS Excel, MS Access, R, Python. It is advisable to try the database access features of many software for learning & practice.\nSummary:\nFinal server setup: SQL Server 2008 on Windows 7.\nFinal client setup: PHP using IIS on Windows XP.\nThe main idea is to simulate a corporate/research data science/analytics environment where data & software resources are spread throughout the network. The design choices are made to use minimum resources. This enables students to learn the basics efficiently without having to worry about the advanced features. The implementation of these choices will be discussed in another post."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "",
    "text": "To help the Fenway Park concessions evaluate and compare the quality of these three competing forecasting approaches."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#visualization-of-raw-data",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Visualization of raw data",
    "text": "Visualization of raw data"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#comparison-of-the-mean-deviation-md-among-the-three-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Comparison of the mean deviation (MD) among the three models",
    "text": "Comparison of the mean deviation (MD) among the three models\n\n\n\n\n\n\n\n\nModel\nMD\n% deviation from mean of actual sales (3750.056)\n\n\n\n\nM1\n‐526.988\n‐14.0528\n\n\nM2\n24.45679\n0.652171\n\n\nM3\n‐11.6852\n‐0.3116\n\n\n\nAs seen above M1 exhibits the most bias. M3 has the least bias. This is done to\nModels 2 and 3 have an average error in the vicinity of only a few dozen hotdogs from the actual. In fact,\nM2 & M3 have a MD equivalent to less than 1% of the average hot dog sales per game.\nHowever, Model 1 seems to consistently over‐estimate the demand for hotdogs by an average of 500 units per game ‐ or about 14% of the average hot dog sales per game). Therefore, we can say that – based on the historical data we have – Models 2 and 3 are less biased than Model 1. Model 1 is the most biased of the pack and it is OVER forecasting."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#calculation-of-rmse-of-each-of-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Calculation of RMSE of each of the models",
    "text": "Calculation of RMSE of each of the models\n\n\n\nModel\nRMSE\n\n\n\n\nM1\n597.6846\n\n\nM2\n286.457\n\n\nM3\n500.2935\n\n\n\nAs seen above, M2 has an error that is half of the other models & hence is the most suitable forecasting model besides also have a small deviation in forecasted sales from the actual demand."
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#effect-of-underestimating-overestimating-by-the-models",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Effect of underestimating & overestimating by the models",
    "text": "Effect of underestimating & overestimating by the models\nAs mentioned in the introduction,\n\n\n\n\n\n\n\n\nForecasting Issues\nEffect\nCost\n\n\n\n\nUnderestimating\nUnsatisfied & hungry fans\n$3 per lost sale\n\n\nOverestimating\nWaste of ingredients, energy & labor\n$2 per unsold hot dog\n\n\n\nCalculations of the lost‐sale or unsold inventory by the three models.\n\n\n\nMetric\nM1\nM2\nM3\n\n\n\n\nShortage of hotdogs\n0\n21765\n32883\n\n\nCost of Lost Sales $\n0\n65295\n98649\n\n\nUnsold Hotdogs\n85372\n17803\n34776\n\n\nCost of Unsold hotdogs $\n170744\n35606\n69552\n\n\nTotal Loss $\n170744\n100901\n168201\n\n\n\n\nLeast Total loss"
  },
  {
    "objectID": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "href": "posts/cs04-analysisofblackboxtypedemand/index.html#conclusion",
    "title": "Analysis of three black‐box type demand forecasting models",
    "section": "Conclusion",
    "text": "Conclusion\nWhen comparing the MD, RMSE among the three models, M2 seems to be the most accurate in its predictions and it also manages to provide the least total loss & a balance between the cost of lost sales and unsold hot dogs better than the other two models.\nTherefore, from among the three available options, M2 seems to make the predictions that make most economic sense."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "",
    "text": "Objective\nFormulation & testing of different exponential models on the product data."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#simple-exponential-smoothing-ses",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Simple Exponential Smoothing (SES)",
    "text": "Simple Exponential Smoothing (SES)\nWe know that SES assumes stationary demand. i.e. it forecasts does not take into account trends or seasonalities. Even so, we would still like to know the effect of using SES on the forecasts.\nUnderlying model: \\(x_{t} = a + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+1} = \\alpha x_{t} + (1 – \\alpha) \\hat{x}_{t-1,t}\\)\nWhere \\(\\hat{x}_{t,t+1}\\) is forecast for the next period, \\(x_{t}\\) is the present actual demand and \\(\\hat{x}_{t-1,t}\\) is forecast for the previous period.\nInitialization of the parameters\nThere are many ways of doing this. We can take the centered average for the first 4 or 5 periods. We can also take the average of the first 3, 4 or 5 periods depending on the data.\nWe take \\(\\hat{a}_{4,5}\\)=205.25\nWe take α=0.15\nUsing the above model, the forecast for period 25 is around 654 bars. Also the forecast for period\n30 will also be the same i.e. 654 since the model assumes stationary demand.\nMAPE for SES is 0.279329"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#holts-model-hm",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Holt’s Model (HM)",
    "text": "Holt’s Model (HM)\n\nSince the data shows a positive trend, we use HM which assumes level & trend.\nUnderlying model: \\(x_{t} = a + bt + e_{t}\\)\nForecasting model: \\(\\hat{x}_{t,t+\\tau} = \\hat{a}_{t}+\\tau \\hat{b}_{t}\\)\nUpdating Component:\n\\(\\hat{a}_{t}=\\alpha \\hat{x}_{t}+ (1-\\alpha)\\hat{x}_{t-1,t}\\)\na^t = α xt + (1 – α) x^t‐1,t\n\\(\\hat{b}_{t}=\\beta (\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta) \\hat{b}_{t-1}\\)\nb^t = β (a^t ‐ a^t‐1) + (1 – β) b^t‐1\nInitialization of parameters\nWe take α = 0.2 and β = 0.05\nWe take \\(\\hat{a}_{t}\\) = 157.5 & \\(\\hat{b}_{t}\\) = 19.1"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#comparison-of-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Comparison of models",
    "text": "Comparison of models"
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#mape-of-the-various-models",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "MAPE of the various models",
    "text": "MAPE of the various models\n\n\n\n\nMAPE\n\n\n\n\nSES\n0.279329\n\n\nHM (alp=0.2bet=0.05)\n0.128729\n\n\nHM (alp=0.5bet=0.05)\n0.120443\n\n\nHM (alp=0.99bet=0.05)\n0.112559\n\n\n\nThe MAPE and various other measures such as RMSE or MAD or most any other metric will improve as we increase the value of Alpha. This does not mean we are getting a better model. This means is that we are only fitting the model better to the historical data that we have. We are simple placing more weight to the most recent observations. Therefore, we should monitor effect of alpha & change as and when required."
  },
  {
    "objectID": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "href": "posts/cs07-forecastingforsugarbonbon/index.html#conclusion",
    "title": "Forecasting for Sugar Bon‐Bon Cereals",
    "section": "Conclusion",
    "text": "Conclusion\nHolt’s model with alpha=0.2 seems to be best way to forecast the demand of sugar cereals."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/08-projecttoimplementremotedb/index.html",
    "href": "posts/08-projecttoimplementremotedb/index.html",
    "title": "Project to implement remote PHP web client/ MSSQL database server network setup",
    "section": "",
    "text": "Posted on Friday, May 16th, 2014 at 2:43 pm\nFor students learning data science/analytics, accessing data sets stored from databases in remote servers is a necessary skill. People in corporations/institutions have Systems Administrators to set up most of the interfaces for such purposes. However knowing how such a process is setup is a good knowledge. Here PHP is used to demonstrate database access.\nAlthough this project is not related to data science/analytics, it will give a good hands-on experience of client/server networking. These are mere guidelines and not details as specific options & settings will depend on the version of the software & components.\nPre-requisites: Fairly good understanding of computer hardware especially networking, Windows operating system, drivers & software installation. Hardware/Networking\nPhysical: Atleast two computers with Ethernet/WiFi capability and a wireless/wired router.\nFinal Setup: Ability to create Workgroups & shared folders. Also ability to ping & telnet the remote computers/servers. Server Setup\nThere are many versions & editions MSSQL depending on user requirements (The Developer Edition is best for learning & academic use). For data mining features, SQL Server Analysis Services(SSAS) must be installed. To install OLTP/DW AdventureWorks sample databases, enable FILESTREAM and make sure the SQL Full-text Filter Daemon Launcher service is running.\n\nIf you have the space, do a full install of every feature. It will make it easier to troubleshoot problems.\nCreate a MSSQL user exclusively for remote access. http://technet.microsoft.com/en-us/library/aa337545.aspx\nAllow remote connection to MSSQL. http://technet.microsoft.com/en-us/library/ms191464.aspx\nEnable Shared Memory, TCP/IP, pipes http://technet.microsoft.com/en-us/library/ms181035.aspx\nIn the Windows Firewall, allow incoming/outgoing connections for port 1433 (default MSSQL port.) http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx\n\nFinal Setup: From the Command Prompt in the client, execute ping dbpcip and telnet dbpcip 1433 where dbpcip is the IP address of MSSQL machine. If you can connect with both the commands, the client/server networking is probably working. Although accessing the datebase & tables finally depends on the permissions assigned to the remote user. Client Setup\nKeep these details in mind before starting installation. 1) Some of the types of PHP functions for access to MSSQL. This article only discusses mssql_() and sqldrv_().\n`mssql_()` Supported by PHP 5.2 (php52) and lower. MSSQL Driver for PHP (SSDPHP) not required\n`sqldrv_()` Supported by PHP 5.3 (php53) and higher. SSDPHP required.\n`pdo_sqlsrv_()` Supported by php52 and higher. SSDPHP required.\n`pdo_odbc_()` Supported by php51 and higher. SSDPHP not required.\n\nSQL Server Native Client (SSNC): This is needed by PHP to access MSSQL Server. SSNC2008: installs on winxp SSNC2012: does not install on winxp. php52: requires atleast SSNC2008 php53: requires atleast SSNC2012\nProblem: php53 onwards requires SSNC2012, but SSNC2012 does not work on winxp. Also php53 does not support mssql_() functions.\n\nSolution: SSNC2008 works on winxp. Use php 5.2.13 to 5.2.17 as php52 requires SSNC2008. Also php 5.2.x supports mssql_()functions and sqldrv_() functions if you install the SSDPHP 2.0. Client software\n\nInternet Information Services (IIS): Official Documentation: https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/iiiisin2.mspx?mfr=true\n\nStep-by-step illustrated guide: Follow until step 10 http://www.wikihow.com/Configure-IIS-for-Windows-XP-Pro\nYou can test if it is working by opening the http://localhost/ address on the web browser. You should also test it on a remote computer using http://clientip/\nFinal Setup: Windows Version: XP IIS Version: 5.1\n\nFastCGI:\n\nFastCGI helps IIS work better with PHP. Install the one which is compatible with your version of IIS & OS. Official source: http://www.iis.net/downloads/microsoft/fastcgi-for-iis\nFinal Setup:fcgisetup_1.5_rtw_x86.msi\n\nPHP installation:\n\n\nSteps to installing PHP with IIS are explained in &lt;www.php.net/manual/en/install.windows.iis6.php&gt;\n5.2 and lower mssql_()documentation http://www.php.net/manual/en/book.mssql.php\n5.3 and higher sqlsrv_() documentation http://www.php.net/manual/en/book.sqlsrv.php\n5.2 and higher pdo_sqlsrv() documentation http://www.php.net/manual/en/ref.pdo-sqlsrv.php\n5.1 and higher pdo_odbc() documentation http://www.php.net/manual/en/ref.pdo-odbc.php\n\nFinal Setup: php-5.2.17-nts-Win32-VC6-x86.msi\n\nMSSQL Driver for PHP:\n\nOfficial Documentation: http://technet.microsoft.com/en-us/library/dn425064%28v=sql.10%29.aspx\nInstallation help: http://www.iis.net/learn/application-frameworks/install-and-configure-php-on-iis/install-the-sql-server-driver-for-php\nFinal Setup: SQLSRV20.exe\n\nSQL Server Native Client:\n\nUse this page to decide what components are needed for your setup. http://msdn.microsoft.com/en-us/library/cc296170%28SQL.105%29.aspx\nFinal Setup: sqlncli2k8r2x86.msi\n\nOverview: The entire process can be very broadly summarised as:\n\nIIS -&gt; FastCGI -&gt; PHP 5.3 -&gt; sqlsrv_() -&gt; SSDPHP 3.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; sqlsrv_() -&gt; SSDPHP 2.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; mssql_()-&gt; No SSDPHP -&gt; SSNC -&gt; MSSQL Testing the final setup:\n\nPHP System Information Test: If PHP is properly installed, the following code should execute:\n\nSOURCE CODE:\n&lt;?php echo phpinfo(); ?&gt;\nYou should get PHP system information which includes information about msql_() and sqlsrv_() extensions. Only relevant partial output shown below.\nOUTPUT:\n\n    \n        Registered PHP Streams \n            php, file, data, http, ftp, compress.zlib, compress.bzip2, \n        https, ftps, zip, sqlsrv   \n    \n\n\n\n\ncgi-fcgi\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        cgi.check_shebang_line\n            1\n            1\n    \n\n    \n        cgi.fix_pathinfo\n            1\n            1\n    \n\n    \n        cgi.force_redirect\n            0\n            0\n    \n\n    \n        cgi.nph\n            0\n            0\n    \n\n    \n        cgi.redirect_status_env\n            no value\n            no value\n    \n\n    \n        cgi.rfc2616_headers\n            0\n            0\n    \n\n    \n        fastcgi.impersonate\n            1\n            1\n    \n\n    \n        fastcgi.logging\n            0\n            0\n    \n\n\n\nmsql\n\n\n    \n        MSQL Support \n            enabled \n    \n\n    \n        Allow Persistent Links \n            yes \n    \n\n    \n        Persistent Links \n            0/unlimited \n    \n\n    \n        Total Links \n            0/unlimited \n    \n\n\n\nmssql\n\n\n    \n        MSSQL Support\n            enabled\n    \n\n    \n        Active Persistent Links \n            0 \n    \n\n    \n        Active Links \n            0 \n    \n\n    \n        Library version \n            7.0 \n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        mssql.allow_persistent\n            On\n            On\n    \n\n    \n        mssql.batchsize\n            0\n            0\n    \n\n    \n        mssql.compatability_mode\n            Off\n            Off\n    \n\n    \n        mssql.connect_timeout\n            5\n            5\n    \n\n    \n        mssql.datetimeconvert\n            On\n            On\n    \n\n    \n        mssql.max_links\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_persistent\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.max_procs\n            Unlimited\n            Unlimited\n    \n\n    \n        mssql.min_error_severity\n            10\n            10\n    \n\n    \n        mssql.min_message_severity\n            10\n            10\n    \n\n    \n        mssql.secure_connection\n            Off\n            Off\n    \n\n    \n        mssql.textlimit\n            Server default\n            Server default\n    \n\n    \n        mssql.textsize\n            Server default\n            Server default\n    \n\n    \n        mssql.timeout\n            60\n            60\n    \n\n\n\npdo_sqlsrv\n\n\n    \n        pdo_sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        pdo_sqlsrv.log_severity\n            0\n            0\n    \n\n\n\nsqlsrv\n\n\n    \n        sqlsrv support\n            enabled\n    \n\n\n\n\n    \n        Directive\n            Local Value\n            Master Value\n    \n\n    \n        sqlsrv.LogSeverity\n            0\n            0\n    \n\n    \n        sqlsrv.LogSubsystems\n            0\n            0\n    \n\n    \n        sqlsrv.WarningsReturnAsErrors\n            On\n            On\n    \n\n\nRemote MSSQL Access Test: If phpinfo() indicates that all the drivers & extensions are installed properly, then test to see if the database server & client are properly registered.\n\nSOURCE CODE:\n\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n\nif( $client_info = sqlsrv_client_info( $conn)) {\n    foreach( $client_info as $key =&gt; $value) {\n        echo $key.\": \".$value.\"\n\";\n    }\n} else {\n    echo \"Error in retrieving client info.\n\";\n}\n\n\n$server_info = sqlsrv_server_info( $conn);\nif( $server_info )\n{\n    foreach( $server_info as $key =&gt; $value) {\n       echo $key.\": \".$value.\"\n\";\n    }\n} else {\n      die( print_r( sqlsrv_errors(), true));\n}\n?&gt;\nOUTPUT:\nConnection established.\n\n\n    \n        Parameter\n            Value\n    \n\n    \n        DriverDllName:\n            sqlncli10.dll\n    \n\n    \n        DriverODBCVer:\n            03.52\n    \n\n    \n        DriverVer:\n            10.50.1600\n    \n\n        \n        ExtensionVer:\n            2.0.1802.200\n    \n\n        \n        SQLServerVersion:\n            10.50.1600\n    \n\n        \n        SQLServerName:\n            DBPC\n\nTest of SQL Query: Once a connection is established, the final test is to see whether query execution is possible or not. This is a quick crude code. But the fact that it retrieves data from master.dbo.spt_monitor table shows the setup & the connection works.\n\nSOURCE CODE:\n&lt;?php\n$serverName = \"dbpcip, 1433\"; //serverName\\instanceName, portNumber (default is 1433)\n$connectionInfo = array( \"Database\"=&gt;\"master\", \"UID\"=&gt;\"userName\", \"PWD\"=&gt;\"passWord\");\n$conn = sqlsrv_connect( $serverName, $connectionInfo);\n\nerror_reporting(-1);\n\n\nif( $conn ) {\n     echo \"Connection established.\n\";\n}else{\n     echo \"Connection could not be established.\n\";\n     die( print_r( sqlsrv_errors(), true));\n}\n/* SQL Query */\n$sql=\"select * from dbo.spt_monitor\";\n$results = sqlsrv_query( $conn, $sql );\nif( $results === false) {\n    die( print_r( sqlsrv_errors(), true) );\n}\n    echo \"MSSQL master.dbo.spt_monitor TABLE\n\";\n        echo \"\n            &lt;table border=1&gt;\n            &lt;tr&gt;\n                &lt;th&gt;cpu_busy&lt;/th&gt;\n                &lt;th&gt;io_busy&lt;/th&gt;\n                &lt;th&gt;idle&lt;/th&gt;\n                &lt;th&gt;pack_received&lt;/th&gt;\n                &lt;th&gt;pack_sent&lt;/th&gt;\n                &lt;th&gt;connections&lt;/th&gt;\n                &lt;th&gt;pack_errors&lt;/th&gt;\n                &lt;th&gt;total_read&lt;/th&gt;\n                &lt;th&gt;total_write&lt;/th&gt;\n                &lt;th&gt;total_errors&lt;/th&gt;\n             &lt;/tr&gt;\";\n    while ($row = sqlsrv_fetch_array($results))\n    {\n                $cpu_busy=$row[1];\n                $io_busy=$row[2];\n                $idle=$row[3];\n                $pack_received=$row[4];\n                $pack_sent=$row[5];\n                $connections=$row[6];\n                $pack_errors=$row[7];\n                $total_read=$row[8];\n                $total_write=$row[9];\n                $total_errors=$row[10];\n             \n    echo \"\n            &lt;tr&gt;\n                &lt;td&gt;$cpu_busy&lt;/td&gt;;\n                &lt;td&gt;$io_busy&lt;/td&gt;;\n                &lt;td&gt;$idle&lt;/td&gt;;\n                &lt;td&gt;$pack_received&lt;/td&gt;;\n                &lt;td&gt;$pack_sent&lt;/td&gt;;\n                &lt;td&gt;$connections&lt;/td&gt;;\n                &lt;td&gt;$pack_errors&lt;/td&gt;;\n                &lt;td&gt;$total_read&lt;/td&gt;;\n                &lt;td&gt;$total_write&lt;/td&gt;;\n                &lt;td&gt;$total_errors&lt;/td&gt;;\n            &lt;/tr&gt;\";\n    }\n    echo \"&lt;/table&gt;\";\n    ?&gt;\nOUTPUT:\nConnection established.\n\nMSSQL master.dbo.spt_monitor TABLE\n\n; ; ; ; ; ; ; ; ; ;\n\n            \n\n                \n                    cpu_busy\n                        io_busy\n                        idle\n                        pack_received\n                        pack_sent\n                        connections\n                        pack_errors\n                        total_read\n                        total_write\n                        total_errors\n                \n\n                \n                    9\n                        7\n                        792\n                        28\n                        28\n                        14\n                        0\n                        0\n                        0\n                        0\n                \n\nIncorrect SSDPHP version:\n\nProblem: If you use php53/SSDPHP 3.0 code but use SSNC2008 instead of SSNC 2012, you get an error message.\nCould not connect. Array ( [0] =&gt; Array ( [0] =&gt; IMSSP [SQLSTATE] =&gt; IMSSP [1] =&gt; -49 [code] =&gt; -49 [2] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 [message] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: http://go.microsoft.com/fwlink/?LinkId=163712 ) [1] =&gt; Array ( [0] =&gt; IM002 [SQLSTATE] =&gt; IM002 [1] =&gt; 0 [code] =&gt; 0 [2] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified [message] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified ) )\nSolution: Install php52/SSDPHP 2.0 & SSNC2008\nSummary:\nAlthough this project takes a lot of time to setup, troubleshoot & tweak all the settings, the end result is a client/server setup one can experiment with. Once the database server is setup properly, various types of software can be configured to retrieve/store data. PHP can be used to create web applications for doing data analysis using MSSQL.\nTo access data from MSSQL using php52, mssql_() functions is used. But php53 onwards uses only sqldrv_() functions to access MSSQL. Hence it is better to learn sqldrv_() functions. However knowing how to use the older functions will help when dealing with legacy systems having php52."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n“Turn yourself not away from three best things: Good Thought, Good Word, and Good Deed.” ~ Zarathushtra\nThe website content is divided into four main types:\n\nProjects: Longer term and more complex work involving code, and other software websites.\nArticles: Shorter term work involving coding and analysis and opinion. Reviews would come under articles.\nGuides: Longer term and frequently updated notes for learning topics.\nMisc: Random stuff that does not fit anything formal.\n\nThe content categories are further divided into four main areas:\n\nProduct Management: Planning, creating, designing, managing, & selling products. UI/UX design, technology, business & startups.\nData Science: Data science, analytics, visualization, machine learning & artificial intelligence.\nMetaverse: Includes Augmented, Virtual, Mixed, Extended Reality hardware, software, code & business\nSoftware: Usage, coding, setup, scripts, algorithsm, & recommendations.\n\nClick here to view Portfolio of work\nThoughts that have had an impact on my thinking:\n\n“In order to change, I have to be something I’m not. I came here to turn things around. To become the best in the world. Unless I beat someone stronger than me, that won’t happen.” ~ Blue Lock"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "L'Œuvre",
    "section": "",
    "text": "Explore product management, data science, & the metaverse.\n\n\n\n\n\n\n\n\n\nAbout this website.\nCurrent Work & Projects:\n\n\nJira Product Discovery Workflows\nPostman API Software\nPM Frameworks\nPM Templates\n\nBelow is a list of all work done and ongoing projects.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nModified\n\n\n\n\n\n\nPost With Code\n\n\nFeb 18, 2029\n\n\n\n\nWelcome To My Blog\n\n\nFeb 18, 2026\n\n\n\n\nSome questions to ask entrepreneurs\n\n\nFeb 11, 2016\n\n\n\n\nExtracting initial parameters from an existing Holt‐Winter forecasting model\n\n\nNov 29, 2015\n\n\n\n\nForecasting with seasonal trends at BLAYK restaurant\n\n\nNov 24, 2015\n\n\n\n\nForecasting for Sugar Bon‐Bon Cereals\n\n\nNov 23, 2015\n\n\n\n\nExponential smoothing models at TrainMax Systems\n\n\nNov 23, 2015\n\n\n\n\nChoice of moving average or exponential smoothing for a particular product profile\n\n\nNov 23, 2015\n\n\n\n\nEvaluation of stationary demand models\n\n\nNov 20, 2015\n\n\n\n\nAnalysis of three black‐box type demand forecasting models\n\n\nNov 19, 2015\n\n\n\n\nImproving the naïve model forecast using cumulative period model\n\n\nNov 18, 2015\n\n\n\n\nPerformance characteristics of forecasting models\n\n\nNov 18, 2015\n\n\n\n\nSuitability of stationary demand models for forecasting\n\n\nNov 18, 2015\n\n\n\n\nProject to implement remote PHP web client/ MSSQL database server network setup\n\n\nAug 16, 2015\n\n\n\n\nCombinatorial analysis & calculations using SAS functions – fact(), perm() and comb()\n\n\nAug 16, 2015\n\n\n\n\nHow do I to get started in & learn ‘data science’, ‘data analytics’?\n\n\nAug 16, 2015\n\n\n\n\nSoftware choices to implement a remote database client/server network setup\n\n\nAug 16, 2015\n\n\n\n\nConnect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC\n\n\nAug 16, 2015\n\n\n\n\nThe best options to study for SAS Certified Base Programmer for SAS 9 Credential\n\n\nAug 16, 2015\n\n\n\n\nHow to find out and use the number of observations in a given SAS data set\n\n\nAug 16, 2015\n\n\n\n\nReferring to a SAS data set with its full filesystem path\n\n\nAug 16, 2015\n\n\n\n\nWhat is ‘data science’ / data analytics? - Yet another opinion\n\n\nJun 20, 2015\n\n\n\n\n11-geointfinalproject\n\n\nJul 31, 2014\n\n\n\n\n12-briefoverviewprinciplesgeoint\n\n\nJul 31, 2014\n\n\n\n\nBusiness opportunities from the analysis of customer data\n\n\nJul 31, 2014\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/04-combinatorialfunctionsas/index.html",
    "href": "posts/04-combinatorialfunctionsas/index.html",
    "title": "Combinatorial analysis & calculations using SAS functions – fact(), perm() and comb()",
    "section": "",
    "text": "Created on Friday, March 7th, 2014 at 5:56 am\nSAS provides several kinds of functions for doing combinatorial analysis and calculations. Three basic ones will be demonstrated here. fact(),perm() and comb(). All three functions return a missing value for the arguments they cannot compute.\nfact(n) is the function for calculating the factorial n! of any non-negative number n.\n275  data _null_;\n276  a=fact(-3);\n277  b=fact(0);\n278  c=fact(9);\n279  d=fact(170);*170! is the max calculable by this particular computer.;\n280  e=fact(171);\n281  f=fact(1000);\n282  put _all_;\n283  run;\nThe output is given by:\nNOTE: Invalid argument to function FACT at line 276 column 3.\nNOTE: Invalid argument to function FACT at line 280 column 3.\nNOTE: Invalid argument to function FACT at line 281 column 3.\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\na=. b=1 c=362880 d=7.257416E306 e=. f=. _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 276:3   1 at 280:3   1 at 281:3\nNOTE: DATA statement used (Total process time):\nSimilarly for permutation of n objects taken r at a time (where n&gt;r), we have the perm(n,r). A single argument in the perm() function will calculate the factorial of the argument.\n384  data _null_;\n385  a=perm(3,3);\n386  b=perm(2,5);\n387  c=perm(5,2);\n388  d=perm(5,4);\n389  e=perm(1660,170);\n390  f=perm(4);\n391  put _all_;\n392  run;\nThe output is given by\nNOTE: Argument 2 to function PERM at line 386 column 3 is invalid.\nNOTE: Invalid argument to function PERM at line 389 column 3.\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\na=6 b=. c=20 d=120 e=. f=24 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places. The results of the\n      operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 386:3   1 at 389:3\nNOTE: DATA statement used (Total process time):\nSimilarly for combination of n objects taken r at a time (where n&gt;r), we have the comb(n,r). The comb() function requires two arguments.\n433  data _null_;\n434  a=comb(3,3);\n435  b=comb(2,5);\n436  c=comb(5,2);\n437  d=comb(5,4);\n438  e=comb(9960,170);\n439  f=comb(1,0);\n440  put _all_;\n441  run;\nThe output is given by:\nNOTE: Argument 2 to function COMB at line 435 column 3 is invalid.\nNOTE: Invalid argument to function COMB at line 438 column 3.\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\na=1 b=. c=10 d=5 e=. f=1 _ERROR_=1 _N_=1\nNOTE: Mathematical operations could not be performed at the following places.\n      The results of the operations have been set to missing values.\n      Each place is given by: (Number of times) at (Line):(Column).\n      1 at 435:3   1 at 438:3\nWe have the logarithmic (natural) counterparts of the above three functions i.e. lfact(), lperm() and lcomb() The output is given below.\n452  data _null_;\n453  a=lfact(10);\n454  b=lperm(10,5);\n455  c=lcomb(10,5);\n456  put _all_;\n457  run;\n\na=15.104412573 b=10.31692083 c=5.5294290875 _ERROR_=0 _N_=1\nNOTE: DATA statement used (Total process time):\n      real time           0.00 seconds\n      cpu time            0.00 seconds"
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html",
    "href": "posts/cs05-evaluationofstationarydemand/index.html",
    "title": "Evaluation of stationary demand models",
    "section": "",
    "text": "Objective\nSelect a suitable model among the given choices."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#check-for-stationary-demand",
    "title": "Evaluation of stationary demand models",
    "section": "Check for stationary demand",
    "text": "Check for stationary demand\nOne way of doing is to determine the coefficient of variation (CV)\n\\[\nCV=\\frac{STDEV(data)}{AVERAGE(data)}\n\\]\nWe get\n\n\n\nSTDEV\n62.6899312\n\n\n\n\nAVERAGE\n1103.78571\n\n\nCV\n0.05679538\n\n\n\nCV is very low & hence the demand is quite stationary & stable in nature."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#calculations-among-the-models",
    "title": "Evaluation of stationary demand models",
    "section": "Calculations among the models",
    "text": "Calculations among the models\n\n\n\n\nPrediction for period 15\nMAPE(%)\n\n\n\n\nPeriod 14 value\n1169\n\n\n\nNaïve\n1169\n7.08446274\n\n\nCumulative\n1103.78571\n5.13422811\n\n\n2MA\n1145\n6.46420824\n\n\n4MA\n1113.25\n4.79251584\n\n\n\nHere we see that the moving average forecasts need not always be between the naïve & cumulative forecasts."
  },
  {
    "objectID": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "href": "posts/cs05-evaluationofstationarydemand/index.html#selection-of-the-model-in-the-presence-of-a-trend",
    "title": "Evaluation of stationary demand models",
    "section": "Selection of the model in the presence of a trend",
    "text": "Selection of the model in the presence of a trend\n\nIf we assume there is a positive trend in the data then none of these models are appropriate for demand with a trend pattern. The Cumulative, Naive, and Moving Average forecasts all assume stationary demand. That means that you only assume a Level pattern to the demand with some random noise."
  },
  {
    "objectID": "posts/01-whatisdatascience/index.html",
    "href": "posts/01-whatisdatascience/index.html",
    "title": "What is ‘data science’ / data analytics? - Yet another opinion",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:52 am\nIn the last twenty years of the Information Age, computer networks & the Internet have allowed us to gather & store information from a wide variety of devices (industrial/medical equipment, photos/audio/video sensors, ATMs, credit/debit cards, mobiles & computers, etc..) & platforms (retailing, telecom, banking & insurance , pharmaceutical, security, social media, etc..).\nThe information being collected is increasing as more people & businesses use computers & the Internet as a primary means for conducting business, social & monetary transactions. Technologies such as 3G/4G data networks, mobile computing devices & cloud based computing systems have accelerated this trend.\n\n“The value of data is no longer in how much of it you have. In the new regime, the value is in how quickly and how effectively can the data be reduced, explored, manipulated and managed.”\nUsama Fayyad – President & CEO of digiMine, Inc. [1]\n\nQuestions to think about:\n\nCan the increasing amount of information be collected, stored & managed in a consistent manner for easy access to others?\nCan information from different sources be integrated & linked to create meaningful relationships between them?\nCan information systems be built to easily get answers to characteristics of stored data?\nCan we detect underlying patterns in the data & “mine” the data to reveal patterns of behavior that will be insightful & profitable in terms of business/research before it loses its value?\nCan analytic processes/models be built to allow prediction of future outcomes/behavior from existing data more quickly than the competition?\nCan it be ensured that business/research returns will be greater than the investment cost of the data collection & analytics system?\n\n\nIt takes brilliance to ask the right questions, at the right time in history. The value of a Big Data resource is that a good analyst can start to see connections between different types of data, and this may prompt the analyst to determine whether there is a way to describe these connections in terms of general relationships among the data objects [2]\n\nIt is important to realize that most of these problems have been discussed [1] & studied in research journals & other industry publications for the last 30-40 years under various labels such as “Business Intelligence”, “Knowledge Discovery”, “Data Mining”, “Decision Science”, “Statistical Learning”, “Predictive Modeling” “Machine Learning”, “Business Forecasting”, etc… Essentially, it is the coming together of data analysis techniques, large scale computing & domain knowledge.\nNow in the present time, all these are under the labels of “Big Data” [2] “Data Science” & “Data Analytics”. The big change now is in the commoditization of technologies where all these techniques can be applied in a cost effective manner & almost in real-time.\nWhile it is not realistically possible for a single person to perform all the above tasks, a new practitioner has emerged who has the relevant knowledge in statistical & data mining techniques, computing & programming techniques as well as domain knowledge of the business/research problem.\nThe main skill required (besides the usual technical knowledge) is curiosity about what patterns exist or can be “mined” from stored data sources as well what can be predicted from it, the ability to experiment with new methods to get new insights & explanations just like a scientist would. Perhaps that is why they call people in this field “data scientists”!\nMy own equation for this “emerging” field would be:\n\nData Science/Data Analytics =\nDomain knowledge of the business/research problem +\nMathematical formulation of business/research problem into a statistical model +\nProgramming the statistical model into software code +\nBusiness/Research analysis of the statistical output\n\nReferences\n[1] HAMPARSUM BOZDOGAN (ed.) Statistical data mining and knowledge discovery (2003) Chapman & Hall/CRC\n[2] JULES J BERMAN Principles of big data: preparing, sharing, and analyzing complex information (2013) Morgan Kaufmann"
  },
  {
    "objectID": "posts/02-howdoigetstarted/index.html",
    "href": "posts/02-howdoigetstarted/index.html",
    "title": "How do I to get started in & learn ‘data science’, ‘data analytics’?",
    "section": "",
    "text": "Created on Friday, February 21st, 2014 at 6:57 am\nThis is an open ended question that has created a lot of discussion on the Web. There are no right answers or approaches. It all depends on what you are interested in & want to achieve. If you choose the self study route, this would be my personal approach.\n1) Decide your domain of interest: The fields of “data analytics”, “data science” are very vast in their scope to learn everything. So although the general statistical & analytic principles are the same for all fields, it is best to find your industry of interest (say pharmaceutical, econometrics, finance, social & bio sciences, human resources, actuarial sciences, marketing, energy forecasting, predictive modeling, business statistical analysis, medical sciences, etc..) & develop domain specific knowledge for your interests. This will allow you to focus your learning & effort. Trying to master too many domains might get you confused & mentally drained in the long run.\n2) Application or technical side: This is a broad generalization. Application side is where the understanding & application is more important than the implementation of the code or the software system. Typically for students of business, social sciences, pharmaceutical, etc…\nUsually understanding & framing the research/business problem required to be solved precedes analysis. The important thing is to know the business impact of your analysis. For example, by changing the values of the variables or doing a “WHAT…IF” analysis, one should be able to interpret the change in output in terms of how it addresses the research/business problem that is required to be solved. This type of skill comes only with having the right domain knowledge.\nTechnical side usually interests students of mathematics, computer science, engineering & fields where focus is on developing new techniques, software & improving existing ones. The fields of data mining & “Big Data” are pretty technical in terms of the mathematical & programming knowledge required. This includes algorithms & equations for data mining, machine learning, pattern recognition, text processing. Database design using “Big Data” technologies like Hadoop, NOSQL, Hive, MongoDb, PIG, MapReduce etc… Also most existing databases like MS SQL Server & Oracle have data mining features.\nAs technical conferences & statistical journals discuss the latest techniques & methods in terms of mathematics, it would be best to get the mathematical background (usually calculus 1 & 2, linear algebra & probability at undergraduate level) as quickly as you can. This will not only enable you to understand but also express yourself in terms of mathematics.\nThe ability to understand the research/business problem & convert it into mathematical form & then choose or create an appropriate algorithmic method, that is relatively fast & with minimum error, for the specific software system are skills you should aim to acquire.\n3) Books & reading material: The books & material you read from should match your level of expertise & also must be based on the software you plan to use. Your best option is to use academic websites, publisher websites & book review websites like Amazon.com to know the contents & the subject matter of the books. There are specialized statistical books for students of social sciences, marketing, computer science, pharmaceutical etc… Also books that teach data analytics/data science/statistics using particular software (such as R, SAS, SPSS, STATA, MS Excel, Python & many more).\nBuy textbooks on the following criteria:\n\nBased on your mathematical level i.e. based on advanced math like calculus or simple math like algebra.\nThat teaches & uses the software you plan to learn with. Using R, SAS, SPSS, STATA, MS Excel, etc….\nThat deal with the techniques & methods for your domain of interest i.e. finance, marketing, pharmaceutical, biostatistics etc…\n\n4) Get the software running: Being familiar with the various functions & features of the software is almost as important as learning statistical theory. You want to be productive & not waste time looking up help/documentation all the time.\nThere are software specifically for statistical analysis. Some are suited for certain domains & industries. The hyperlink below shows a list of them.\nhttp://www.amstat.org/careers/statisticalsoftware.cfm\nThe R project software is the ideal to begin learning with. It is extensive & has many user submitted packages for almost every kind of statistical analysis. It is available for free (as in gratis).\nSince R is distributed under the GPL software license you might need to be familiar with the licensing issues of the various R packages especially if you plan to use R commercially & your code also contains other proprietary code using restrictive licenses.\nMicrosoft Excel is a good option as most office/college computers have machines running MS Windows & MS Office. It has many add-ins (e.g. XLMiner, neuroXL, Oracle Spreadsheet Add-In, Perfringens Predictor Excel Add-in, ADAPA Add-in for Microsoft® Office Excel®, RExcel, DataMinerXL, SAS Add-in for Rapid Predictive Modeler, Palisade Neuraltools Add-in, 11Ants Model Builder Microsoft Excel Add-in, etc.. ) available for analytics & data mining. It can also be used to interact with the data mining features of MS SQL Server.\nCommercial statistical software like SAS, IBM SPSS & STATA are used widely in industry & academia. Those in academia should be able to get an academic license to access & use SAS/SPSS/STATA on their personal computers. SAS also offers an SAS OnDemand service to access SAS through the Internet for a fee.\n5) Programming: Since each software package has its own programming environment. Learning a general programming course at undergraduate level will help you understand programming principles that each software uses. Those technically inclined would do well to do a course in computer algorithms & database design.\nKnowing SQL is important since most of the data you will analyze or process will reside in a database system like MySQL, MS SQL Server, Oracle etc…\nA major part of analytics & data science is modifying existing data into a particular format (especially dates, currency, telephone, number formats) for processing. Every software has built-in features for checking errors & missing values, replacing, searching, sorting, filtering & extracting text from a larger dataset. Text processing tools like grep, perl, and python are also used.\nIt is good idea, although not necessary, to get the basic certifications for commercial software like SAS, SPSS. The exams allow you to brush up your skills & your clients or the company would be somewhat assured of your competency.\nLast but not least, it is best to explore websites & join academic/industry communities specific to your area of research or choice.\nSummary:\n\nDecide on your area of interest & domain.\nDo you prefer to be on the technical/programming side or be application/business oriented?\nGet the right books & study material for your academic level & area of interest.\nDecide on the appropriate software used by your industry.\nLearn programming & algorithms.\nExplore the Internet for websites & join communities specific to your needs."
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "",
    "text": "Objective"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#initial-seasonality-factors",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Initial Seasonality Factors",
    "text": "Initial Seasonality Factors\nThe above diagram clearly points to a seasonality of the sales.\nSince at this point it is not very clear as to whether there is a trend in the data or not, we find use two methods to find the seasonality factors.\n\nAssuming no trend\nWith no trend the seasonality factors (SF) need not be normalized each season.\nSF per period = total sales per shift / (total sales per month/no. of periods)\nAlso\nSF per period = total sales per shift / average no. of sales per period\nMathematically we can express it as\n\\[\nF_{t}=\\frac{\\sum_{t=1}^{n} D}{(\\sum_{t=1}^{n} D_{t})/P}\n\\]\n\n\nCentered Moving Average Method (CMA)\nSince each season has 4 periods, we use 4‐point Centered Moving Average. Here since the season has an even number of points. We need to take the moving average of the season from both sides & then take the final average.\nBelow is a sample of the data used to calculate part of the Fi’s\nMATop is the average of Shift 1,2,3 & 4\nMABottom is the average of Shift 2,3,4 & 5\nMA_Avgi is the average of MATop & MABottom.\nEach Fi is the xi/MA_Avgi except the first two & last two of the time series. The first two & last two Fi are calculated by first & the last MA_Avg values respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period d(t)\nDate\nShift Number\nPints Sold, xi\nMATop\nMABottom\nMA_Avg i\nFi\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n\n\n\n\nNow if the assumption is incorrect & then is a small trend, then the sum of the factors will not add up to number of periods in a season. i.e P = 4 Hence a correction is required in the form and we simply multiply each of your Seasonality Factors by\n\\[\n\\frac{P}{\\sum_{i=1}^{n} F_{i}}\n\\]\nOnce all the Fi are calculated, we average them according to Shift Number. The summary is in the table.\n\n\n\n\n\n\n\n\n\n\n\nTotal Pints Sold\nIf equal sales per shift, pints per shift sold\nRatio of Sales per shift compared with average\n4‐point Moving Centered Averaged Seasonality Factors\n\n\n\n\nEntire Month\n37423\n\n\n\n\n\nShift 1\n13045\n9356\n1.39432969\n1.402007\n\n\nShift 2\n1737\n9356\n0.185661224\n0.185922\n\n\nShift 3\n8700\n9356\n0.929909414\n0.928191\n\n\nShift 4\n13941\n9356\n1.490099671\n1.483154\n\n\n\n\n\n4\n3.999273"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#holtwinter-model-levelseasonalitytrend",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Holt­Winter Model (level+seasonality+trend)",
    "text": "Holt­Winter Model (level+seasonality+trend)\nLevel & Trend:\nRunning a linear regression we get the equation.\ny = 0.812x + 262.6\nFrom the regression equation we get a level of about 263 pints of beer per shift with an trend of 0.8 additional pints per time period. i.e.\nThe regression gives you an estimated level of 265 pints per each shift with a trend of 0.80 additional pints per time period. This means that the sales of beer is increasing about 3.2 pints per day. Hence there is a positive trend trend.\n\n\nSeasonality\nThis involves estimating the initial values of the level and trend “de‐seasoning” the actual demand by the Seasonality Factors we just found. Part of the data used to calculate the normalized seasonality factors.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime Period (t)\nDate\nShift Number\nPints Sold\nMATop\nMABottom\nMA_Avg\nFi\nSUM of each season\nFi Normalized\nNormalized Sum\n—\n\n\n\n\n1\n1‐Jun\n1\n357\n\n\n\n1.363897\n3.957406897\n1.378576309\n4\n\n\n\n2\n1‐Jun\n2\n49\n\n\n\n0.187202\n3.957406897\n0.189216356\n\n\n\n\n3\n1‐Jun\n3\n242\n260\n264\n262\n0.924546\n3.957406897\n0.934497106\n\n\n\n\n4\n1‐Jun\n4\n391\n264\n264\n264\n1.481762\n3.957406897\n1.497710229\n\n\n\n\n5\n2‐Jun\n1\n373\n264\n264\n264\n1.41221\n4.023368199\n1.404007844\n4\n\n\n\n6\n2‐Jun\n2\n50\n264\n269\n266\n0.187705\n4.023368199\n0.186615088\n\n\n\n\n7\n2‐Jun\n3\n243\n269\n269\n269\n0.904607\n4.023368199\n0.89935273\n\n\n\n\n8\n2‐Jun\n4\n408\n269\n269\n269\n1.518846\n4.023368199\n1.510024337\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo taking the average of the all the normalized factors we get,\n\n\n\n\nBefore Normalized\nAfter Normalized\n\n\n\n\n\n\nFs1\n1.402007\n1.402205906\n\n\n\n\nFs2\n0.185922\n0.185977102\n\n\n\n\nFs3\n0.928191\n0.928395676\n\n\n\n\nFs4\n1.483154\n1.483421316\n\n\n\n\nSUM\n3.999273\n4\n\n\n\n\n\n\n\nInitial Parameters\nAssume that Alpha=0.15, Beta=0.06 & gamma = 0.05\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t})\\hat{F}_{t+\\tau-P}\n\\] \\[\n\\hat{a}_{t}=\\alpha\\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right)+(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\n\\] \\[\n\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\n\\] \\[\n\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right)+(1-\\gamma)\\hat{F}_{t-P}\n\\]\nWe have for the period 120, the following initial parameters,\n\n\n\nFs1\n1.402205906\n\n\n\nFs2\n0.185977102\n\n\n\nFs3\n0.928395676\n\n\n\nFs4\n1.483421316\n\n\n\n\\(\\hat{a}_{120}\\)\n360.04\n0.812*(120) + 262.6\n\n\n\\(\\hat{a}_{120}\\)\n0.812\n\n\n\nAlpha\n0.15\n\n\n\nBeta\n0.06\n\n\n\nGamma\n0.05\n\n\n\n\nUsing above data we can start forecasting for the coming periods 122 i.e. July 2 Shift 2\n\n\n\n\n\n\n\n\n\n\n\n\nActual x(t)\n\\(\\hat{a}_{i}\\)\n\\(\\hat{b}_{i}\\)\n\\(\\hat{F}_{i}\\)\n\\(\\hat{x}_{t+4}\\)\n\n\n\n\n120\n557\n360\n0.81\n\n\n\n\n121\n520\n362.3151378\n0.900308265\n1.403856344\n513.694 (for t=125)\n\n\n\nUsing just the 122 forecast, the rest of the periods i.e. 123, 124 & 125 can be calculated using\n\\[\n\\hat{x}_{t,t+\\tau}=(\\hat{a}_{t}+\\tau\\hat{b}_{t}) \\hat{F}_{t+\\tau-P}\n\\]"
  },
  {
    "objectID": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "href": "posts/cs09-forecastingseasonaltrendsblayk/index.html#conclusion",
    "title": "Forecasting with seasonal trends at BLAYK restaurant",
    "section": "Conclusion",
    "text": "Conclusion\nAs observed above, the data about beer consumption follows seasonality & has a positive trend. This can be modeled using the Holt‐Winter Model. Of course, error analysis must be done to tweak the model especially the seasonality factors."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "",
    "text": "DATA SCIENCE SERVICES\nBusiness opportunities from the analysis of customer data\nData science in the enterprise"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#data-as-a-commodity",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#data-as-a-commodity",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.1 Data as a commodity",
    "text": "1.1 Data as a commodity\nThe whole notion of data and the use of data in organizations have changed. Especially for smaller organizations & those that are not software companies.\nThe paradigm change that has occurred is that\n\nData is now a commodity.\nValue creation from data.\nCreating new data products from existing data.\n\n\n“A data application acquires its value from the data itself, and creates more data as a result. It’s not just an application with data; it’s a data product. Data science enables the creation of data products.” 1\n\nSo far all organizations were collecting data & analyzing it to solve specific business/research problems.\n\nHere often only a part of the data was evaluated for specific business or research purposes.\nAlso people did not think about insights from the rest of the related but uncombined data?"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#what-has-caused-this-shift-in-the-value-of-data",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#what-has-caused-this-shift-in-the-value-of-data",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.2 What has caused this shift in the value of data?",
    "text": "1.2 What has caused this shift in the value of data?\n\nMy analogy would be DSP & IC revolution\nEvents after Sep-11\nS.M.A.C -&gt; Social Mobile Analytics Cloud\n\nThis is where most companies missing the paradigm & the opportunities. Most companies especially in the industrial & technical sphere feel that they are not a data company but a hardware, embedded, biomedical, engineering instrumentation company."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#data-science-a-brave-new-world-2",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#data-science-a-brave-new-world-2",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.3 Data Science: a brave new world 2",
    "text": "1.3 Data Science: a brave new world 2\n“Whether it is called data mining, predictive analytics, sense making, or knowledge discovery, the rapid development and increased availability of advanced computational techniques have changed our world in many ways.\nThere are very few, if any, electronic transactions that are not monitored, collected, aggregated, analyzed, and modeled. Data are collected about everything, from our financial activities to our shopping habits. Even casino gambling is being analyzed and modeled in an effort to characterize, predict, or modify behavior.”\n\n1.3.1 Relation between the subjects 3\n\n\n\n1.3.2 Statistical pattern recognition 4\nStatistical pattern recognition is a term used to cover all stages of an investigation from problem formulation and data collection through to discrimination and classification, assessment of results and interpretation.\nIt developed significantly in the 1960s. It was very much an interdisciplinary subject. Approaches for analyzing such data include those for signal processing, filtering, data summarization, dimension reduction, variable selection, regression and classification.\nThe large number of applications ranging from the classical ones such as automatic character recognition and medical diagnosis to the more recent ones in data mining (such as credit scoring, consumer sales analysis and credit card transaction analysis) have attracted considerable research effort.\nWithin these areas significant progress has been made. These developments include, for example, kernel-based methods (including support vector machines) and Bayesian computational methods.\nThe term machine learning describes the study of machines that can adapt to their environment and learn from example. The machine learning emphasis is perhaps more on computationally intensive methods and less on a statistical approach.\nTwo complementary approaches to discrimination\n\nA decision theory approach based on calculation of probability density functions\nThe use of Bayes theorem and a discriminant function approach. (Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership. 5)\n\nTwo approaches to classification\n\nBoth supervised (using class information to design a classifier – i.e. discrimination)\nUnsupervised (allocating to groups without class information – i.e. clustering).\n\nA practical example of pattern recognition that is familiar to many people is classifying email messages (as spam/not spam) based upon message header, content and sender.\n\n\n1.3.3 Knowledge Discovery in Databases (KDD)\n\nFrom Data to Knowledge:6\n\nIn short, KDA is a sequence of six steps, one of which is the data mining step concerned with building the data model. …from understanding of data and preprocessing to deployment of the results.\n\n\nOverview of KDD 7\nHistorically, the notion of finding useful patterns in data has been given a variety of names, including data mining, knowledge extraction, information discovery, information harvesting, data archaeology, and data pattern processing.\nThe phrase knowledge discovery in databases was coined at the first KDD workshop in 1989 (Piatetsky-Shapiro 1991) to emphasize that knowledge is the end product of a data-driven discovery.\nThe interdisciplinary nature of KDD has evolved, and continues to evolve, from the intersection of research fields such as machine learning, pattern recognition, databases, and statistics.\nData mining is the application of specific algorithms for extracting patterns from data. The datamining component of KDD currently relies heavily on known techniques from machine learning, pattern recognition, and statistics to find patterns from data in the data-mining step of the KDD process.\nThe term data mining has mostly been used by statisticians, data analysts, and the management information systems (MIS) communities. It has also gained popularity in the database field.\nBlind application of data-mining methods (rightly criticized as data dredging in the statistical literature) can be a dangerous activity, easily leading to the discovery of meaningless and invalid patterns.\nA natural question is: How is KDD different from pattern recognition or machine learning (and related fields)?\nThe answer is that these fields provide some of the data-mining methods that are used in the datamining step of the KDD process, including how the data are stored and accessed, how algorithms can be scaled to massive data sets ultimate and still run efficiently, how results can be interpreted and visualized, and how the overall man-machine interaction can usefully be modeled and supported.\nThe KDD process can be viewed as a multidisciplinary activity that encompasses techniques beyond the scope of any one particular discipline such as machine learning. Thus, for example, neural networks, although a powerful modeling tool, are relatively difficult to understand compared to decision trees.\nKDD also emphasizes scaling and robustness properties of modeling algorithms for large noisy data sets.\nKnowledge discovery from data is fundamentally a statistical endeavor. Statistics provides a language and framework for quantifying the uncertainty that results when one tries to infer general patterns from a particular sample of an overall population.\nThe term data mining has had negative connotations in statistics since the1960s when computerbased data analysis techniques were first introduced. The concern arose because if one searches long enough in any data set (even randomly generated data), one can find patterns that appear to be statistically significant but, in fact, are not.\nKDD can also be viewed as encompassing a broader view of modeling than statistics. KDD aims to provide tools to automate (to the degree possible) the entire process of data analysis and the statistician’s “art” of hypothesis selection.\nEspecially database techniques for gaining efficient data access, grouping and ordering operations when accessing data, and optimizing queries constitute the basics for scaling algorithms to larger data sets.\nMost data-mining algorithms from statistics, pattern recognition, and machine learning assume data are in the main memory and pay no attention to how the algorithm breaks down if only limited views of the data are possible.\n\n\n\n1.3.4 Data mining 8\nThe aim of data mining is to make sense of large amounts of mostly unsupervised data, in some domain.\nBusinesses are the largest group of DM users, since they routinely collect massive amounts of data and have a vested interest in making sense of the data. Their goal is to make their companies more competitive and profitable.\nIn the above definition, the first key term is to make sense, which has different meanings depending on the user’s experience. Probably the most important requirement is that the discovered new knowledge needs to be understandable to data owners who want to use it to some advantage. The most convenient outcome by far would be knowledge or a model of the data that can be described in easy-to-understand terms, say, via production rules such as:\nIF abnormality (obstruction) in coronary arteries\nTHEN coronary artery disease\nIn the example, the input data may be images of the heart and accompanying arteries. If the images are diagnosed by cardiologists as being normal or abnormal (with obstructed arteries), then such data are known as learning/training data. Some DM techniques generate models of the data in terms of production rules, and cardiologists may then analyze these and either accept or reject them (in case the rules do not agree with their domain knowledge).\nNote, however, that cardiologists may not have used, or even known, some of the rules generated by DM techniques, even if the rules are correct (as determined by cardiologists after deeper examination), or as shown by a data miner to be performing well on new unseen data, known as test data.\nWe then come to the second requirement; the generated model needs to be valid. If, in our example, all the generated rules were already known to cardiologists, these rules would be considered trivial and of no interest, although the generation of the already-known rules validates the generated models and the DM methodology. However, in the latter case, the project results would be considered a failure by the cardiologists (data owners).\nThus, we come to the third requirement associated with making sense, namely, that the discovered knowledge must be novel. Let us suppose that the new knowledge about how to diagnose a patient had been discovered not in terms of production rules but by a different type of data model, say, a neural network. In this case, the new knowledge may or may not be acceptable to the cardiologists, since a neural network is a “black box” model that, in general, cannot be understood by humans. A trained neural network, however, might still be acceptable if it were proven to work well on hundreds of new cases. To illustrate the latter case, assume that the purpose of DM was to automate the analysis (prescreening) of heart images before a cardiologist would see a patient; in that case, a neural network model would be acceptable. We thus associate with the term making sense the fourth requirement, by requesting that the discovered knowledge be useful. This usefulness must hold true regardless of the type of model used (in our example, it was rules vs. neural networks).\nThe other key term in the definition is large amounts of data. DM is not about analyzing small data sets that can be easily dealt with using many standard techniques, or even manually. To give the reader a sense of the scale of data being collected that are good candidates for DM, let us look at the following examples:\n\nAT&T handles over 300 million calls daily to serve about 100 million customers and stores the information in a multiterabyte database.\nWal-Mart, in all its stores taken together handles about 21 million transactions a day, and stores the information in a database of about a dozen terabytes.\nNASA generates several gigabytes of data per hour through its Earth Observing System.\nOil companies like Mobil Oil store hundreds of terabytes of data about different aspects of oil exploration.\nThe Sloan Digital Sky Survey project will collect observational data of about 40 terabytes.\nModern biology creates, in projects like the human genome and proteome, data measured in terabytes and petabytes. Although no data are publiclyd available,\nHomeland Security in the U.S.A. is collecting petabytes of data on its own and other countries’ citizens.\n\nIt is clear that none of the above databases can be analyzed by humans or even by the best algorithms (in terms of speed and memory requirements); these large amounts of data necessarily require the use of DM techniques to reduce the data in terms of both quantity and dimensionality.\nThe third key term in the above definition is mostly unsupervised data. It is much easier, and less expensive, to collect unsupervised data than supervised data. The reason is that with supervised data we must have known inputs corresponding to known outputs, as determined by domain experts. In our example, “input” images correspond to the “output” diagnosis of coronary artery disease (determined by cardiologists – a costly and error-prone process).\nSo what can be done if only unsupervised data are collected? To deal with the problem, one of the most difficult in DM, we need to use algorithms that are able to find “natural” groupings/clusters, relationships, and associations in the data. For example, if clusters can be found, they can possibly be labeled by domain experts. If we are able to do both, our unsupervised data becomes supervised, resulting in a much easier problem to deal with. Finding natural groupings or relationships in the data, however, is very difficult and remains an open research problem. Clustering is exacerbated by the fact that most clustering algorithms require the user a priori to specify (guess) the number of clusters in the data.\nSimilarly, the association-rule mining algorithms require the user to specify parameters that allow the generation of an appropriate number of high-quality associations. Another scenario exists when the available data are semisupervised, meaning that there are a few known training data pairs along with thousands of unsupervised data points. In our cardiology example, this situation would correspond to having thousands of images without diagnosis and only a few images that have been diagnosed. The question then becomes: Can these few data points help in the process of making sense of the entire data set?\nFortunately, there exist techniques of semi-supervised learning which take advantage of these few training data points.\nBy far the easiest scenario in DM is when all data points are fully supervised, since the majority of existing DM techniques are quite good at dealing with such data, with the possible exception of their scalability. A DM algorithm that works well on both small and large data is called scalable, but, unfortunately, few are.\nThe final key term in the definition is domain. The success of DM projects depends heavily on access to domain knowledge, and thus it is crucial for data miners to work very closely with domain experts/data owners. Discovering new knowledge from data is a process that is highly interactive (with domain experts) and iterative. We cannot simply take a successful DM system, built for some domain, and apply it to another domain and expect good results.\nAnother hundreds of available DM algorithms, such as clustering or machine learning, only small numbers of them are scalable to large data.\nHow does Data Mining Differ from Other Approaches?\nData mining came into existence in response to technological advances in many diverse disciplines. For instance, over the years computer engineering contributed significantly to the development of more powerful computers in terms of both speed and memory; computer science and mathematics continued to develop more and more efficient database architectures and search algorithms; and the combination of these disciplines helped to develop the World Wide Web.\nAll the data in the world are of no value without mechanisms to efficiently and effectively extract information and knowledge from them. Early pioneers such as U. Fayyad, H. Mannila, G. PiatetskyShapiro, G. Djorgovski, W. Frawley, P. Smith, and others recognized this urgent need, and the data mining field was born.\nData mining is not just an “umbrella” term coined for the purpose of making sense of data. The major distinguishing characteristic of DM is that it is data driven, as opposed to other methods that are often model driven.\nIn statistics, researchers frequently deal with the problem of finding the smallest data size that gives sufficiently confident estimates. In DM, we deal with the opposite problem, namely, data size is large and we are interested in building a data model that is small (not too complex) but still describes the data well.\nFinding a good model of the data, which at the same time is easy to understand, is at the heart of DM. We need to keep in mind, however, that none of the generated models will be complete (using all the relevant variables/attributes of the data), and that almost always we will look for a compromise between model completeness and model. This approach is in accordance with Occam’s razor:\n\nsimpler models are preferred over more complex ones.\n\nThe users should understand that the application of even a very good tool to one’s data will most often not result in the generation of valuable knowledge for the data owner after simply clicking “run”.\n“Since its genesis in the mid 1990s, data mining has been thought of as encompassing two tasks:\n\nusing data to test some pre-determined hypothesis, or\nusing data to determine the hypothesis in the first place.\n\nThe full automation of both these tasks – hypothesising and then testing – leads to what is known as automated discovery or machine learning.” 9\n\n\n1.3.5 Machine learning:\nThe notion of performance improving with experience is central to most, if not all, forms of machine learning. We will use the following general definition:\n\nMachine learning is the systematic study of algorithms and systems that improve their knowledge or performance with experience. 10\n\n“The goal of machine learning is to teach machines to carry out tasks by providing them with a couple of examples (how to do or not do a task) & let an algorithm come up with the best rule set. The pairs would be your training data, and the resulting rule set (also called model) could then be applied to future data that we have not yet seen.” 11\n“It’s only over the past decade or so that the inherent multi-disciplinarity of machine learning has been recognized. It merges ideas from neuroscience and biology, statistics, mathematics, and physics to make machines learn. …..\nAnother thing that has driven the change in direction in machine learning research is data mining which looks at the extraction of useful information from massive data sets, and which requires efficient algorithms, putting more of the emphasis back onto computer science” 12\n\n\n1.3.6 Predictive analytics 13\nData mining is the discovery of hidden patterns of data through machine learning — and sophisticated algorithms are the mining tools.\nPredictive analytics is the process of refining that data resource, using business knowledge to extract hidden value from those newly discovered patterns.\nData mining + business knowledge = predictive analytics =&gt; value\n\nTwo broad, identifiable branches to predictive analytics 14\n■ Decision analytics has to do with classifying (mainly) people into segments of interest to the analyst. This branch of analytics depends heavily on multivariate statistical analyses, such as cluster analysis and multidimensional scaling. Decision analytics also uses a method called logistic regression to deal with the special problems created by dependent variables that are binary or nominal, such as buys versus doesn’t buy and survives versus doesn’t survive.\n■ Predictive analytics deals with forecasting, and often employs techniques that have been used for decades. Exponential smoothing (also termed exponentially weighted moving averages or EMWA) is one such technique, as is autoregression. Box-Jenkins analysis dates to the middle of the twentieth century and comprises the moving average and regression approaches to forecasting.\nOf course, these two broad branches aren’t mutually exclusive. There’s not a clear dividing line between situations in which you would use one and not the other, although that’s often the case. But you can certainly find yourself asking questions such as these:\n\nI’ve classified my current database of prospects into likely buyers and likely non-buyers, according to demographics such as age, income, ZIP Code, and education level. Can I create a credible quarterly forecast of purchase volume if I apply the same classification criteria to a data set consisting of past prospects?\nI’ve extracted two principal components from a set of variables that measure the weekly performance of several product lines over the past two years. How do I forecast the performance of the products for the next quarter using the principal components as the outcome measures?\n\nSo, there can be overlap between decision analytics and predictive analytics. But not always— sometimes all you want to do is forecast, say, product revenue without first doing any classification or multivariate analysis. But at times you believe there’s a need to forecast the behavior of segments or of components that aren’t directly measurable. It’s in that sort of situation that the two broad branches, decision and predictive analytics, nourish one another.\n\n\nPredictive analytic techniques 15\n\nPredictive analytics is the process of using a set of sophisticated analytic tools to develop models and estimations of what the environment will do in the future. In addition to the preceding definition of predictive analytics, Gartner Research Director Gareth Herschel says this: “Predictive analytics helps connect data to effective action by drawing reliable conclusions about current conditions and future events.”\n\n\nPredictive analytics is data-driven16\nLearning how to predict from data is sometimes called machine learning—but, it turns out, this is mostly an academic term you find used within research labs, conference papers, and university courses .In commercial, industrial, and government applications—in the real-world usage of machine learning to predict—it’s called something else:\nPredictive analytics (PA)— Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\nBuilt upon computer science and statistics and bolstered by devoted conferences and university degree programs, PA has emerged as its own discipline. But, beyond a field of science, PA is a movement that exerts a forceful impact. Millions of decisions a day determine whom to call, mail, approve, test, diagnose, warn, investigate, incarcerate, set up on a date, and medicate. PA is the means to drive per-person decisions empirically, as guided by data. By answering this mountain of smaller questions, PA may in fact answer the biggest question of all:\nHow can we improve the effectiveness of all these massive functions across government, healthcare, business, nonprofit, and law enforcement work?\nIn this way, PA is a completely different animal from forecasting. Forecasting makes aggregate predictions on a macroscopic level.\nWhereas forecasting estimates the total number of ice cream cones to be purchased next month in Nebraska, predictive technology tells you which individual Nebraskans are most likely to be seen with cone in hand.\nPA leads within the growing trend to make decisions more “data driven,” relying less on one’s “gut” and more on hard, empirical evidence.\nEnter this fact-based domain and you’ll be attacked by buzzwords, including analytics, big data, business intelligence, and data science. While PA fits underneath each of these umbrellas, these evocative terms refer more to the culture and general skill sets of technologists who do an assortment of creative, innovative things with data, rather than alluding to any specific technology or method.\nThese areas are broad; in some cases, they refer simply to standard Excel reports—that is, to things that are important and require a great deal of craft, but may not rely on science or sophisticated math. And so they are more subjectively defined.\nAnother term, data mining, is often used as a synonym for PA, but, as an evocative metaphor depicting “digging around” through data in one fashion or another, it is\n\n\nManaging risk is a critical aspect of Decision Management Systems. 17\nThe first real commercial use of predictive analytics was to manage credit risk by predicting the likelihood that a consumer would miss a payment in the immediate future.\nSuddenly there is all this data about who uses what power when. Using predictive analytics to find trends, segments with specific behaviors, and to predict how people might react to specific price changes will become the norm.\nThe process of building mathematical optimization models has similarities with its predictive analytics counterpart, but there are a few major differences worth highlighting to avoid confusion:\n■ Although predictive models are generated by applying an algorithm to a data set, an optimization model is formulated by hand to represent a business problem by defining the decision variables, the objective, and the constraints.\n■ Although the scope and input to a predictive model is often relatively small (such as information about a customer), the scope of an optimization model is usually a complex transaction or a set of transactions.\n■ Predictive analytic models generally require access to large amounts of historical data that can be used to train the model. Optimization models can be run against historical data but do not require it.\n■ Although invoking a predictive model in a Decision Service is relatively fast and simple—it simply involves evaluating a formula or interpreting a decision tree—solving an optimization model can consume significant time and memory, depending on the complexity of the model and size of the data. The optimization model must search a large set of possible actions to determine the one that best fits the constraints and goals.\nOptimization is well established in supply chain problem domains where it is often used to define which products to make on which machines in a factory to maximize the value of products produced given restricted access to the various machines needed to make the products.\nSimilarly, your airplane seat, rental car, and hotel room are all likely to be priced using optimization technology.\n\n\n\nStatistical hypothesis testing 18\nA preliminary study may suggest that customers in the Northeast have a churn rate of 22.5%, whereas the nationwide average churn rate is only 15%. This may be just a chance fluctuation since the churn rate is not constant; it varies over regions and over time, so differences are to be expected. But the Northeast rate is one and a half times the U.S. average, which seems unusually high.\n\nWhat is the chance that this pattern or phenomenon is due to random variation? Statistical hypothesis testing is used to answer such questions.\n\n\n\nData science 19\nThe statistician William S. Cleveland defined data science as an interdisciplinary field larger than statistics itself.\n\nWe define data science as managing the process that can transform hypotheses and data into actionable predictions.\n\nTypical predictive analytic goals include predicting who will win an election, what products will sell well together, which loans will default, or which advertisements will be clicked on.\nThe data scientist is responsible for acquiring the data, managing the data, choosing the modeling technique, writing the code, and verifying the results.\n\nData science and its relationship to Big Data and data-driven decision making 20\n\nCompanies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz.\n\nWe can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important\n\n\nto understand its relationships to these other important and closely related concepts,\nto begin to understand what are the fundamental principles underlying data science.\n\n\nAt a high level, data science is a set of fundamental principles that support and guide the principled extraction of information and knowledge from data.\nData science is viewed as the connective tissue between data-processing technologies (including those for ‘‘big data’’) and data-driven decision making.\nProbably the broadest business applications are in marketing for tasks such as targeted marketing, online advertising, and recommendations for cross-selling. Data science also is applied for general customer relationship management to analyze customer behavior in order to manage attrition and maximize expected customer value. The finance industry uses data science for credit scoring and trading and in operations via fraud detection and workforce management\nA data-science perspective provides practitioners with structure and principles, which give the data scientist a framework to systematically treat problems of extracting useful knowledge from data.\nBacked by the trillions of bytes’ worth of shopper history that is stored in Wal-Mart’s data warehouse, she felt that the company could ‘‘start predicting what’s going to happen, when Hurricane Charley struck, several weeks earlier, instead of waiting for it to happen,’’ as she put it.\nThe New York Times reported that: ‘‘.the experts mined the data and found that the stores would indeed need certain products— and not just the usual flashlights. ‘We didn’t know in the past that strawberry Pop-Tarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,’ Ms. Dillman said in a recent interview.’ And the pre-hurricane top-selling item was beer.’’’2\nHow should MegaTelCo decide on the set of customers to target to best reduce churn for a particular incentive budget?\nOne standard deviation higher on the DDD scale is associated with a 4–6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.\nOur two example case studies illustrate two different sorts of decisions: (1) decisions for which ‘‘discoveries’’ need to be made within data, and (2) decisions that repeat, especially at massive scale, and so decision making can benefit from even small increases in accuracy based on data analysis. The Wal-Mart example above illustrates a type-1 problem. Linda Dillman would like to discover knowledge that will help Wal-Mart prepare for Hurricane Frances’s imminent arrival. Our churn example illustrates a type-2 DDD problem.\nA large telecommunications company may have hundreds of millions of customers, each a candidate for defection. Tens of millions of customers have contracts expiring each month, so each one of them has an increased likelihood of defection in the near future. If we can improve our ability to estimate, for a given customer, how profitable it would be for us to focus on her, we can potentially reap large benefits by applying this ability to the millions of customers in the population. This same logic applies to many of the areas where we have seen the most intense application of data science and data mining:\nDirect marketing, online advertising, credit scoring, financial trading, help-desk management, fraud detection, search ranking, product recommendation, and so on.\nThe use of big data technologies correlates with significant additional productivity growth. Specifically, one standard deviation higher utilization of big data technologies is associated with 1–3% higher productivity than the average firm; one standard deviation lower in terms of big data utilization is associated with 1–3% lower productivity. This leads to potentially very large productivity differences between the firms at the extremes.\nSimilarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the forefront, providing data-driven recommendations from massive data. There are other examples as well. Online advertisers must process extremely large volumes of data (billions of ad impressions per day is not unusual) and maintain a very high throughput (real-time bidding systems make decisions in tens of milliseconds).\nManagers and line employees in other functional areas will only get the best from the company’s datascience resources if they have some basic understanding of the fundamental principles.\nFacebook, Twitter, Yahoo, Google, Amazon along with many other ‘‘Digital 100’’ companies,5 have high valuations due primarily to data assets they are committed to capturing or creating.\nFundamentals concepts of data science:\n\nExtracting useful knowledge from data to solve business problems can be treated systematically by following a process with reasonably well-defined stages. The Cross-Industry Standard Process for Data Mining (CRISP-DM)\nEvaluating data-science results requires careful consideration of the context in which they will be used: For our churn-management example, how exactly are we going to use the patterns that are extracted from historical data? More generally, does the pattern lead to better decisions than some reasonable alternative?\nThe relationship between the business problem and the analytics solution often can be decomposed into tractable subproblems via the framework of analyzing expected value. We have many specific tools for estimating probabilities and values from data. For our churn example, should the value of the customer be taken into account in addition to the likelihood of leaving?\nInformation technology can be used to find informative data items from within a large body of data: In our churn example, a quantity of interest is the likelihood that a particular customer will leave after her contract expires. Before the contract expires, this would be an unknown quantity. However, there may be known data items (usage, service history, how many friends have canceled contracts) that correlate with our quantity of interest. This fundamental concept underlies a vast number of techniques for statistical analysis, predictive modeling, and other data mining.\nEntities that are similar with respect to known features or attributes often are similar with respect to unknown features or attributes.\nIf you look too hard at a set of data, you will find something—but it might not generalize beyond the data you’re observing. This is referred to as ‘‘overfitting’’ a dataset.\nTo draw causal conclusions, one must pay very close attention to the presence of confounding factors, possibly unseen ones.\n\nFor example, it is common to see job advertisements mentioning data-mining techniques (random forests, support vector machines), specific application areas (recommendation systems, ad placement optimization), alongside popular software tools for processing big data (SQL, Hadoop, MongoDB)."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#where-is-the-concept-of-data-science-heading",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#where-is-the-concept-of-data-science-heading",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.4 Where is the concept of data science heading?",
    "text": "1.4 Where is the concept of data science heading?\n“Google is not really a search company. It’s a machine-learning company,” says Matthew Zeiler, the CEO of visual search startup Clarifai, who worked on Google Brain during a pair of internships. He says that all of Google’s most-important projects—autonomous cars, advertising, Google Maps—stand to gain from this type of research. “Everything in the company is really driven by machine learning.” 21\n\nMachine learning applications for data center efficiency optimization 22\n“The sheer number of possible equipment combinations and their setpoint values makes it difficult to determine where the optimal efficiency lies,” Gao writes in the white paper on his initiative. “In a live DC, it is possible to meet the target setpoints through many possible combinations of hardware (mechanical and electrical equipment) and software (control strategies and setpoints). Testing each and every feature combination to maximize efficiency would be unfeasible given time constraints, frequent fluctuations in the IT load and weather conditions, as well as the need to maintain a stable DC environment.”\n“Google’s Search engine has tried to approximate human intelligence by rapidly analyzing enormous amounts of data, but people like LeCun (from Facebook) aim to build massive “neutral networks” that actually mimic the way the brain works.”[^23]\n“Vern Brownell, D-Wave’s CEO, talks about the company’s ongoing efforts to prove the potential of its hardware and its plans for the future … third category is in the broad area of machine learning, which we think is one of the most exciting things going on in computer science today.”23\n“In 2000, physicist Alexei Kitaev (then at Microsoft Research) proposed that a mysterious quasiparticle known as a Majorana could be used in quantum information processing, showing that Majoranas located at opposite ends of a quantum wire could effectively create a topologically protected qubit. Five years later, Das Sarma – along with Freedman and Chetan Nayak, Station Q’s other main leader – co-authored a paper suggesting an experimental proposal for creating a topologically protected qubit using something called the “fractional quantum Hall” system (twodimensional electron gas in a strong magnetic field) along with a similar quasiparticle. These important discoveries pointed to a promising new direction for protecting qubits, and therefore getting them to behave. After all, qubits working together in harmony is fundamental to getting them to compute.” 24\n“Viv is not the only company competing for a share of those billions. The field of artificial intelligence has become the scene of a frantic corporate arms race, with Internet giants snapping up AI startups and talent. …Their goal is to build a new generation of AI that can process massive troves of data to predict and fulfill our desires.” 25\nGoogle 3D phone/Amazon 3D phone. IBM Neural chip."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#some-of-the-current-trends.",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#some-of-the-current-trends.",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.5 Some of the current trends.",
    "text": "1.5 Some of the current trends.\n\nJuly 2009 - IBM to buy SPSS(statistics software vendor) for $1.2 Billion to gain analytics\nMar 2011 - Via Science Acquires Dataspora, a pioneer in predictive analytics to leverage proprietary machine learning platform, REFS™, beyond healthcare and financial services.\nOct 2013 - SAP buys KXEN(statistics software vendor) to further predictive analytics\nFeb 2014 – Workday acquires HR predictive analytics company Identified\nMar 2014 - Dell acquires StatSoft (statistics software vendor) and the list of predictive platform vendors gets even shorter… by Simon Arkell, CEO, Predixion Software\nJun 2014 - Nokia’s HERE mapping service continued its buying spree with today’s acquisition of real-time predictive analytics firm Medio Systems\nJul 2014 - Twitter acquires image search firm Madbits (uses deep learning techniques to understand the content of an image)"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#what-would-typical-application-areas-of-data-science-be",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#what-would-typical-application-areas-of-data-science-be",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "1.6 What would typical application areas of data science be?",
    "text": "1.6 What would typical application areas of data science be?\n\nMicrosoft Machine Learning software\nOSISoft is working with Carnegie Mellon University on real time fault detection and the diagnosis of energy output variations across campus buildings. Machine learning is helping to mitigate issues in real time and to predictably optimize energy usage and cost.\nGitHub: sebastianbk/BreastCancerNeuralNetwork Implementation of a Neural Network in .NET using the Diagnostic Wisconsin Breast Cancer Database. After completing the example with the Breast Cancer data set by coding it myself, I thought of using Azure Machine Learning to do the same job.\nKaggle, the leading platform for predictive modeling competitions:\n\nUPenn and Mayo Clinic’s Seizure Detection Challenge: Detect seizures in intracranial EEG recordings\nThe Heritage Provider Network (HPN): The goal of the prize is to develop a predictive algorithm that can identify patients who will be admitted to a hospital within the next year, using historical claims data.\nDunnhumby, a U.K. firm that does analytics for supermarket chains, was looking to build a model to predict when supermarket shoppers will next visit the store and how much they will spend.\nBoehringerIngelheimBioSciences: Predict a biological response of molecules from their chemical properties as optimally as this data allows, relate molecular information, to an actual biological response.\nJetpac is an online travel magazine startup: Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as ‘good’.\nAllState Insurance: The goal of the Claim Prediction Challenge was to predict bodily injury liability, based solely on the characteristics of the insured vehicle.\n\n\n\nIBM Life Sciences\nRicardo Machado, (IBM, Brazil) published many papers on neural networks and a predictive expert system named Next. The power of this system stemmed from its ability to use “knowledge graphs” obtained from interviews with medical experts to form the basis of a model capable of altering these graphs when presented with data, thus transforming them into an artificial neural network. Next was successfully used to diagnose and classify kidney diseases.\nBeatriz Leao (IBM, Brazil), developed a system called HYCONES, which also combined symbolic knowledge and neural networks. It was able to successfully detect and classify congenital heart diseases. The results of the work were published in M.D. Computing in 1994.\n\n\nStanford Machine Learning Projects (CS229)\n\nCharacterizing and diagnosing hypertrophic cardiomyopathy from ECG data.\nElectrical energy modeling in Y2E2 building based on distributed sensors information.\nPredicting semantic features from CT images of liver lesions using deep learning.\nMachine learning classification of kidney and lung cancer types.\nGaussian process based image segmentation and object detection in pathology slides.\nListen to your heart: stress prediction using consumer heart rate sensors."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#top-ten-algorithms-in-data-mining-27",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#top-ten-algorithms-in-data-mining-27",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.1 Top ten algorithms in data mining 26",
    "text": "2.1 Top ten algorithms in data mining 26\n\nC4.5\nK-Means\nSVM: Support Vector Machines\nApriori\nExpecation Maximization\nPageRank\nAdaBoost\nkNN: k-Nearest Neighbors.\nNaïve Bayes\nCART: Classification and Regression Trees"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#microsoft-sql-server-analysis-services-ssas-algorithms",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#microsoft-sql-server-analysis-services-ssas-algorithms",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms",
    "text": "2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms\n\n\n\n\n\n\n\nExamples of tasks\nMicrosoft algorithms to use\n\n\n\n\nPredicting a discrete attribute - Flag the customers in a prospective buyers list as good or poor prospects. - Calculate the probability that a server will fail within the next 6 months.- Categorize patient outcomes and explore related factors.\n* Decision Trees Algorithm * Naive Bayes Algorithm * Clustering Algorithm * Neural Network Algorithm\n\n\nPredicting a continuous attribute - Forecast next year’s sales. - Predict site visitors given past historical and seasonal trends. - Generate a risk score given demographics.\n* Decision Trees Algorithm * Time Series Algorithm * Linear Regression Algorithm\n\n\nPredicting a sequence - Perform clickstream analysis of a company’s Web site. - Analyze the factors leading to server failure. - Capture and analyze sequences of activities during outpatient visits, to formulate best practices around common activities.\n* Sequence Clustering Algorithm\n\n\nFinding groups of common items in transactions - Use market basket analysis to determine product placement. - Suggest additional products to a customer for purchase. - Analyze survey data from visitors to an event, to find which activities or booths were correlated, to plan future activities.\n* Association Algorithm * Decision Trees Algorithm\n\n\nFinding groups of similar items - Create patient risk profiles groups based on attributes such as demographics and behaviors. - Analyze users by browsing and buying patterns. - Identify servers that have similar usage characteristics.\n* Clustering Algorithm * Sequence Clustering Algorithm"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#madlib-algorithms",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#madlib-algorithms",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.3 MADlib algorithms",
    "text": "2.3 MADlib algorithms\nKey philosophies driving the architecture of MADlib are:\n\nOperate on the data locally—in database. Do not move it between multiple runtime environments unnecessarily.\nUtilize best of breed database engines, but separate the machine learning logic from database specific implementation details.\nLeverage MPP Share nothing technology, such as the Pivotal Greenplum Database, to provide parallelism and scalability.\nOpen implementation maintaining active ties into ongoing academic research.\n\n\n\n\n\n\n\n\nTechnique\nExample Use Cases\n\n\n\n\nLinear Regression\ncan be used to model a linear relationship of a scalar dependent variable to one or more explanatory independent variables.\n\n\nLatent Dirichlet Allocation\nis a topic modeling function used to identify recurring themes in a large document corpus.\n\n\nSummary Function\nprovides summary statistics for any data table. These statistics include statistics such as: number of distinct values, number of missing values, mean, variance, min, max, most frequent values, quantiles, etc.\n\n\nLogistic Regression\ncan be used to predict a binary outcome of a dependent variable from one or more explanatory independent variables.\n\n\nElastic Net Regularization\nis a regularization technique that can be implemented for either linear or logistic regression to help build a more robust model in the event of large numbers of explanatory independent variables.\n\n\nPrincipal Component Analysis\nis a dimensional reduction technique that can be used to transform a high dimensional space into a lower dimensional space.\n\n\nApriori\nis a technique for evaluating frequent item-sets, which allows analysis of what events tend to occur together. For instance what items customers frequently purchase in a single transaction.\n\n\nk-Means Clustering\nis a clustering method used to identify regions of similarity within a dataset. It can be used for many types of analysis including customer segmentation analysis."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#oracle-advanced-analytics",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#oracle-advanced-analytics",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.4 Oracle advanced analytics",
    "text": "2.4 Oracle advanced analytics\n\n\n\nTechnique\nApplicability\nAlgorithms\n\n\n\n\n./media/image9.jpg Classification\nMost commonly used technique for predicting a specific outcome such as response / no-response, high / medium / low-value customer, likely to buy / not buy.\n* Logistic Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Naive Bayes —Fast, simple, commonly applicable * Support Vector Machine—Next generation, supports text and wide data. Decision Tree —Popular, provides human-readable rules.\n\n\n./media/image10.jpg Regression\nTechnique for predicting a continuous numerical outcome such as customer lifetime value, house value, process yield rates.\n* Multiple Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Support Vector Machine\n\n\n./media/image11.jpg Attribute Importance\nRanks attributes according to strength of relationship with target attribute. Use cases include finding factors most associated with customers who respond to an offer, factors most associated with healthy patients.\n* Minimum Description – Considers each attribute as a simple predictive model of the target classLength—\n\n\n./media/image12.jpg Anomaly Detection\nIdentifies unusual or suspicious cases based on deviation from the norm. Common examples include health care fraud, expense report fraud, and tax compliance.\n* One-Class Support Vector Machine — Trains on “normal” cases to flag unusual cases\n\n\n./media/image13.jpg Clustering\nUseful for exploring data and finding natural groupings. Members of a cluster are more like each other than they are like members of a different cluster. Common examples include finding new customer segments, and life sciences discovery.\n* Enhanced K-Means—Supports text mining, hierarchical clustering, distance based * Orthogonal Partitioning Clustering— Hierarchical clustering, density based. * Expectation Maximization—Clustering technique that performs well in mixed data (dense and sparse) data mining problems.\n\n\n./media/image14.jpg Association\nFinds rules associated with frequently co-occurring items, used for market basket analysis, cross-sell, root cause analysis. Useful for product bundling, instore placement, and defect analysis.\n* Apriori—Industry standard for market basket analysis\n\n\n./media/image15.jpg Feature Selection and Extraction\nProduces new attributes as linear combination of existing attributes. Applicable for text data, latent semantic analysis, data compression, data decomposition and projection, and pattern recognition.\nNon-negative Matrix Factorization— Next generation, maps the original data into the new set of attributes. * Principal Components Analysis (PCA)— creates new fewer composite attributes that represent all the attributes. * Singular Vector Decomposition— established feature extraction method that has a wide range of applications."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#apache-mahout-for-hadoop",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#apache-mahout-for-hadoop",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.5 Apache Mahout for Hadoop",
    "text": "2.5 Apache Mahout for Hadoop\nThe Apache Mahout project’s goal is to build a scalable machine learning library. There is some degree of overlap with big data analytics within a Hadoop\nMahout includes algorithms for clustering, classification and collaborative filtering. You can also find:\n\nMatrix factorization based recommenders\nK-Means, Fuzzy K-Means clustering\nLatent Dirichlet Allocation\nSingular Value Decomposition\nLogistic regression classifier\n(Complementary) Naive Bayes classifier\nRandom forest classifier"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#microsoft-azure-cloud-machine-learning-studio",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#microsoft-azure-cloud-machine-learning-studio",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.6 Microsoft Azure cloud machine learning studio",
    "text": "2.6 Microsoft Azure cloud machine learning studio\nThere is a pool of VMs running machine learning algorithms using an orchestration engine, freeing the data scientist from moving data and moving to different services.\nThe ML Studio is targeting the emerging data scientists. You can train 10 models in minutes, not days. You can put a predictive model into production in minutes, not weeks or months. Some customers are reporting a 10X-100X in reduction in cost relative to SAS.\nEmployees can create their own workspaces, giving re-use and cross-teaming and sharing models with others.\nThe predictive models can be shared as a service across an enterprise leverage Azure as the public cloud back-end. For example, you can write JSON-based back ends that leverage your predictive models, allowing you to build decision making dashboards for your business.\nMachine Learning algorithms are built to continually improve over time by leverage training sets. Training sets make it possible to continually improve the robustness of your predictive model.\nThe good news is that R is easily integrated into ML Studio. Right now, R is dominant in machine learning space."
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#google-prediction-api",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#google-prediction-api",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.7 Google Prediction API",
    "text": "2.7 Google Prediction API\nGoogle’s cloud-based machine learning tools can help analyze your data to add the following features to your applications:\n\nCustomer sentiment analysis\n\nSpam detection\n\nMessage routing decisions\n\nUpsell opportunity analysis\n\nDocument and email classification\n\nDiagnostics\n\nChurn analysis\n\nSuspicious activity identification\n\nRecommendation systems\n\nAnd much more…"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#implementation-options",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#implementation-options",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "2.8 Implementation options",
    "text": "2.8 Implementation options\n\n2.8.1 Hardware\nOffline: Stand-Alone machine, private cloud.\nOnline: shared hosting, VPS, Virtual Machine\nHosting:1000/month\nVPS: 2000-5000/month\nCloud: AzureML, Google Predictive API, Amazon, Oracle DM, ADAPA Cloud, FICO Cloud, SAS Cloud, Mathematica Cloud\n\n\n2.8.2 Software\nOS: Windows. Linux/UNIX:\nDatabases: SQL Server, Oracle DM, MySQL, MariaDB, Hadoop, NOSQL\nClient End: Excel Add-ins\nAnalysis Packages: MS Excel, SAS, SPSS [ibm], Statistica [dell], STATA, KXEN [sap]\nMADLIB ~ SQL library for databases\nLanguages: sql, python, perl, awk, sed, vba, R, C#, F#, julia\nGraphics/Visualization: 2d/3d ggplot2\n\nPaid: SAS (best), IBM SPSS, Statistica, Stata, MS Excel (most versatile), MS SQL Server Analysis Services (Built-in Data mining Algorithms implemented as SQL based queries & GUI tools), Oracle Data Mining (ODM), Predixion Tools.\nOSS: WekaGUI (Machine Learning), KNIME, R, Python etc….\n\n\n\n2.8.3 Standards\nCRISP-DM (Cross Industry Standard Process for Data Mining): Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment\nSEMMA (Sample, Explore, Modify, Model and Assess) by SAS Inc.\nPMML (Predictive Model Markup Language) – can exchange predictive models across software using XML. Zemetis, The Data Mining Group (dmg.org)\nCONCLUSION"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#why-even-bother",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#why-even-bother",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "3.1 Why even bother?",
    "text": "3.1 Why even bother?\n\nCAPABILITY"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#future-trends",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#future-trends",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "3.2 Future trends?",
    "text": "3.2 Future trends?\n\nThe big players of the internet are investing in deep learning, AI & quantum computing to deal with overflowing data.\nAll technologies considered hi-tech or cutting-edge are becoming commercialized for normal businesses.\nNovell Computers or IBM Personal computers were very specialized services. But now are commoditized.\nNow we have a situation where data sets are getting larger and the software to do advanced analysis & create prediction models from the vast amounts of data is getting cheaper (SAS).\nSmaller firms, individuals & startups are getting a level playing field in terms of cost of operations, renting cloud, machines, people (expertise is flowing from research labs to engineering & design depts. of companies.) etc…\n“20-25% of outsourcing contracts now relating to Cloud; IT services companies that don’t invest in Cloud may be making a mistake, say experts”27"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#business-opportunities-from-the-analysis-of-customer-data.",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#business-opportunities-from-the-analysis-of-customer-data.",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "3.3 Business opportunities from the analysis of customer data.",
    "text": "3.3 Business opportunities from the analysis of customer data.\nThey include:\n\nBasic reporting & analysis\nAdvanced/premium/value-added services\nEngineering, quality control of production, manufacturing.\nWear/Tear, Performance of sensors / devices. • Optimization of resources for customers\nSell raw data to bigger firms.\nSell value added data to bigger firms.\nSell data services to bigger firms.\nGet contracts from bigger firms.\nCreate a marketplace for sensors with advanced analysis in the commercial, industrial or consumer space.\nIntellectual property creation\n\nFor developing countries, it is a wide open market. They have basic electronic infrastructure. Most of the infrastructure in not “smart” or even standardized for advanced data analysis & automated solutions. Less competition especially in the industrial, hardware, electronics, and engineering domain.\nData science in the enterprise\n© 2014 by data science services"
  },
  {
    "objectID": "posts/12-briefoverviewprinciplesgeoint/index.html#footnotes",
    "href": "posts/12-briefoverviewprinciplesgeoint/index.html#footnotes",
    "title": "12-briefoverviewprinciplesgeoint",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMIKE LOUKIDES What is data science? The future belongs to the companies and people that turn data into products. (June, 2010) radar.oreilly.com/2010/06/what-is-data-science.html↩︎\nCOLLEEN MCCUE Data Mining and Predictive Analysis (2007) Elsevier Inc1.↩︎\nF. VAN DER HEIJDEN Classification, parameter estimation and state estimation: an engineering approach using MATLAB (2004) John Wiley & Sons Ltd.↩︎\nANDREW R. WEBB AND KEITH D. COPSEY Statistical Pattern Recognition, Third Edition (2011) John Wiley & Sons, Ltd↩︎\nS.B. GREEN, N. J. SALKIND & T. M. AKEY Using SPSS for Windows and Macintosh: Analyzing and understanding data (2008) Prentice Hall. New Jersey↩︎\nKRZYSZTOF J. CIOS Data Mining: A Knowledge Discovery Approach (2007) Springer Science+Business Media, LLC↩︎\nUSAMA FAYYAD, GREGORY PIATETSKY-SHAPIRO AND PADHRAIC SMYTH “From Data Mining to Knowledge Discovery in Databases” AI Magazine (1996), vol. 17 No.3, pp.37—54↩︎\nKRZYSZTOF J. CIOS Data Mining: A Knowledge Discovery Approach (2007) Springer Science+Business Media, LLC↩︎\nMOHAMED MEDHAT GABER (Editor) Scientific Data Mining and Knowledge Discovery: Principles and Foundations (2010) Springer-Verlag Berlin Heidelberg↩︎\nPETER FLACH Machine learning: The Art and Science of Algorithms that Make Sense of Data (2012) Cambridge University Press, New York↩︎\nWILLI RICHERT Building Machine Learning Systems with Python (2013) Packt Publishing↩︎\nSTEPHEN MARSLAND Machine Learning: An Algorithmic Perspective (2009) CRC Press↩︎\nANASSE BARI Predictive Analytics for Dummies (2013) Wiley↩︎\nCONRAD CARLBERG Predictive Analytics with Microsoft Excel (2013) QUE↩︎\nMICHAEL WESSLER Predictive Analytics For Dummies, Alteryx Special Edition (2014) Wiley↩︎\nERIC SIEGEL Predictive Analytics: The Power to Predict who will click, buy, lie or die (2013)↩︎\nJAMES TAYLOR Decision Management Systems - A Practical Guide to Using Business Rules and Predictive Analytics (2012) IBM Press↩︎\nFOSTER PROVOST AND TOM FAWCETT Data Science for Business (2013) O’Reilly Media, Inc↩︎\nNINA ZUMEL Practical Data Science with R (2014) Manning Publications↩︎\nFOSTER PROVOST AND TOM FAWCETT “Data Science and its Relationship to Big Data and Data-Driven Decision Making” Big Data (March 2013), 1(1): 51-59.↩︎\nROBERT MCMILLAN “Inside the Artificial Brain That’s Remaking the Google Empire” Wired Magazine (07.16.14) www.wired.com/2014/07/google_brain/↩︎\nwww.datacenterknowledge.com/archives/2014/05/28/google-using-machine-learning-boost-data-center-efficiency/2/↩︎\nRACHEL COURTLAND “D-Wave Aims to Bring Quantum Computing to the Cloud” (9 Apr 2014) IEEE Spectrum http://spectrum.ieee.org/podcast/computing/hardware/dwave-aims-to-bring-quantum-computing-to-the-cloud↩︎\nwww.microsoft.com/en-us/news/stories/stationq/index.html↩︎\nSTEVEN LEVY “Siri’s Inventors Are Building a Radical New AI That Does Anything You Ask” Wired Magazine (08.12.14) www.wired.com/2014/08/viv/↩︎\nXINDONG WU AND VIPIN KUMAR (eds.) The Top Ten Algorithms in Data Mining (2009) Chapman and Hall/CRC Press↩︎\nITIKA SHARMA PUNIT “Cloud: The reality that enterprises cannot escape” Business Standard Newspaper Bangalore Edition (July 7, 2014)↩︎"
  },
  {
    "objectID": "posts/cs02-performancecharacteristics/index.html",
    "href": "posts/cs02-performancecharacteristics/index.html",
    "title": "Performance characteristics of forecasting models",
    "section": "",
    "text": "Objective\nInvestigate existing forecasting capabilities of Ordroid devices & provide suggestions.\n\nIntroduction\nYou have just been hired by a company that manufactures mid‐range communication devices that use the Ordroid open source operating system. The company is focused on innovating its products and has not put much thought on its inventory or forecasting capabilities. Your boss thinks there might be a problem in the forecasting of the Ordroid Devices and wants you to figure it out. The Ordroid, far from being new to the market, has been out for two years.\nKnowing this, you have asked for data on both years of historical sales as well as any forecasts, promotions, pricing changes, or competitive analyses made during this time. Your boss laughs and provides you with all the data they have: the last six months of sales. You ask to meet with the current demand planner for the Ordroid Devices and she tells you that they use a forecasting algorithm of her own design and there is no documentation.\n\n\nVisualization of the raw data\nRaw data & forecasts supplied by the demand planner at Ordroid Devices\n\nCalculate some different performance characteristics for the data sample given.\n\\[\nMD=\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})}{n}\n\\] \\[\nMAD=\\frac{\\sum_{i=1}^{n} \\lvert Actual_{i}-Forecast_{i}\\rvert}{n}\n\\] \\[\nRMSE=\\sqrt{\\frac{\\sum_{i=1}^{n} (Actual_{i}-Forecast_{i})^2}{n}}\n\\] \\[\nMPE=\\frac{\\sum_{i=1}^{n} \\frac{ (Actual_{i}-Forecast_{i})}{Actual_{i}}}{n}\n\\] \\[\nMAPE=\\frac{\\sum_{i=1}^{n} \\lvert \\frac{Actual_{i}-Forecast_{i}}{Actual_{i}}\\rvert}{n}\n\\]\n\n\n\n\n\n\nNumber of devices\n\n\n\n\nMean Deviation\nMD\n112.5\n\n\nMean Absolute Deviation\nMAD\n509.5\n\n\nRoot Mean Square Error\nRMSE\n540.6115\n\n\nMean Percent Error\nMPE\n0.04112\n\n\nMean Absolute Percent Error\nMAPE\n0.269002\n\n\n\n\nQ1. What can you say about the presence seasonality of demand?\nSeasonality requires a whole cycle. There is not even one full year of data. So as of now, it is too early to fully evaluate seasonality. You need at least two full cycles to determine seasonality.\nQ2. What can you say about the presence of a trend in the demand?\nAlthough we don’t have a year’s worth of data, there seems to be a positive trend of about 10% increase in demand in the data or about 171 devices per month.\n\nQ3. What can you say about the bias of the forecast?\nA bias is a persistent tendency to over or under predict. These forecasts are not persistent in either. In fact, of the six periods, half are over forecast and half are under forecast. So, there does not appear to be any bias in the forecast.\nQ4. What can you say about the accuracy of the forecast?\nThis is not a very good forecast because even though there is a strong positive trend, the forecasts ignores the trend & also the MAPE is almost 27% ‐ quite high."
  },
  {
    "objectID": "posts/13-questionstoaskentrepreneurs/index.html",
    "href": "posts/13-questionstoaskentrepreneurs/index.html",
    "title": "Some questions to ask entrepreneurs",
    "section": "",
    "text": "Based on MITx bootcamp notes\n\nPart 1 - Basic questions:\nQ01. What is the problem you want to solve?\nQ02. Who experiences the problem?\nQ03. How do you want to solve this problem?\nQ04. Why is this a better solution?\nQ05. If you could describe your product in about 10 words without using anything fancy. How would you say it?\nQ06. What is the one thing you feel you can do (for your customer) better than everyone else?\n\n\nPart 2 - Business Plan:\nQ07. Why did you pick up this particular field of all the other things to solve?\nQ08. What are the many business opportunities do you see in this field?\nQ09. What skills do you need to learn to pursue these opportunities?\nQ10. And with people of what skills, strengths, and interests would you like to collaborate in that pursuit?\n\n\nPart 3 - Market Segmentation:\nQ11. What are you top 3 markets (e.g. educational, aerospace, medical etc… ) for your business?\nQ12. Did you do primary customer research? If so, what was the main feedback given?\nQ13. In what way did you change your business model as a result of this feedback?\n\n\nPart 4 - Beachhead market: initial market segment that is easy to grow & profit\nQ14. What do you think is your beachhead market? (e.g. small biz.,schools, houses, retail etc…)\nQ15. Is there competition that could block you from getting this business of this market?\nQ16. What is your plan to deal with them?\nQ17. If you win this market will it help you to win other market segments?\n\n\nPart 5 - End user profile: Common characteristics among all your customers\nQ18. Have you profiled your customers in terms of demographics, their motivations for solving their problem & also socio-economic profiles?\nQ19. Do you have the unique characteristics of your customers that you can use to identify other customers?\nQ20. How many such kind of customers are there in your beachhead market?\n\n\nPart 6 - TAM (Total addressable market): Total revenue from your beachhead market.\nTAM = Total no. of customers possible * Avg. Revenue per customer per Year.\nQ21. Do you have any idea of your TAM size? Can you capture 100% of it?\nQ22. How much percent can you achieve in the next 5 years?\n\n\nPart 7 - Persona: More detailed profiling of customers.\nQ23. How do you plan to get your next 100 customers?\n\n\nPart 8 - High Level Product Specification: Complete overview of the product.\nQ24. Does your team & customers have the same idea about what the service is & the direction it is evolving?\nQ25. What steps are you taking to improve on this?\n\n\nPart 9 - Last Words:\nQ26. What motivates you to do this every single day?\nQ27. What keeps you awake at night about your business?\nQ28. Is there any situation where you might seriously consider walking away?\nQ29. What are the top 3 things you learnt from starting & running this?\nQ30. Anything you wish to share about your experiences that most people don’t know?\n\n\nPart 10 - Introspection\n\nWhat made you interested to meet this entrepreneur and how the entrepreneur’s work or life story aligns with your interests.\nWhat you learned about the entrepreneur and the entrepreneur’s startup before your meeting, and what questions you prepared for the entrepreneur.\nWhat you learned by meeting the entrepreneur, and how that compares or contrasts with your perspective before the meeting.\nWhat problem the entrepreneur is solving, for what target customer, with what solution, and what makes the solution unique.\nDescribe your thoughts on the potential of the entrepreneur’s startup."
  },
  {
    "objectID": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "href": "posts/cs10-extractinginitialparametersexistingholtwinter/index.html",
    "title": "Extracting initial parameters from an existing Holt‐Winter forecasting model",
    "section": "",
    "text": "Objective\nThe model is known but the initial parameters need to be found.\n\nIntroduction\nYou are hired by a local company to help them improve their forecasting capabilities. You are tasked with coming up with quarterly forecasts for an item that appears to have level, seasonality, and trend. The good news is that the company has an existing Holt‐Winter forecasting model. The bad news is that no one knows what the parameters (Alpha, Beta, or Gamma) are.\nYou do have some information. For example, you know that historically, the demand in each quarter follows this distribution:\n\nQ1 (January through March) = 50% of average quarterly demand\nQ2 (April through June) = 75% of average quarterly demand\nQ3 (July through September) = 150% of average quarterly demand\nQ4 (October through December) = 125% of average quarterly demand.\n\nYou just ran the forecast at the end of September (end of 2014Q3) and you have the following estimates:\nFor level: \\(\\hat{a}_{2014Q3}\\) = 1052 units\nFor trend: \\(\\hat{b}_{2014Q3}\\) = 46.2 units per quarter\nQ1. What is the forecast for demand for 2014Q4?\nWe know that\n\\(\\hat{x}_{t,t+\\tau}=(\\hat{a_{t}}+\\tau\\hat{b_{t}})\\hat{F}_{t+\\tau-P}\\)\n\\(\\hat{a}_{t}=\\alpha \\left(\\frac{x_{t}}{\\hat{F}_{t-P}}\\right) +(1-\\alpha)(\\hat{a}_{t-1}+\\hat{b}_{t-1})\\)\n\\(\\hat{b}_{t}=\\beta(\\hat{a}_{t}-\\hat{a}_{t-1})+(1-\\beta)\\hat{b}_{t-1}\\)\n\\(\\hat{F}_{t}=\\gamma \\left(\\frac{x_{t}}{\\hat{a}_{t}}\\right) +(1-\\gamma)\\hat{F}_{t-P}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nx\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{F}_{t-P}\\)\n\n\n\n\n2014Q3\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\n2014Q4\n\n\n\n\n\n\n\n\n\nQ2. Suppose the actual demand in 2014Q4 is 1100 units. What is the smallest & largest possible value for your estimate for level, \\(\\hat{a}_{2014Q4}\\)?\nThe fourth quarter level estimate = \\(\\hat{a}_{2014Q4} = \\hat{x}_{2014Q4} / \\hat{F}_{2013Q4}\\)\nWithout seasonality, level estimate = \\(\\hat{a}_{2014Q4} = (\\hat{a}_{2014Q3}+\\hat{b}_{2014Q34})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlpha\n\nx\n\n\\(\\hat{a}\\)\n\\(\\hat{b}\\)\n\\(\\hat{F}\\)\n\\(\\hat{x}\\)\n\\(\\hat{a}_{t-P}\\)\n\n\n\n\n\n\n2014Q3\n\n\n1052\n46.2\n\n1372.75\n1.25 (2013Q4)\n\n\nSmallest\n1\n2014Q4\n\n1100\n880\n\n\n\n\n\n\nLargest\n0\n2014Q4\n\n1100\n1098.2\n\n\n\n\n\n\n\nQ3. The model was run at the end of 2014Q4. It provided you with the most recent estimates of each pattern. A) The estimate for level, \\(\\hat{a}_{2014Q4}\\) was 1065.5. What is value of alpha? B) Estimate of trend, \\(\\hat{b}_{2014Q4}\\) = 42.9, what is value of beta? C) Estimate of seasonality is \\(\\hat{F}_{2014Q4}\\) = 1.239\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\nGiven\nEquation\n\n\nParameter\nSolved\n\n\n\n\n\\(\\hat{a}_{2014Q4}\\)\n1065.5\n1065.5\\(=\\alpha \\left(\\frac{1100}{1.25}\\right) +(1-\\alpha)(1052+46.2)\\)\n1100\n1052 46.2\nAlpha\n0.15\n\n\n\\(\\hat{b}_{2014Q4}\\)\n42.9\n42.9\\(=\\beta(1.065.5-1052)+(1-\\beta)46.2\\)\n1065.5 1052\n1 46.2\nBeta\n0.1\n\n\n\\(\\hat{F}_{2014Q4}\\)\n1.239\n1.239\\(=\\gamma \\left(\\frac{1100}{1065.5}\\right) +(1-\\gamma)1.25\\)\n1100\n1 1.25\nGamma\n0.05\n\n\n\nQ4. What is your forecast for demand for the 1st quarter of 2015? That is \\(\\hat{a}_{2014Q4,2015Q1}\\)?\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q4}\\)\nWe have the unnormalized seasonality factor, \\(\\hat{F}_{2014Q4} = 1.239\\)\nSince the sum of the most recent season estimates (0.500, 0.750, 1.500, and 1.239 for Q1, Q2, Q3, and\nQ4) adds up to 3.98912, , we need to normalize \\(\\hat{F}_{2014Q4}\\) before we use it in our calculations. We use the formula\n\\[\n\\hat{F}_{iadj}=\\hat{F}_{iold}\\frac{P}{\\sum{\\hat{F}_{i}}}\n\\]\nSo we have \\(\\hat{F}_{2014Q1} = 0.500*(4.000/3.989) = 0.50136\\)\n\\(\\hat{x}_{2014Q4,2015Q1} =(\\hat{a}_{2014Q1} + \\hat{b}_{2014Q1}) \\hat{F}_{2014Q1}\\)\n\\(\\hat{x}_{2014Q4,2015Q1} = (1065.5 + 42.9)(0.501) = 555.3084 = 555.31\\)\nIf you did not normalize the seasonality factor you would have gotten = (1065.5 + 42.9)(0.500) = 554.20. Normalizing the seasonality factors prevents the estimates from drifting. In this case, it is a small drift ‐ but over time it would grow."
  },
  {
    "objectID": "posts/05-howtofindnoofobssas/index.html",
    "href": "posts/05-howtofindnoofobssas/index.html",
    "title": "How to find out and use the number of observations in a given SAS data set",
    "section": "",
    "text": "Created on Sunday, March 16th, 2014 at 5:43 am\nIf one is in a situation where they have to know the number of observations in a particular SAS data set or use it for further calculations, SAS allows many ways of doing so. Let us look at a few quick ways using the DATA step. SAS documentation also includes a SAS Macro approach which will not be replicated here.\nMethod 1: The most straightforward way is to load the set in a _null_ DATA step.\n data _null_;\n set dataset3obs;\n run;\nThe output is given by\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 2: Another way to extract & print is to output the NOBS= value, but since the DATA step loops to read all observations, the put statement is executed at every iteration.\n data _null_;\n set dataset3obs nobs=nobs;\n put nobs;\n run;\nThe output is given by\n 3\n 3\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 3: One way to overcome this & print a single value is to output the number of observations either at the beginning or at the end of DATA step.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 4: Output at the end of the data loop.\n data _null_;\n set dataset3obs nobs=nobs end=last;\n if last then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nMethod 5: One can even extract the value before the SET statement, as SAS loads it in the descriptor portion during the compilation phase.\n data _null_;\n set dataset3obs nobs=nobs;\n if _n_=1 then put nobs;\n run;\nThe output is given by\n 3\n NOTE: There were 3 observations read from the data set WORK.DATASET3OBS.\n NOTE: DATA statement used (Total process time):\n real time 0.00 seconds\n cpu time 0.00 seconds\nThere are many other methods but these are simple for just a quick lookup."
  },
  {
    "objectID": "posts/06-referringsasdatasetfullpath/index.html",
    "href": "posts/06-referringsasdatasetfullpath/index.html",
    "title": "Referring to a SAS data set with its full filesystem path",
    "section": "",
    "text": "Posted on Friday, April 4th, 2014 at 3:14 am\nThe standard way of referring to a SAS data set is through its library reference. i.e. LIBREF.DATASETNAME. However SAS also allows accessing the data set using the full filesystem path. Although not very useful, it does show the versatility that SAS allows the user. It can be used in situations where the data set needs to be accessed without defining a library name reference to access it.\nproc print data=sasuser.admit;\nrun;\nUsing the full filesystem path we get:\nproc print data='C:\\Documents and Settings\\sasuser\\My Documents\\My SAS Files\\9.1\\admit.sas7bdat'; \nrun;"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "",
    "text": "Objective:"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#simple-exponential-smoothing-ses-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Simple Exponential Smoothing (SES) model",
    "text": "Simple Exponential Smoothing (SES) model\nWe try this model as it looks like there is stationary demand & no trend. We need to assign the initial parameters first. We start with period 0 where we assume the forecast for the period 1 is the same as the demand for period 1. Also assume initial α=0.12\nUsing this we have"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#varying-alpha-to-get-the-most-accurate-model",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Varying alpha to get the most accurate model",
    "text": "Varying alpha to get the most accurate model\n\n\n\n\n\n\n\n\n\n\n\n\nSES alpha\nMAPE\n\nRMSE\n\n\n\n\n\n\n\n0.12\n0.052619\n\n74.00847195\n\n\n\n\n\n0.20925\n0.04772\nLEAST MAPE\n69.79034\n\n\n\n\n\n0.29816\n0.048748836\n\n68.96512784\nLEAST RMSE\n\n\n\n\n0.4\n0.05095862\n\n69.57546188\n\n\n\n\n\n0.9\n0.062207335\n\n81.68324857"
  },
  {
    "objectID": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "href": "posts/cs08-exponentialsmoothingmodelstrainmax/index.html#conclusion",
    "title": "Exponential smoothing models at TrainMax Systems",
    "section": "Conclusion",
    "text": "Conclusion\nBy varying Alpha we are merely trying to fit the model & minimize the error to historical data. Such tweaking will not necessarily produce a forecast. Also the coefficient of variation needs to be looked at. Higher CV means that data is more volatile & thus Alpha needs to be high to follow these fast changes.\nAlso increasing Alpha does not change the forecast much. This shows the robustness of the SES model.\nFrom the above data, an increase between 0.15 and 0.20 would give a good forecasting model. But whatever the value of Alpha to be used in the model, it needs to be tested on new data to see how it performs."
  }
]