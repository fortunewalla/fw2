<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>L&#39;Œuvre</title>
<link>https://fortunewalla.github.io/fw2/</link>
<atom:link href="https://fortunewalla.github.io/fw2/index.xml" rel="self" type="application/rss+xml"/>
<description>Explore product management, data science, &amp; the metaverse.</description>
<generator>quarto-1.6.37</generator>
<lastBuildDate>Fri, 17 Feb 2023 00:00:00 GMT</lastBuildDate>
<item>
  <title>Post With Code</title>
  <dc:creator>Harlow Malloc</dc:creator>
  <link>https://fortunewalla.github.io/fw2/posts/post-with-code/</link>
  <description><![CDATA[ 




<p>This is a post with executable code.</p>



 ]]></description>
  <category>news</category>
  <category>code</category>
  <category>analysis</category>
  <guid>https://fortunewalla.github.io/fw2/posts/post-with-code/</guid>
  <pubDate>Fri, 17 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://fortunewalla.github.io/fw2/posts/post-with-code/image.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Welcome To My Blog</title>
  <link>https://fortunewalla.github.io/fw2/posts/welcome/</link>
  <description><![CDATA[ 




<p>This is the first post in a Quarto blog. Welcome!</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/welcome/thumbnail.jpg" class="img-fluid"></p>
<p>Since this post doesn’t specify an explicit <code>image</code>, the first image in the post will be used in the listing page of posts.</p>



 ]]></description>
  <category>news</category>
  <guid>https://fortunewalla.github.io/fw2/posts/welcome/</guid>
  <pubDate>Fri, 17 Feb 2023 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Some questions to ask entrepreneurs</title>
  <link>https://fortunewalla.github.io/fw2/posts/13-questionstoaskentrepreneurs/</link>
  <description><![CDATA[ 




<p><em>Based on MITx bootcamp notes</em></p>
<section id="part-1---basic-questions" class="level3">
<h3 class="anchored" data-anchor-id="part-1---basic-questions">Part 1 - Basic questions:</h3>
<p>Q01. What is the problem you want to solve?<br>
Q02. Who experiences the problem?<br>
Q03. How do you want to solve this problem?<br>
Q04. Why is this a better solution?<br>
Q05. If you could describe your product in about 10 words without using anything fancy. How would you say it?<br>
Q06. What is the one thing you feel you can do (for your customer) better than everyone else?</p>
</section>
<section id="part-2---business-plan" class="level3">
<h3 class="anchored" data-anchor-id="part-2---business-plan">Part 2 - Business Plan:</h3>
<p>Q07. Why did you pick up this particular field of all the other things to solve?<br>
Q08. What are the many business opportunities do you see in this field?<br>
Q09. What skills do you need to learn to pursue these opportunities?<br>
Q10. And with people of what skills, strengths, and interests would you like to collaborate in that pursuit?</p>
</section>
<section id="part-3---market-segmentation" class="level3">
<h3 class="anchored" data-anchor-id="part-3---market-segmentation">Part 3 - Market Segmentation:</h3>
<p>Q11. What are you top 3 markets (e.g.&nbsp;educational, aerospace, medical etc… ) for your business?<br>
Q12. Did you do primary customer research? If so, what was the main feedback given?<br>
Q13. In what way did you change your business model as a result of this feedback?</p>
</section>
<section id="part-4---beachhead-market-initial-market-segment-that-is-easy-to-grow-profit" class="level3">
<h3 class="anchored" data-anchor-id="part-4---beachhead-market-initial-market-segment-that-is-easy-to-grow-profit">Part 4 - Beachhead market: initial market segment that is easy to grow &amp; profit</h3>
<p>Q14. What do you think is your beachhead market? (e.g.&nbsp;small biz.,schools, houses, retail etc…)<br>
Q15. Is there competition that could block you from getting this business of this market?<br>
Q16. What is your plan to deal with them?<br>
Q17. If you win this market will it help you to win other market segments?</p>
</section>
<section id="part-5---end-user-profile-common-characteristics-among-all-your-customers" class="level3">
<h3 class="anchored" data-anchor-id="part-5---end-user-profile-common-characteristics-among-all-your-customers">Part 5 - End user profile: Common characteristics among all your customers</h3>
<p>Q18. Have you profiled your customers in terms of demographics, their motivations for solving their problem &amp; also socio-economic profiles?<br>
Q19. Do you have the unique characteristics of your customers that you can use to identify other customers?<br>
Q20. How many such kind of customers are there in your beachhead market?</p>
</section>
<section id="part-6---tam-total-addressable-market-total-revenue-from-your-beachhead-market." class="level3">
<h3 class="anchored" data-anchor-id="part-6---tam-total-addressable-market-total-revenue-from-your-beachhead-market.">Part 6 - TAM (Total addressable market): Total revenue from your beachhead market.</h3>
<p>TAM = Total no. of customers possible * Avg. Revenue per customer per Year.<br>
Q21. Do you have any idea of your TAM size? Can you capture 100% of it?<br>
Q22. How much percent can you achieve in the next 5 years?</p>
</section>
<section id="part-7---persona-more-detailed-profiling-of-customers." class="level3">
<h3 class="anchored" data-anchor-id="part-7---persona-more-detailed-profiling-of-customers.">Part 7 - Persona: More detailed profiling of customers.</h3>
<p>Q23. How do you plan to get your next 100 customers?</p>
</section>
<section id="part-8---high-level-product-specification-complete-overview-of-the-product." class="level3">
<h3 class="anchored" data-anchor-id="part-8---high-level-product-specification-complete-overview-of-the-product.">Part 8 - High Level Product Specification: Complete overview of the product.</h3>
<p>Q24. Does your team &amp; customers have the same idea about what the service is &amp; the direction it is evolving?<br>
Q25. What steps are you taking to improve on this?</p>
</section>
<section id="part-9---last-words" class="level3">
<h3 class="anchored" data-anchor-id="part-9---last-words">Part 9 - Last Words:</h3>
<p>Q26. What motivates you to do this every single day?<br>
Q27. What keeps you awake at night about your business?<br>
Q28. Is there any situation where you might seriously consider walking away?<br>
Q29. What are the top 3 things you learnt from starting &amp; running this?<br>
Q30. Anything you wish to share about your experiences that most people don’t know?</p>
</section>
<section id="part-10---introspection" class="level3">
<h3 class="anchored" data-anchor-id="part-10---introspection">Part 10 - Introspection</h3>
<ol type="1">
<li>What made you interested to meet this entrepreneur and how the entrepreneur’s work or life story aligns with your interests.</li>
<li>What you learned about the entrepreneur and the entrepreneur’s startup before your meeting, and what questions you prepared for the entrepreneur.</li>
<li>What you learned by meeting the entrepreneur, and how that compares or contrasts with your perspective before the meeting.</li>
<li>What problem the entrepreneur is solving, for what target customer, with what solution, and what makes the solution unique.</li>
<li>Describe your thoughts on the potential of the entrepreneur’s startup.</li>
</ol>


</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>mitx</category>
  <guid>https://fortunewalla.github.io/fw2/posts/13-questionstoaskentrepreneurs/</guid>
  <pubDate>Thu, 11 Feb 2016 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Extracting initial parameters from an existing Holt‐Winter forecasting model</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs10-extractinginitialparametersexistingholtwinter/</link>
  <description><![CDATA[ 




<p><strong>Objective</strong></p>
<p>The model is known but the initial parameters need to be found.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>You are hired by a local company to help them improve their forecasting capabilities. You are tasked with coming up with quarterly forecasts for an item that appears to have level, seasonality, and trend. The good news is that the company has an existing Holt‐Winter forecasting model. The bad news is that no one knows what the parameters (Alpha, Beta, or Gamma) are.</p>
<p>You do have some information. For example, you know that historically, the demand in each quarter follows this distribution:</p>
<ul>
<li>Q1 (January through March) = 50% of average quarterly demand</li>
<li>Q2 (April through June) = 75% of average quarterly demand</li>
<li>Q3 (July through September) = 150% of average quarterly demand</li>
<li>Q4 (October through December) = 125% of average quarterly demand.</li>
</ul>
<p>You just ran the forecast at the end of September (end of 2014Q3) and you have the following estimates:</p>
<p>For level: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q3%7D"> = 1052 units</p>
<p>For trend: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7B2014Q3%7D"> = 46.2 units per quarter</p>
<p><strong>Q1. What is the forecast for demand for 2014Q4?</strong></p>
<p>We know that</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt,t+%5Ctau%7D=(%5Chat%7Ba_%7Bt%7D%7D+%5Ctau%5Chat%7Bb_%7Bt%7D%7D)%5Chat%7BF%7D_%7Bt+%5Ctau-P%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7Bt%7D=%5Calpha%20%5Cleft(%5Cfrac%7Bx_%7Bt%7D%7D%7B%5Chat%7BF%7D_%7Bt-P%7D%7D%5Cright)%20+(1-%5Calpha)(%5Chat%7Ba%7D_%7Bt-1%7D+%5Chat%7Bb%7D_%7Bt-1%7D)"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7Bt%7D=%5Cbeta(%5Chat%7Ba%7D_%7Bt%7D-%5Chat%7Ba%7D_%7Bt-1%7D)+(1-%5Cbeta)%5Chat%7Bb%7D_%7Bt-1%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7Bt%7D=%5Cgamma%20%5Cleft(%5Cfrac%7Bx_%7Bt%7D%7D%7B%5Chat%7Ba%7D_%7Bt%7D%7D%5Cright)%20+(1-%5Cgamma)%5Chat%7BF%7D_%7Bt-P%7D"></p>
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 3%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 14%">
<col style="width: 19%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>x</th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7Bt-P%7D"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2014Q3</td>
<td></td>
<td>1052</td>
<td>46.2</td>
<td></td>
<td>1372.75</td>
<td>1.25 (2013Q4)</td>
</tr>
<tr class="even">
<td>2014Q4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Q2. Suppose the actual demand in 2014Q4 is 1100 units. What is the smallest &amp; largest possible value for your estimate for level, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4%7D">?</strong></p>
<p>The fourth quarter level estimate = <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4%7D%20=%20%5Chat%7Bx%7D_%7B2014Q4%7D%20/%20%5Chat%7BF%7D_%7B2013Q4%7D"></p>
<p>Without seasonality, level estimate = <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4%7D%20=%20(%5Chat%7Ba%7D_%7B2014Q3%7D+%5Chat%7Bb%7D_%7B2014Q34%7D)"></p>
<table class="caption-top table">
<colgroup>
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 2%">
<col style="width: 5%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 14%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Alpha</th>
<th></th>
<th>x</th>
<th></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7Bt-P%7D"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td></td>
<td>2014Q3</td>
<td></td>
<td></td>
<td>1052</td>
<td>46.2</td>
<td></td>
<td>1372.75</td>
<td>1.25 (2013Q4)</td>
</tr>
<tr class="even">
<td>Smallest</td>
<td>1</td>
<td>2014Q4</td>
<td></td>
<td>1100</td>
<td>880</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Largest</td>
<td>0</td>
<td>2014Q4</td>
<td></td>
<td>1100</td>
<td>1098.2</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Q3. The model was run at the end of 2014Q4. It provided you with the most recent estimates of each pattern. A) The estimate for level, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4%7D"> was 1065.5. What is value of alpha? B) Estimate of trend, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7B2014Q4%7D"> = 42.9, what is value of beta? C) Estimate of seasonality is <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7B2014Q4%7D"> = 1.239</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 12%">
<col style="width: 15%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 16%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Estimate</strong></th>
<th><strong>Given</strong></th>
<th><strong>Equation</strong></th>
<th></th>
<th></th>
<th><strong>Parameter</strong></th>
<th><strong>Solved</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4%7D"></td>
<td>1065.5</td>
<td>1065.5<img src="https://latex.codecogs.com/png.latex?=%5Calpha%20%5Cleft(%5Cfrac%7B1100%7D%7B1.25%7D%5Cright)%20+(1-%5Calpha)(1052+46.2)"></td>
<td>1100</td>
<td>1052 46.2</td>
<td>Alpha</td>
<td>0.15</td>
</tr>
<tr class="even">
<td><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7B2014Q4%7D"></td>
<td>42.9</td>
<td>42.9<img src="https://latex.codecogs.com/png.latex?=%5Cbeta(1.065.5-1052)+(1-%5Cbeta)46.2"></td>
<td>1065.5 1052</td>
<td>1 46.2</td>
<td>Beta</td>
<td>0.1</td>
</tr>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7B2014Q4%7D"></td>
<td>1.239</td>
<td>1.239<img src="https://latex.codecogs.com/png.latex?=%5Cgamma%20%5Cleft(%5Cfrac%7B1100%7D%7B1065.5%7D%5Cright)%20+(1-%5Cgamma)1.25"></td>
<td>1100</td>
<td>1 1.25</td>
<td>Gamma</td>
<td>0.05</td>
</tr>
</tbody>
</table>
<p><strong>Q4. What is your forecast for demand for the 1st quarter of 2015? That is <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B2014Q4,2015Q1%7D">?</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7B2014Q4,2015Q1%7D%20=(%5Chat%7Ba%7D_%7B2014Q1%7D%20+%20%5Chat%7Bb%7D_%7B2014Q1%7D)%20%5Chat%7BF%7D_%7B2014Q4%7D"></p>
<p>We have the unnormalized seasonality factor, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7B2014Q4%7D%20=%201.239"></p>
<p>Since the sum of the most recent season estimates (0.500, 0.750, 1.500, and 1.239 for Q1, Q2, Q3, and</p>
<p>Q4) adds up to 3.98912, , we need to normalize <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7B2014Q4%7D"> before we use it in our calculations. We use the formula</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BF%7D_%7Biadj%7D=%5Chat%7BF%7D_%7Biold%7D%5Cfrac%7BP%7D%7B%5Csum%7B%5Chat%7BF%7D_%7Bi%7D%7D%7D%0A"></p>
<p>So we have <img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7B2014Q1%7D%20=%200.500*(4.000/3.989)%20=%200.50136"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7B2014Q4,2015Q1%7D%20=(%5Chat%7Ba%7D_%7B2014Q1%7D%20+%20%5Chat%7Bb%7D_%7B2014Q1%7D)%20%5Chat%7BF%7D_%7B2014Q1%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7B2014Q4,2015Q1%7D%20=%20(1065.5%20+%2042.9)(0.501)%20=%20555.3084%20=%20555.31"></p>
<p>If you did not normalize the seasonality factor you would have gotten = (1065.5 + 42.9)(0.500) = 554.20. Normalizing the seasonality factors prevents the estimates from drifting. In this case, it is a small drift ‐ but over time it would grow.</p>


</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs10-extractinginitialparametersexistingholtwinter/</guid>
  <pubDate>Sun, 29 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Forecasting with seasonal trends at BLAYK restaurant</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs09-forecastingseasonaltrendsblayk/</link>
  <description><![CDATA[ 




<p><strong>Objective</strong></p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>You are the lead demand planner for Beers Looking At You, Kid (or BLAYK) a restaurant that features sandwiches, appetizers, and, of course, beer. The restaurant is known for its very fresh beer so the management tries to monitor beer consumption by each shift. You have been tasked to look at how the beer consumption is being forecasted in order to improve the quality and lower the costs of having fresh beer.</p>
<p>There are four shifts in each day the restaurant is open:</p>
<p>Shift 1 from 11 AM to 2 PM</p>
<p>Shift 2 from 2 PM to 5 PM</p>
<p>Shift 3 from 5 PM to 8 PM</p>
<p>Shift 4 from 8 PM to 11 PM</p>
<p>The fields in the spreadsheet are:</p>
<p>Time Period (t) – a sequential numbering of each shift in your data from 1 to 120</p>
<p>Date – the date of the record</p>
<p>Shift Number – the shift number for that record (1, 2, 3, or 4), and</p>
<p>Pints Sold – the number of pints of beer sold on that day during that shift.</p>
</section>
<section id="visualization-of-the-raw-data" class="level1">
<h1>Visualization of the raw data</h1>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs09-forecastingseasonaltrendsblayk/9f7c787bf68f68e97ee79c9eb6bc6ad1.png" class="img-fluid"></p>
<section id="initial-seasonality-factors" class="level2">
<h2 class="anchored" data-anchor-id="initial-seasonality-factors">Initial Seasonality Factors</h2>
<p>The above diagram clearly points to a seasonality of the sales.</p>
<p>Since at this point it is not very clear as to whether there is a trend in the data or not, we find use two methods to find the seasonality factors.</p>
<section id="assuming-no-trend" class="level3">
<h3 class="anchored" data-anchor-id="assuming-no-trend">Assuming no trend</h3>
<p>With no trend the seasonality factors (SF) need not be normalized each season.</p>
<p>SF per period = total sales per shift / (total sales per month/no. of periods)</p>
<p>Also</p>
<p>SF per period = total sales per shift / average no. of sales per period</p>
<p>Mathematically we can express it as</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AF_%7Bt%7D=%5Cfrac%7B%5Csum_%7Bt=1%7D%5E%7Bn%7D%20D%7D%7B(%5Csum_%7Bt=1%7D%5E%7Bn%7D%20D_%7Bt%7D)/P%7D%0A"></p>
</section>
<section id="centered-moving-average-method-cma" class="level3">
<h3 class="anchored" data-anchor-id="centered-moving-average-method-cma">Centered Moving Average Method (CMA)</h3>
<p>Since each season has 4 periods, we use 4‐point Centered Moving Average. Here since the season has an even number of points. We need to take the moving average of the season from both sides &amp; then take the final average.</p>
<p>Below is a sample of the data used to calculate part of the Fi’s</p>
<p>MATop is the average of Shift 1,2,3 &amp; 4</p>
<p>MABottom is the average of Shift 2,3,4 &amp; 5</p>
<p>MA_Avgi is the average of MATop &amp; MABottom.</p>
<p>Each Fi is the xi/MA_Avgi except the first two &amp; last two of the time series. The first two &amp; last two Fi are calculated by first &amp; the last MA_Avg values respectively.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 7%">
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 8%">
<col style="width: 11%">
<col style="width: 11%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th>Time Period d(t)</th>
<th>Date</th>
<th>Shift Number</th>
<th>Pints Sold, xi</th>
<th>MATop</th>
<th>MABottom</th>
<th>MA_Avg i</th>
<th>Fi</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1‐Jun</td>
<td>1</td>
<td>357</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>1‐Jun</td>
<td>2</td>
<td>49</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>1‐Jun</td>
<td>3</td>
<td>242</td>
<td>260</td>
<td>264</td>
<td>262</td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>1‐Jun</td>
<td>4</td>
<td>391</td>
<td>264</td>
<td>264</td>
<td>264</td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>2‐Jun</td>
<td>1</td>
<td>373</td>
<td>264</td>
<td>264</td>
<td>264</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>2‐Jun</td>
<td>2</td>
<td>50</td>
<td>264</td>
<td>269</td>
<td>266</td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>2‐Jun</td>
<td>3</td>
<td>243</td>
<td>269</td>
<td>269</td>
<td>269</td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>2‐Jun</td>
<td>4</td>
<td>408</td>
<td>269</td>
<td>269</td>
<td>269</td>
<td></td>
</tr>
</tbody>
</table>
<p>Now if the assumption is incorrect &amp; then is a small trend, then the sum of the factors will not add up to number of periods in a season. i.e P = 4 Hence a correction is required in the form and we simply multiply each of your Seasonality Factors by</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cfrac%7BP%7D%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20F_%7Bi%7D%7D%0A"></p>
<p>Once all the Fi are calculated, we average them according to Shift Number. The summary is in the table.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 10%">
<col style="width: 25%">
<col style="width: 25%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Total Pints Sold</strong></th>
<th><strong>If equal sales per shift, pints per shift sold</strong></th>
<th><strong>Ratio of Sales per shift compared with average</strong></th>
<th><strong>4‐point Moving</strong> <strong>Centered Averaged</strong> <strong>Seasonality Factors</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Entire Month</td>
<td>37423</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Shift 1</td>
<td>13045</td>
<td>9356</td>
<td>1.39432969</td>
<td>1.402007</td>
</tr>
<tr class="odd">
<td>Shift 2</td>
<td>1737</td>
<td>9356</td>
<td>0.185661224</td>
<td>0.185922</td>
</tr>
<tr class="even">
<td>Shift 3</td>
<td>8700</td>
<td>9356</td>
<td>0.929909414</td>
<td>0.928191</td>
</tr>
<tr class="odd">
<td>Shift 4</td>
<td>13941</td>
<td>9356</td>
<td>1.490099671</td>
<td>1.483154</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td>4</td>
<td>3.999273</td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="holtwinter-model-levelseasonalitytrend" class="level2">
<h2 class="anchored" data-anchor-id="holtwinter-model-levelseasonalitytrend">Holt­Winter Model (level+seasonality+trend)</h2>
<p><strong>Level &amp; Trend:</strong></p>
<p>Running a linear regression we get the equation.</p>
<p>y = 0.812x + 262.6</p>
<p>From the regression equation we get a level of about 263 pints of beer per shift with an trend of 0.8 additional pints per time period. i.e.</p>
<p>The regression gives you an estimated level of 265 pints per each shift with a trend of 0.80 additional pints per time period. This means that the sales of beer is increasing about 3.2 pints per day. Hence there is a positive trend trend.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs09-forecastingseasonaltrendsblayk/a23739094719aa4968faaf829fddb96c.png" class="img-fluid"></p>
<section id="seasonality" class="level3">
<h3 class="anchored" data-anchor-id="seasonality">Seasonality</h3>
<p>This involves estimating the initial values of the level and trend “de‐seasoning” the actual demand by the Seasonality Factors we just found. Part of the data used to calculate the normalized seasonality factors.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 5%">
<col style="width: 10%">
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 4%">
<col style="width: 13%">
<col style="width: 10%">
<col style="width: 11%">
<col style="width: 1%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Time Period (t)</strong></th>
<th><strong>Date</strong></th>
<th><strong>Shift Number</strong></th>
<th><strong>Pints Sold</strong></th>
<th><strong>MATop</strong></th>
<th><strong>MABottom</strong></th>
<th><strong>MA_Avg</strong></th>
<th><strong>Fi</strong></th>
<th><strong>SUM of each season</strong></th>
<th><strong>Fi Normalized</strong></th>
<th><strong>Normalized Sum</strong></th>
<th>—</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1‐Jun</td>
<td>1</td>
<td>357</td>
<td></td>
<td></td>
<td></td>
<td>1.363897</td>
<td>3.957406897</td>
<td>1.378576309</td>
<td>4</td>
<td></td>
</tr>
<tr class="even">
<td>2</td>
<td>1‐Jun</td>
<td>2</td>
<td>49</td>
<td></td>
<td></td>
<td></td>
<td>0.187202</td>
<td>3.957406897</td>
<td>0.189216356</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>3</td>
<td>1‐Jun</td>
<td>3</td>
<td>242</td>
<td>260</td>
<td>264</td>
<td>262</td>
<td>0.924546</td>
<td>3.957406897</td>
<td>0.934497106</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>1‐Jun</td>
<td>4</td>
<td>391</td>
<td>264</td>
<td>264</td>
<td>264</td>
<td>1.481762</td>
<td>3.957406897</td>
<td>1.497710229</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>5</td>
<td>2‐Jun</td>
<td>1</td>
<td>373</td>
<td>264</td>
<td>264</td>
<td>264</td>
<td>1.41221</td>
<td>4.023368199</td>
<td>1.404007844</td>
<td>4</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>2‐Jun</td>
<td>2</td>
<td>50</td>
<td>264</td>
<td>269</td>
<td>266</td>
<td>0.187705</td>
<td>4.023368199</td>
<td>0.186615088</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>2‐Jun</td>
<td>3</td>
<td>243</td>
<td>269</td>
<td>269</td>
<td>269</td>
<td>0.904607</td>
<td>4.023368199</td>
<td>0.89935273</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>2‐Jun</td>
<td>4</td>
<td>408</td>
<td>269</td>
<td>269</td>
<td>269</td>
<td>1.518846</td>
<td>4.023368199</td>
<td>1.510024337</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>So taking the average of the all the normalized factors we get,</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Before Normalized</th>
<th>After Normalized</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Fs1</td>
<td>1.402007</td>
<td>1.402205906</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Fs2</td>
<td>0.185922</td>
<td>0.185977102</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Fs3</td>
<td>0.928191</td>
<td>0.928395676</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Fs4</td>
<td>1.483154</td>
<td>1.483421316</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>SUM</td>
<td>3.999273</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="initial-parameters" class="level3">
<h3 class="anchored" data-anchor-id="initial-parameters">Initial Parameters</h3>
<p>Assume that Alpha=0.15, Beta=0.06 &amp; gamma = 0.05</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bx%7D_%7Bt,t+%5Ctau%7D=(%5Chat%7Ba%7D_%7Bt%7D+%5Ctau%5Chat%7Bb%7D_%7Bt%7D)%5Chat%7BF%7D_%7Bt+%5Ctau-P%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Ba%7D_%7Bt%7D=%5Calpha%5Cleft(%5Cfrac%7Bx_%7Bt%7D%7D%7B%5Chat%7BF%7D_%7Bt-P%7D%7D%5Cright)+(1-%5Calpha)(%5Chat%7Ba%7D_%7Bt-1%7D+%5Chat%7Bb%7D_%7Bt-1%7D)%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bb%7D_%7Bt%7D=%5Cbeta(%5Chat%7Ba%7D_%7Bt%7D-%5Chat%7Ba%7D_%7Bt-1%7D)+(1-%5Cbeta)%5Chat%7Bb%7D_%7Bt-1%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BF%7D_%7Bt%7D=%5Cgamma%20%5Cleft(%5Cfrac%7Bx_%7Bt%7D%7D%7B%5Chat%7Ba%7D_%7Bt%7D%7D%5Cright)+(1-%5Cgamma)%5Chat%7BF%7D_%7Bt-P%7D%0A"></p>
<p>We have for the period 120, the following initial parameters,</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>Fs1</td>
<td>1.402205906</td>
<td></td>
</tr>
<tr class="even">
<td>Fs2</td>
<td>0.185977102</td>
<td></td>
</tr>
<tr class="odd">
<td>Fs3</td>
<td>0.928395676</td>
<td></td>
</tr>
<tr class="even">
<td>Fs4</td>
<td>1.483421316</td>
<td></td>
</tr>
<tr class="odd">
<td><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B120%7D"></td>
<td>360.04</td>
<td>0.812*(120) + 262.6</td>
</tr>
<tr class="even">
<td><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B120%7D"></td>
<td>0.812</td>
<td></td>
</tr>
<tr class="odd">
<td>Alpha</td>
<td>0.15</td>
<td></td>
</tr>
<tr class="even">
<td>Beta</td>
<td>0.06</td>
<td></td>
</tr>
<tr class="odd">
<td>Gamma</td>
<td>0.05</td>
<td></td>
</tr>
</tbody>
</table>
<p>Using above data we can start forecasting for the coming periods 122 i.e.&nbsp;July 2 Shift 2</p>
<table class="caption-top table">
<colgroup>
<col style="width: 6%">
<col style="width: 21%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 15%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Actual x(t)</strong></th>
<th><strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7Bi%7D"></strong></th>
<th><strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7Bi%7D"></strong></th>
<th><strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7BF%7D_%7Bi%7D"></strong></th>
<th><strong><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt+4%7D"></strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>120</td>
<td>557</td>
<td>360</td>
<td>0.81</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>121</td>
<td>520</td>
<td>362.3151378</td>
<td>0.900308265</td>
<td>1.403856344</td>
<td>513.694 (for t=125)</td>
</tr>
</tbody>
</table>
<p>Using just the 122 forecast, the rest of the periods i.e.&nbsp;123, 124 &amp; 125 can be calculated using</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7Bx%7D_%7Bt,t+%5Ctau%7D=(%5Chat%7Ba%7D_%7Bt%7D+%5Ctau%5Chat%7Bb%7D_%7Bt%7D)%20%5Chat%7BF%7D_%7Bt+%5Ctau-P%7D%0A"></p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>As observed above, the data about beer consumption follows seasonality &amp; has a positive trend. This can be modeled using the Holt‐Winter Model. Of course, error analysis must be done to tweak the model especially the seasonality factors.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs09-forecastingseasonaltrendsblayk/</guid>
  <pubDate>Tue, 24 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Forecasting for Sugar Bon‐Bon Cereals</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/</link>
  <description><![CDATA[ 




<p><strong>Objective</strong></p>
<p>Formulation &amp; testing of different exponential models on the product data.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>You are working for a retailer that sells a lot of breakfast cereals. One of the cereals that you sell is the popular Sugar Bon‐Bon brand that features twice the sugar and even more caffeine.</p>
</section>
<section id="visualization-of-raw-data" class="level1">
<h1>Visualization of raw data</h1>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/fig1.jpg" class="img-fluid"></p>
<section id="simple-exponential-smoothing-ses" class="level2">
<h2 class="anchored" data-anchor-id="simple-exponential-smoothing-ses">Simple Exponential Smoothing (SES)</h2>
<p>We know that SES assumes stationary demand. i.e.&nbsp;it forecasts does not take into account trends or seasonalities. Even so, we would still like to know the effect of using SES on the forecasts.</p>
<p>Underlying model: <img src="https://latex.codecogs.com/png.latex?x_%7Bt%7D%20=%20a%20+%20e_%7Bt%7D"></p>
<p>Forecasting model: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt,t+1%7D%20=%20%5Calpha%20x_%7Bt%7D%20+%20(1%20%E2%80%93%20%5Calpha)%20%5Chat%7Bx%7D_%7Bt-1,t%7D"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt,t+1%7D"> is forecast for the next period, <img src="https://latex.codecogs.com/png.latex?x_%7Bt%7D"> is the present actual demand and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt-1,t%7D"> is forecast for the previous period.</p>
<p>Initialization of the parameters</p>
<p>There are many ways of doing this. We can take the centered average for the first 4 or 5 periods. We can also take the average of the first 3, 4 or 5 periods depending on the data.</p>
<p>We take <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B4,5%7D">=205.25</p>
<p>We take α=0.15</p>
<p>Using the above model, the forecast for period 25 is around 654 bars. Also the forecast for period</p>
<p>30 will also be the same i.e.&nbsp;654 since the model assumes stationary demand.</p>
<p>MAPE for SES is 0.279329</p>
</section>
<section id="holts-model-hm" class="level2">
<h2 class="anchored" data-anchor-id="holts-model-hm">Holt’s Model (HM)</h2>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/fig2.jpg" class="img-fluid"></p>
<p>Since the data shows a positive trend, we use HM which assumes level &amp; trend.</p>
<p>Underlying model: <img src="https://latex.codecogs.com/png.latex?x_%7Bt%7D%20=%20a%20+%20bt%20+%20e_%7Bt%7D"></p>
<p>Forecasting model: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_%7Bt,t+%5Ctau%7D%20=%20%5Chat%7Ba%7D_%7Bt%7D+%5Ctau%20%5Chat%7Bb%7D_%7Bt%7D"></p>
<p>Updating Component:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7Bt%7D=%5Calpha%20%5Chat%7Bx%7D_%7Bt%7D+%20(1-%5Calpha)%5Chat%7Bx%7D_%7Bt-1,t%7D"></p>
<p>a^t = α xt + (1 – α) x^t‐1,t</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7Bt%7D=%5Cbeta%20(%5Chat%7Ba%7D_%7Bt%7D-%5Chat%7Ba%7D_%7Bt-1%7D)+(1-%5Cbeta)%20%5Chat%7Bb%7D_%7Bt-1%7D"></p>
<p>b^t = β (a^t ‐ a^t‐1) + (1 – β) b^t‐1</p>
<p>Initialization of parameters</p>
<p>We take α = 0.2 and β = 0.05</p>
<p>We take <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7Bt%7D"> = 157.5 &amp; <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bb%7D_%7Bt%7D"> = 19.1</p>
</section>
<section id="comparison-of-models" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-models">Comparison of models</h2>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/fig3.jpg" class="img-fluid"></p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/fig4.jpg" class="img-fluid"></p>
</section>
<section id="mape-of-the-various-models" class="level2">
<h2 class="anchored" data-anchor-id="mape-of-the-various-models">MAPE of the various models</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>MAPE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SES</td>
<td>0.279329</td>
</tr>
<tr class="even">
<td>HM (alp=0.2bet=0.05)</td>
<td>0.128729</td>
</tr>
<tr class="odd">
<td>HM (alp=0.5bet=0.05)</td>
<td>0.120443</td>
</tr>
<tr class="even">
<td>HM (alp=0.99bet=0.05)</td>
<td>0.112559</td>
</tr>
</tbody>
</table>
<p>The MAPE and various other measures such as RMSE or MAD or most any other metric will improve as we increase the value of Alpha. This does not mean we are getting a better model. This means is that we are only fitting the model better to the historical data that we have. We are simple placing more weight to the most recent observations. Therefore, we should monitor effect of alpha &amp; change as and when required.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Holt’s model with alpha=0.2 seems to be best way to forecast the demand of sugar cereals.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs07-forecastingforsugarbonbon/</guid>
  <pubDate>Mon, 23 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Exponential smoothing models at TrainMax Systems</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs08-exponentialsmoothingmodelstrainmax/</link>
  <description><![CDATA[ 




<p>Objective:</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>You have been called in to take another look at TrainMax’s forecasting problems. Recall from the earlier problem that TrainMax is a manufacturer of high‐end specialty engine equipment for high speed trains. They produce parts that are sent to the original equipment manufacturers (OEMs) for manufacturing new engines. They face a continuing challenge of trying to forecast demand for their products. The demand for one part in particular, XC‐288, was highlighted as needing to be examined.</p>
</section>
<section id="visualization-of-the-raw-data" class="level1">
<h1>Visualization of the raw data</h1>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs08-exponentialsmoothingmodelstrainmax/fig1.jpg" class="img-fluid"></p>
<section id="simple-exponential-smoothing-ses-model" class="level2">
<h2 class="anchored" data-anchor-id="simple-exponential-smoothing-ses-model">Simple Exponential Smoothing (SES) model</h2>
<p>We try this model as it looks like there is stationary demand &amp; no trend. We need to assign the initial parameters first. We start with period 0 where we assume the forecast for the period 1 is the same as the demand for period 1. Also assume initial α=0.12</p>
<p>Using this we have</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs08-exponentialsmoothingmodelstrainmax/fig2.jpg" class="img-fluid"></p>
</section>
<section id="varying-alpha-to-get-the-most-accurate-model" class="level2">
<h2 class="anchored" data-anchor-id="varying-alpha-to-get-the-most-accurate-model">Varying alpha to get the most accurate model</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 4%">
<col style="width: 4%">
</colgroup>
<thead>
<tr class="header">
<th><strong>SES alpha</strong></th>
<th><strong>MAPE</strong></th>
<th></th>
<th><strong>RMSE</strong></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.12</td>
<td>0.052619</td>
<td></td>
<td>74.00847195</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>0.20925</td>
<td>0.04772</td>
<td>LEAST MAPE</td>
<td>69.79034</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>0.29816</td>
<td>0.048748836</td>
<td></td>
<td>68.96512784</td>
<td>LEAST RMSE</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>0.4</td>
<td>0.05095862</td>
<td></td>
<td>69.57546188</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>0.9</td>
<td>0.062207335</td>
<td></td>
<td>81.68324857</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs08-exponentialsmoothingmodelstrainmax/fig3.jpg" class="img-fluid"></p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>By varying Alpha we are merely trying to fit the model &amp; minimize the error to historical data. Such tweaking will not necessarily produce a forecast. Also the coefficient of variation needs to be looked at. Higher CV means that data is more volatile &amp; thus Alpha needs to be high to follow these fast changes.</p>
<p>Also increasing Alpha does not change the forecast much. This shows the robustness of the SES model.</p>
<p>From the above data, an increase between 0.15 and 0.20 would give a good forecasting model. But whatever the value of Alpha to be used in the model, it needs to be tested on new data to see how it performs.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs08-exponentialsmoothingmodelstrainmax/</guid>
  <pubDate>Mon, 23 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Choice of moving average or exponential smoothing for a particular product profile</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs06-choiceofmovingaverageorexponential/</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The demand data for a product has been shown in the table below. Compare the forecasts using a Moving Average with a period of 5 months, MA(5), and an Exponential smoothing Method with an α of 0.33. For Exponential Smoothing use the midpoint of first 5 month range of the average as the initial Forecast. (Hint: the Exponential Smoothing Forecast will be initialized with a forecast of 4951 for April made in March.)</p>
</section>
<section id="visualization-of-raw-data" class="level2">
<h2 class="anchored" data-anchor-id="visualization-of-raw-data">Visualization of raw data</h2>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs06-choiceofmovingaverageorexponential/fig1.jpg" class="img-fluid"></p>
<section id="forecasting-with-5-point-moving-average-simple-exponential-smoothing" class="level3">
<h3 class="anchored" data-anchor-id="forecasting-with-5-point-moving-average-simple-exponential-smoothing">Forecasting with 5-point moving average &amp; simple exponential smoothing</h3>
<p>Here we use α=1/3 for our exponential smoothing model</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs06-choiceofmovingaverageorexponential/fig2.jpg" class="img-fluid"></p>
</section>
<section id="accuracy-of-the-models" class="level3">
<h3 class="anchored" data-anchor-id="accuracy-of-the-models">Accuracy of the models</h3>
<p>To estimate the accuracy of the models, we first compare the Mean Absolute Deviation (MAD) for each of the models.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th><strong>5MA</strong></th>
<th><strong>SES</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MAD</td>
<td>453.7429</td>
<td>432.0553</td>
</tr>
</tbody>
</table>
<p>Also comparing the error cumulatively gives a better picture of the accuracy of each of the models.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs06-choiceofmovingaverageorexponential/fig3.jpg" class="img-fluid"></p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>For this particular product, the forecasts from June to December show that SES is performing better than 5MA. SES outperforms 5MA for 5 months while 5MA outperforms SES for only 2 months.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs06-choiceofmovingaverageorexponential/</guid>
  <pubDate>Mon, 23 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Evaluation of stationary demand models</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs05-evaluationofstationarydemand/</link>
  <description><![CDATA[ 




<p><strong>Objective</strong></p>
<p>Select a suitable model among the given choices.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>TrainMax is a manufacturer of high‐end specialty engine equipment for high speed trains. They produce parts that are sent to the original equipment manufacturers (OEMs) for manufacturing new engines. They face a continuing challenge of trying to forecast demand for their products. The demand for one part in particular, XC‐288, was highlighted as needing to be examined.</p>
</section>
<section id="visualization-of-all-the-models" class="level1">
<h1>Visualization of all the models</h1>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs05-evaluationofstationarydemand/9240726888c2daf802a451c0bdf69dfa.jpg" class="img-fluid"></p>
<section id="check-for-stationary-demand" class="level2">
<h2 class="anchored" data-anchor-id="check-for-stationary-demand">Check for stationary demand</h2>
<p>One way of doing is to determine the coefficient of variation (CV)</p>
<p><img src="https://latex.codecogs.com/png.latex?%0ACV=%5Cfrac%7BSTDEV(data)%7D%7BAVERAGE(data)%7D%0A"></p>
<p>We get</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>STDEV</th>
<th>62.6899312</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AVERAGE</td>
<td>1103.78571</td>
</tr>
<tr class="even">
<td>CV</td>
<td>0.05679538</td>
</tr>
</tbody>
</table>
<p>CV is very low &amp; hence the demand is quite stationary &amp; stable in nature.</p>
</section>
<section id="calculations-among-the-models" class="level2">
<h2 class="anchored" data-anchor-id="calculations-among-the-models">Calculations among the models</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th><strong>Prediction for period 15</strong></th>
<th><strong>MAPE(%)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Period 14 value</td>
<td>1169</td>
<td></td>
</tr>
<tr class="even">
<td>Naïve</td>
<td>1169</td>
<td>7.08446274</td>
</tr>
<tr class="odd">
<td>Cumulative</td>
<td>1103.78571</td>
<td>5.13422811</td>
</tr>
<tr class="even">
<td>2MA</td>
<td>1145</td>
<td>6.46420824</td>
</tr>
<tr class="odd">
<td>4MA</td>
<td>1113.25</td>
<td>4.79251584</td>
</tr>
</tbody>
</table>
<p>Here we see that the moving average forecasts need not always be between the naïve &amp; cumulative forecasts.</p>
</section>
<section id="selection-of-the-model-in-the-presence-of-a-trend" class="level2">
<h2 class="anchored" data-anchor-id="selection-of-the-model-in-the-presence-of-a-trend">Selection of the model in the presence of a trend</h2>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs05-evaluationofstationarydemand/fig2.jpg" class="img-fluid"></p>
<p>If we assume there is a positive trend in the data then none of these models are appropriate for demand with a trend pattern. The Cumulative, Naive, and Moving Average forecasts all assume stationary demand. That means that you only assume a Level pattern to the demand with some random noise.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs05-evaluationofstationarydemand/</guid>
  <pubDate>Fri, 20 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Analysis of three black‐box type demand forecasting models</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs04-analysisofblackboxtypedemand/</link>
  <description><![CDATA[ 




<section id="objective" class="level1">
<h1>Objective</h1>
<p>To help the Fenway Park concessions evaluate and compare the quality of these three competing forecasting approaches.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Hot dogs are an integral part of the experience of a Red Sox baseball game in Boston. The hot dogs, or frankfurters, that are sold at the ball park are called Fenway Franks. During the regular season, a total of 81 games are played at Fenway Park, each one bringing around 37 thousand fans. Some games bring more fans than others: attendance depends on many factors, from the weather to the reputation of the opposing team. Demand for hot dogs depends not only on attendance, but also on many other factors, such as temperature or the time of the day when the game takes place.</p>
<p>Although it is not a simple task, estimating the demand for hot dogs during each one of the 81 games of a season is very important for concessions at Fenway. On the one hand, underestimating the demand would mean – besides unsatisfied and hungry fans – lost profit: the cost of a lost sale is estimated at $3. On the other hand, overestimating the demand would mean an excess of unsold hot dogs at the end of the game: the cost of an unsold hot dog, including ingredients, energy and labor, is estimated at $2.</p>
<p>A year ago, a group of concessions at Fenway hired three consulting firms for preparing as many mathematical models, one by each firm, to predict the demand for hot dogs during each game of the season based on multiple factors that are known as late as just two days before the game: air temperature, rain, the opposing team, time of day for the game, injured players who would be absent, and dozens of other factors.</p>
<p>After a year of frantic data crunching, the consultants have presented their final models to the Fenway concessions. You have been hired as an independent expert to evaluate the quality of the forecasts produced by their proposed models. Due to intellectual property issues, you are not allowed to see the actual models, only the forecasts they produced. They are, for your purposes, to be treated as black boxes.</p>
<p>As input data, you have been given the forecast that each model yielded for hot dogs demand in each game in the two most recent seasons. You have also been given by the concessions a detailed record of the actual demand for hot dogs in each of the 81 games of these two seasons.</p>
<section id="visualization-of-raw-data" class="level2">
<h2 class="anchored" data-anchor-id="visualization-of-raw-data">Visualization of raw data</h2>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs04-analysisofblackboxtypedemand/9a2a88bdd81c37122889f948d1c6c8d8.png" class="img-fluid"></p>
</section>
<section id="comparison-of-the-mean-deviation-md-among-the-three-models" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-the-mean-deviation-md-among-the-three-models">Comparison of the mean deviation (MD) among the three models</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 12%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>MD</strong></th>
<th><strong>% deviation from mean of actual sales</strong> <strong>(3750.056)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>M1</td>
<td>‐526.988</td>
<td>‐14.0528</td>
</tr>
<tr class="even">
<td>M2</td>
<td>24.45679</td>
<td>0.652171</td>
</tr>
<tr class="odd">
<td>M3</td>
<td>‐11.6852</td>
<td>‐0.3116</td>
</tr>
</tbody>
</table>
<p>As seen above M1 exhibits the most bias. M3 has the least bias. This is done to</p>
<p>Models 2 and 3 have an average error in the vicinity of only a few dozen hotdogs from the actual. In fact,</p>
<p>M2 &amp; M3 have a MD equivalent to less than 1% of the average hot dog sales per game.</p>
<p>However, Model 1 seems to consistently over‐estimate the demand for hotdogs by an average of 500 units per game ‐ or about 14% of the average hot dog sales per game). Therefore, we can say that – based on the historical data we have – Models 2 and 3 are less biased than Model 1. Model 1 is the most biased of the pack and it is OVER forecasting.</p>
</section>
<section id="calculation-of-rmse-of-each-of-the-models" class="level2">
<h2 class="anchored" data-anchor-id="calculation-of-rmse-of-each-of-the-models">Calculation of RMSE of each of the models</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>RMSE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>M1</td>
<td>597.6846</td>
</tr>
<tr class="even">
<td>M2</td>
<td>286.457</td>
</tr>
<tr class="odd">
<td>M3</td>
<td>500.2935</td>
</tr>
</tbody>
</table>
<p>As seen above, M2 has an error that is half of the other models &amp; hence is the most suitable forecasting model besides also have a small deviation in forecasted sales from the actual demand.</p>
</section>
<section id="effect-of-underestimating-overestimating-by-the-models" class="level2">
<h2 class="anchored" data-anchor-id="effect-of-underestimating-overestimating-by-the-models">Effect of underestimating &amp; overestimating by the models</h2>
<p>As mentioned in the introduction,</p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 43%">
<col style="width: 27%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Forecasting Issues</strong></th>
<th><strong>Effect</strong></th>
<th><strong>Cost</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Underestimating</td>
<td>Unsatisfied &amp; hungry fans</td>
<td>$3 per lost sale</td>
</tr>
<tr class="even">
<td>Overestimating</td>
<td>Waste of ingredients, energy &amp; labor</td>
<td>$2 per unsold hot dog</td>
</tr>
</tbody>
</table>
<p>Calculations of the lost‐sale or unsold inventory by the three models.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Metric</strong></th>
<th><strong>M1</strong></th>
<th><strong>M2</strong></th>
<th><strong>M3</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Shortage of hotdogs</td>
<td>0</td>
<td>21765</td>
<td>32883</td>
</tr>
<tr class="even">
<td>Cost of Lost Sales $</td>
<td>0</td>
<td>65295</td>
<td>98649</td>
</tr>
<tr class="odd">
<td>Unsold Hotdogs</td>
<td>85372</td>
<td>17803</td>
<td>34776</td>
</tr>
<tr class="even">
<td>Cost of Unsold hotdogs $</td>
<td>170744</td>
<td>35606</td>
<td>69552</td>
</tr>
<tr class="odd">
<td>Total Loss $</td>
<td>170744</td>
<td>100901</td>
<td>168201</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Least Total loss</td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>When comparing the MD, RMSE among the three models, M2 seems to be the most accurate in its predictions and it also manages to provide the least total loss &amp; a balance between the cost of lost sales and unsold hot dogs better than the other two models.</p>
<p>Therefore, from among the three available options, M2 seems to make the predictions that make most economic sense.</p>


</section>
</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs04-analysisofblackboxtypedemand/</guid>
  <pubDate>Thu, 19 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Improving the naïve model forecast using cumulative period model</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs03-comparisonnaivecumulative/</link>
  <description><![CDATA[ 




<p><strong>Objective:</strong></p>
<p>Comparing error metrics by switching naïve model to cumulative model.</p>
<p><strong>Introduction:</strong></p>
<p>You have been hired by General Miles, a company that produces healthy gluten‐free breakfast cereal bars. The last market introduction happened a year ago and your manager thinks there might be an issue in the forecasting methodology. They are currently using a simple Naive forecasting model and you think there might be some room for improvement.</p>
<p>Your boss provides you with the sales for the last 12 months and the forecasts for the last 11 months. No data is available to forecast the first month as the product was totally new to the market at the time.</p>
<p><strong>Visualization of the raw data</strong></p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs03-comparisonnaivecumulative/fig1.jpg" class="img-fluid"></p>
<p><strong>Comparison of the forecasting models</strong></p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs03-comparisonnaivecumulative/fig2.jpg" class="img-fluid"></p>
<p><strong>Comparison of Model Error Metrics</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Error Metric</strong></th>
<th><strong>RMSE</strong></th>
<th><strong>MAPE</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naïve Model</td>
<td>80.45665</td>
<td>0.155729</td>
</tr>
<tr class="even">
<td>Cumulative Model</td>
<td>131.226</td>
<td>0.269029</td>
</tr>
</tbody>
</table>
<p><strong>Conclusion:</strong></p>
<p>The cumulative model in this case is worse than the Naïve model since the RMSE and the MAPE values are greater. It does not react quickly enough to adapt to the high variability in sales for this new gluten free cereal bar.</p>



 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs03-comparisonnaivecumulative/</guid>
  <pubDate>Wed, 18 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Performance characteristics of forecasting models</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs02-performancecharacteristics/</link>
  <description><![CDATA[ 




<p><strong>Objective</strong></p>
<p>Investigate existing forecasting capabilities of Ordroid devices &amp; provide suggestions.</p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>You have just been hired by a company that manufactures mid‐range communication devices that use the Ordroid open source operating system. The company is focused on innovating its products and has not put much thought on its inventory or forecasting capabilities. Your boss thinks there might be a problem in the forecasting of the Ordroid Devices and wants you to figure it out. The Ordroid, far from being new to the market, has been out for two years.</p>
<p>Knowing this, you have asked for data on both years of historical sales as well as any forecasts, promotions, pricing changes, or competitive analyses made during this time. Your boss laughs and provides you with all the data they have: the last six months of sales. You ask to meet with the current demand planner for the Ordroid Devices and she tells you that they use a forecasting algorithm of her own design and there is no documentation.</p>
</section>
<section id="visualization-of-the-raw-data" class="level1">
<h1>Visualization of the raw data</h1>
<p>Raw data &amp; forecasts supplied by the demand planner at Ordroid Devices</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs02-performancecharacteristics/fig1.jpg" class="img-fluid"></p>
<p><strong>Calculate some different performance characteristics for the data sample given.</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0AMD=%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(Actual_%7Bi%7D-Forecast_%7Bi%7D)%7D%7Bn%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AMAD=%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clvert%20Actual_%7Bi%7D-Forecast_%7Bi%7D%5Crvert%7D%7Bn%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0ARMSE=%5Csqrt%7B%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20(Actual_%7Bi%7D-Forecast_%7Bi%7D)%5E2%7D%7Bn%7D%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AMPE=%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Cfrac%7B%20(Actual_%7Bi%7D-Forecast_%7Bi%7D)%7D%7BActual_%7Bi%7D%7D%7D%7Bn%7D%0A"> <img src="https://latex.codecogs.com/png.latex?%0AMAPE=%5Cfrac%7B%5Csum_%7Bi=1%7D%5E%7Bn%7D%20%5Clvert%20%5Cfrac%7BActual_%7Bi%7D-Forecast_%7Bi%7D%7D%7BActual_%7Bi%7D%7D%5Crvert%7D%7Bn%7D%0A"></p>
<div class="table-responsive">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Number of devices</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Mean Deviation</td>
<td>MD</td>
<td>112.5</td>
</tr>
<tr class="even">
<td>Mean Absolute Deviation</td>
<td>MAD</td>
<td>509.5</td>
</tr>
<tr class="odd">
<td>Root Mean Square Error</td>
<td>RMSE</td>
<td>540.6115</td>
</tr>
<tr class="even">
<td>Mean Percent Error</td>
<td>MPE</td>
<td>0.04112</td>
</tr>
<tr class="odd">
<td>Mean Absolute Percent Error</td>
<td>MAPE</td>
<td>0.269002</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Q1. What can you say about the presence seasonality of demand?</strong></p>
<p>Seasonality requires a whole cycle. There is not even one full year of data. So as of now, it is too early to fully evaluate seasonality. You need at least two full cycles to determine seasonality.</p>
<p><strong>Q2. What can you say about the presence of a trend in the demand?</strong></p>
<p>Although we don’t have a year’s worth of data, there seems to be a positive trend of about 10% increase in demand in the data or about 171 devices per month.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs02-performancecharacteristics/fig2.jpg" class="img-fluid"></p>
<p><strong>Q3. What can you say about the bias of the forecast?</strong></p>
<p>A bias is a persistent tendency to over or under predict. These forecasts are not persistent in either. In fact, of the six periods, half are over forecast and half are under forecast. So, there does not appear to be any bias in the forecast.</p>
<p><strong>Q4. What can you say about the accuracy of the forecast?</strong></p>
<p>This is not a very good forecast because even though there is a strong positive trend, the forecasts ignores the trend &amp; also the MAPE is almost 27% ‐ quite high.</p>


</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs02-performancecharacteristics/</guid>
  <pubDate>Wed, 18 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Suitability of stationary demand models for forecasting</title>
  <link>https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/</link>
  <description><![CDATA[ 




<section id="objective" class="level1">
<h1>Objective</h1>
<p>Study the suitability of stationary demand models for forecasting sales at the Shah Alam Palm Oil Company.</p>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Palm oil is harvested from the fruit of oil palm trees and is widely used as a cooking oil throughout Africa, Southeast Asia, and parts of Brazil. It is becoming widely used throughout the world as it is a lower cost alternative to other vegetable oils and has other attractive properties.</p>
<p>The Shah Alam Palm Oil Company (SAPOC) harvests, processes, and sells palm oil throughout the region. As a demand analyst, you are asked to review the sales volume (in pounds) of you premium palm oil by one of your customers, a local grocery store in the region.</p>
</section>
<section id="visualization-of-the-raw-data" class="level1">
<h1>Visualization of the raw data</h1>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/a0b469b9abf2c42032068c851e70aa81.png" class="img-fluid"></p>
<p><strong>Q1. What is the trend over the last three years?</strong></p>
<p>There appears to be a positive trend. From the graph there is an increase in the demand of palm oil by about 24 lbs per month for the last three years.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/d6300b600473130800c577e600fe9a41.png" class="img-fluid"></p>
<p><strong>Q2. Does there appear to be any seasonality in the demand pattern?</strong></p>
<p>Yes. If we plot the data by the months for each of the years, there seems to be seasonality to the demand. Demand is low from January to May. It picks up from June to August and then again from October to December.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/7e8963b5923d1e0b71940da630ea2e9b.png" class="img-fluid"></p>
<p><strong>Q3. What is the forecast for demand in January 2015?</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 37%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Forecast for Jan 2015 (lbs)</strong></th>
<th><strong>Actual value in Dec 2014</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naïve Model</td>
<td>1512</td>
<td>1512</td>
</tr>
<tr class="even">
<td>Cumulative Model</td>
<td>957.9444</td>
<td>1512</td>
</tr>
<tr class="odd">
<td>12 Period Moving Average</td>
<td>1173.667</td>
<td>1512</td>
</tr>
</tbody>
</table>
<p><strong>Q4. What is the root mean square error (RMSE) for a next period forecast for these three years of demand?</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>RMSE</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Naïve Model</td>
<td>383.7282</td>
<td></td>
</tr>
<tr class="even">
<td>Cumulative</td>
<td>419.8851</td>
<td></td>
</tr>
<tr class="odd">
<td>12 Period Moving Average</td>
<td>423.33</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Q5. Which of these three models is most appropriate for forecasting the January 2015 demand?</strong></p>
<p>None. As shown above, the palm oil data shows a positive trend &amp; seasonality during the years. The Naïve model forecast for Jan 2015 is clearly different from the previous trends. While the cumulative &amp; naïve models are quite calm indicating they are forecasting demand closer to the average of the data. The main reason for this discrepancies is that the above three models assume a stationary demand that is very close to the level of the mean.</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/fig4.png" class="img-fluid"></p>


</section>

 ]]></description>
  <category>article</category>
  <category>product</category>
  <category>excel</category>
  <category>forecasting</category>
  <guid>https://fortunewalla.github.io/fw2/posts/cs01-suitabilityofstationarydemandmodels/</guid>
  <pubDate>Wed, 18 Nov 2015 00:00:00 GMT</pubDate>
</item>
<item>
  <title>11-geointfinalproject</title>
  <link>https://fortunewalla.github.io/fw2/posts/11-geointfinalproject/</link>
  <description><![CDATA[ 







 ]]></description>
  <category>article</category>
  <category>datascience</category>
  <guid>https://fortunewalla.github.io/fw2/posts/11-geointfinalproject/</guid>
  <pubDate>Thu, 31 Jul 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>12-briefoverviewprinciplesgeoint</title>
  <link>https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/</link>
  <description><![CDATA[ 




<p>DATA SCIENCE SERVICES</p>
<p>Business opportunities from the analysis of customer data</p>
<p>Data science in the enterprise</p>
<blockquote class="blockquote">
<p><strong>Fortune Walla</strong></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>7/31/2014</strong></p>
</blockquote>
<blockquote class="blockquote">
<p>This document discusses the emergence of data science in the business sector. There are 3 main parts INTRODUCTION: Overview of the field of data science. ANALYSIS TECHNIQUES &amp; TOOLS: Tools used commercially. CONCLUSION: How can companies leverage data science in their business strategy &amp; development.</p>
</blockquote>
<section id="introduction" class="level1">
<h1>1. INTRODUCTION</h1>
<section id="data-as-a-commodity" class="level2">
<h2 class="anchored" data-anchor-id="data-as-a-commodity">1.1 Data as a commodity</h2>
<p>The whole notion of data and the use of data in organizations have changed. Especially for smaller organizations &amp; those that are not software companies.</p>
<p>The paradigm change that has occurred is that</p>
<ul>
<li><p>Data is now a commodity.</p></li>
<li><p>Value creation from data.</p></li>
<li><p>Creating new data products from existing data.</p></li>
</ul>
<blockquote class="blockquote">
<p><em>“A data application acquires its value from the data itself, and creates more data as a result. It’s not just an application with data; it’s a data product. Data science enables the creation of data products.”</em> <sup>1</sup></p>
</blockquote>
<p>So far all organizations were collecting data &amp; analyzing it to solve specific business/research problems.</p>
<ul>
<li><p>Here often only a part of the data was evaluated for specific business or research purposes.</p></li>
<li><p>Also people did not think about insights from the rest of the related but uncombined data?</p></li>
</ul>
</section>
<section id="what-has-caused-this-shift-in-the-value-of-data" class="level2">
<h2 class="anchored" data-anchor-id="what-has-caused-this-shift-in-the-value-of-data">1.2 What has caused this shift in the value of data?</h2>
<ul>
<li><p>My analogy would be DSP &amp; IC revolution</p></li>
<li><p>Events after Sep-11</p></li>
<li><p>S.M.A.C -&gt; Social Mobile Analytics Cloud</p></li>
</ul>
<p>This is where most companies missing the paradigm &amp; the opportunities. Most companies especially in the industrial &amp; technical sphere feel that they are not a data company but a hardware, embedded, biomedical, engineering instrumentation company.</p>
</section>
<section id="data-science-a-brave-new-world-2" class="level2">
<h2 class="anchored" data-anchor-id="data-science-a-brave-new-world-2">1.3 Data Science: a brave new world <sup>2</sup></h2>
<p>“Whether it is called data mining, predictive analytics, sense making, or knowledge discovery, the rapid development and increased availability of advanced computational techniques have changed our world in many ways.</p>
<p>There are very few, if any, electronic transactions that are not monitored, collected, aggregated, analyzed, and modeled. Data are collected about everything, from our financial activities to our shopping habits. Even casino gambling is being analyzed and modeled in an effort to characterize, predict, or modify behavior.”</p>
<section id="relation-between-the-subjects-3" class="level3">
<h3 class="anchored" data-anchor-id="relation-between-the-subjects-3">1.3.1 Relation between the subjects <sup>3</sup></h3>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/a58ee411f05ac990ba0a1222cf752b7b.jpg" class="img-fluid"></p>
</section>
<section id="statistical-pattern-recognition-4" class="level3">
<h3 class="anchored" data-anchor-id="statistical-pattern-recognition-4">1.3.2 Statistical pattern recognition <sup>4</sup></h3>
<p>Statistical pattern recognition is a term used to cover all stages of an investigation from problem formulation and data collection through to discrimination and classification, assessment of results and interpretation.</p>
<p>It developed significantly in the 1960s. It was very much an interdisciplinary subject. Approaches for analyzing such data include those for signal processing, filtering, data summarization, dimension reduction, variable selection, regression and classification.</p>
<p>The large number of applications ranging from the classical ones such as automatic character recognition and medical diagnosis to the more recent ones in <strong>data mining</strong> (such as credit scoring, consumer sales analysis and credit card transaction analysis) have attracted considerable research effort.</p>
<p>Within these areas significant progress has been made. These developments include, for example, kernel-based methods (including support vector machines) and Bayesian computational methods.</p>
<p>The term <strong>machine learning</strong> describes the study of machines that can adapt to their environment and learn from example. The machine learning emphasis is perhaps more on computationally intensive methods and less on a statistical approach.</p>
<p>Two complementary approaches to discrimination</p>
<ul>
<li><p>A decision theory approach based on calculation of probability density functions</p></li>
<li><p>The use of Bayes theorem and a discriminant function approach. (Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership. <sup>5</sup>)</p></li>
</ul>
<p>Two approaches to classification</p>
<ul>
<li><p>Both supervised (using class information to design a classifier – i.e.&nbsp;discrimination)</p></li>
<li><p>Unsupervised (allocating to groups without class information – i.e.&nbsp;clustering).</p></li>
</ul>
<p>A practical example of pattern recognition that is familiar to many people is classifying email messages (as spam/not spam) based upon message header, content and sender.</p>
</section>
<section id="knowledge-discovery-in-databases-kdd" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-discovery-in-databases-kdd">1.3.3 Knowledge Discovery in Databases (KDD)</h3>
<section id="from-data-to-knowledge6" class="level4">
<h4 class="anchored" data-anchor-id="from-data-to-knowledge6">From Data to Knowledge:<sup>6</sup></h4>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/e28f667de4ecfb104d606eb212bfd452.jpg" class="img-fluid"></p>
<p>In short, KDA is a sequence of six steps, one of which is the data mining step concerned with building the data model. …from understanding of data and preprocessing to deployment of the results.</p>
</section>
<section id="overview-of-kdd-7" class="level4">
<h4 class="anchored" data-anchor-id="overview-of-kdd-7">Overview of KDD <sup>7</sup></h4>
<p>Historically, the notion of finding useful patterns in data has been given a variety of names, including data mining, knowledge extraction, information discovery, information harvesting, data archaeology, and data pattern processing.</p>
<p>The phrase <strong>knowledge discovery in databases</strong> was coined at the first KDD workshop in 1989 (Piatetsky-Shapiro 1991) to emphasize that knowledge is the end product of a data-driven discovery.</p>
<p>The interdisciplinary nature of KDD has evolved, and continues to evolve, from the intersection of research fields such as machine learning, pattern recognition, databases, and statistics.</p>
<p>Data mining is the application of specific algorithms for extracting patterns from data. The datamining component of KDD currently relies heavily on known techniques from machine learning, pattern recognition, and statistics to find patterns from data in the data-mining step of the KDD process.</p>
<p>The term data mining has mostly been used by statisticians, data analysts, and the management information systems (MIS) communities. It has also gained popularity in the database field.</p>
<p>Blind application of data-mining methods (rightly criticized as data dredging in the statistical literature) can be a dangerous activity, easily leading to the discovery of meaningless and invalid patterns.</p>
<p>A natural question is: <em>How is KDD different from pattern recognition or machine learning (and related fields)?</em></p>
<p>The answer is that these fields provide some of the data-mining methods that are used in the datamining step of the KDD process, including how the data are stored and accessed, how algorithms can be scaled to massive data sets ultimate and still run efficiently, how results can be interpreted and visualized, and how the overall man-machine interaction can usefully be modeled and supported.</p>
<p>The KDD process can be viewed as a multidisciplinary activity that encompasses techniques beyond the scope of any one particular discipline such as machine learning. Thus, for example, neural networks, although a powerful modeling tool, are relatively difficult to understand compared to decision trees.</p>
<p>KDD also emphasizes scaling and robustness properties of modeling algorithms for large noisy data sets.</p>
<p>Knowledge discovery from data is fundamentally a statistical endeavor. Statistics provides a language and framework for quantifying the uncertainty that results when one tries to infer general patterns from a particular sample of an overall population.</p>
<p>The term <em>data mining</em> has had negative connotations in statistics since the1960s when computerbased data analysis techniques were first introduced. The concern arose because if one searches long enough in any data set (even randomly generated data), one can find patterns that appear to be statistically significant but, in fact, are not.</p>
<p>KDD can also be viewed as encompassing a broader view of modeling than statistics. KDD aims to provide tools to automate (to the degree possible) the entire process of data analysis and the statistician’s “art” of hypothesis selection.</p>
<p>Especially database techniques for gaining efficient data access, grouping and ordering operations when accessing data, and optimizing queries constitute the basics for scaling algorithms to larger data sets.</p>
<p>Most data-mining algorithms from statistics, pattern recognition, and machine learning assume data are in the main memory and pay no attention to how the algorithm breaks down if only limited views of the data are possible.</p>
</section>
</section>
<section id="data-mining-8" class="level3">
<h3 class="anchored" data-anchor-id="data-mining-8">1.3.4 Data mining <sup>8</sup></h3>
<p>The aim of data mining is to <strong>make sense</strong> of <strong>large amounts</strong> of <strong>mostly unsupervised data</strong>, in <strong>some domain</strong>.</p>
<p>Businesses are the largest group of DM users, since they routinely collect massive amounts of data and have a vested interest in making sense of the data. Their goal is to make their companies more competitive and profitable.</p>
<p>In the above definition, the first key term is to <strong>make sense</strong>, which has different meanings depending on the user’s experience. Probably the most important requirement is that the discovered new knowledge needs to be understandable to data owners who want to use it to some advantage. The most convenient outcome by far would be knowledge or a model of the data that can be described in easy-to-understand terms, say, via production rules such as:</p>
<p>IF abnormality (obstruction) in coronary arteries</p>
<p>THEN coronary artery disease</p>
<p>In the example, the input data may be images of the heart and accompanying arteries. If the images are diagnosed by cardiologists as being normal or abnormal (with obstructed arteries), then such data are known as learning/training data. Some DM techniques generate models of the data in terms of production rules, and cardiologists may then analyze these and either accept or reject them (in case the rules do not agree with their domain knowledge).</p>
<p>Note, however, that cardiologists may not have used, or even known, some of the rules generated by DM techniques, even if the rules are correct (as determined by cardiologists after deeper examination), or as shown by a data miner to be performing well on new unseen data, known as test data.</p>
<p>We then come to the second requirement; the <strong>generated model needs to be valid</strong>. If, in our example, all the generated rules were already known to cardiologists, these rules would be considered trivial and of no interest, although the generation of the already-known rules validates the generated models and the DM methodology. However, in the latter case, the project results would be considered a failure by the cardiologists (data owners).</p>
<p>Thus, we come to the third requirement associated with <strong>making sense</strong>, namely, that the discovered knowledge must be novel. Let us suppose that the new knowledge about how to diagnose a patient had been discovered not in terms of production rules but by a different type of data model, say, a neural network. In this case, the new knowledge may or may not be acceptable to the cardiologists, since a neural network is a “black box” model that, in general, cannot be understood by humans. A trained neural network, however, might still be acceptable if it were proven to work well on hundreds of new cases. To illustrate the latter case, assume that the purpose of DM was to automate the analysis (prescreening) of heart images before a cardiologist would see a patient; in that case, a neural network model would be acceptable. We thus associate with the term making sense the fourth requirement, by requesting that the discovered knowledge be useful. This usefulness must hold true regardless of the type of model used (in our example, it was rules vs.&nbsp;neural networks).</p>
<p>The other key term in the definition is <strong>large amounts of data</strong>. DM is not about analyzing small data sets that can be easily dealt with using many standard techniques, or even manually. To give the reader a sense of the scale of data being collected that are good candidates for DM, let us look at the following examples:</p>
<ul>
<li><p>AT&amp;T handles over <strong>300 million calls daily</strong> to serve about 100 million customers and stores the information in a multiterabyte database.</p></li>
<li><p>Wal-Mart, in all its stores taken together handles about <strong>21 million transactions a day</strong>, and stores the information in a database of about a <strong>dozen terabytes</strong>.</p></li>
<li><p>NASA generates several <strong>gigabytes of data per hour</strong> through its Earth Observing System.</p></li>
<li><p>Oil companies like Mobil Oil store hundreds of <strong>terabytes</strong> of data about different aspects of oil exploration.</p></li>
<li><p>The Sloan Digital Sky Survey project will collect observational data of about <strong>40 terabytes</strong>.</p></li>
<li><p>Modern biology creates, in projects like the human genome and proteome, data measured in <strong>terabytes and petabytes</strong>. Although no data are publiclyd available,</p></li>
<li><p>Homeland Security in the U.S.A. is collecting <strong>petabytes of data</strong> on its own and other countries’ citizens.</p></li>
</ul>
<p>It is clear that none of the above databases can be analyzed by humans or even by the best algorithms (in terms of speed and memory requirements); these large amounts of data necessarily require the use of DM techniques to reduce the data in terms of both quantity and dimensionality.</p>
<p>The third key term in the above definition is mostly <strong>unsupervised data</strong>. It is much easier, and less expensive, to collect unsupervised data than supervised data. The reason is that with supervised data we must have known inputs corresponding to known outputs, as determined by domain experts. In our example, “input” images correspond to the “output” diagnosis of coronary artery disease (determined by cardiologists – a costly and error-prone process).</p>
<p>So what can be done if only unsupervised data are collected? To deal with the problem, one of the most difficult in DM, we need to use algorithms that are able to find “natural” groupings/clusters, relationships, and associations in the data. For example, if clusters can be found, they can possibly be labeled by domain experts. If we are able to do both, our unsupervised data becomes supervised, resulting in a much easier problem to deal with. Finding natural groupings or relationships in the data, however, is very difficult and remains an open research problem. Clustering is exacerbated by the fact that most clustering algorithms require the user a priori to specify (guess) the number of clusters in the data.</p>
<p>Similarly, the association-rule mining algorithms require the user to specify parameters that allow the generation of an appropriate number of high-quality associations. Another scenario exists when the available data are semisupervised, meaning that there are a few known training data pairs along with thousands of unsupervised data points. In our cardiology example, this situation would correspond to having thousands of images without diagnosis and only a few images that have been diagnosed. The question then becomes: Can these few data points help in the process of making sense of the entire data set?</p>
<p>Fortunately, there exist techniques of semi-supervised learning which take advantage of these few training data points.</p>
<p>By far the easiest scenario in DM is when all data points are fully supervised, since the majority of existing DM techniques are quite good at dealing with such data, with the possible exception of their scalability. A DM algorithm that works well on both small and large data is called scalable, but, unfortunately, few are.</p>
<p>The final key term in the definition <strong>is domain</strong>. The success of DM projects depends heavily on access to domain knowledge, and thus it is crucial for data miners to work very closely with domain experts/data owners. Discovering new knowledge from data is a process that is highly interactive (with domain experts) and iterative. We cannot simply take a successful DM system, built for some domain, and apply it to another domain and expect good results.</p>
<p>Another hundreds of available DM algorithms, such as clustering or machine learning, only small numbers of them are scalable to large data.</p>
<p><em>How does Data Mining Differ from Other Approaches?</em></p>
<p>Data mining came into existence in response to technological advances in many diverse disciplines. For instance, over the years computer engineering contributed significantly to the development of more powerful computers in terms of both speed and memory; computer science and mathematics continued to develop more and more efficient database architectures and search algorithms; and the combination of these disciplines helped to develop the World Wide Web.</p>
<p>All the data in the world are of no value without mechanisms to efficiently and effectively extract information and knowledge from them. Early pioneers such as U. Fayyad, H. Mannila, G. PiatetskyShapiro, G. Djorgovski, W. Frawley, P. Smith, and others recognized this urgent need, and the data mining field was born.</p>
<p>Data mining is not just an “umbrella” term coined for the purpose of making sense of data. The major distinguishing characteristic of DM is that it is data driven, as opposed to other methods that are often model driven.</p>
<p>In statistics, researchers frequently deal with the problem of finding the smallest data size that gives sufficiently confident estimates. In DM, we deal with the opposite problem, namely, data size is large and we are interested in building a data model that is small (not too complex) but still describes the data well.</p>
<p>Finding a good model of the data, which at the same time is easy to understand, is at the heart of DM. We need to keep in mind, however, that none of the generated models will be complete (using all the relevant variables/attributes of the data), and that almost always we will look for a compromise between model completeness and model. This approach is in accordance with Occam’s razor:</p>
<blockquote class="blockquote">
<p><strong>simpler models are preferred over more complex ones.</strong></p>
</blockquote>
<p>The users should understand that the application of even a very good tool to one’s data will most often not result in the generation of valuable knowledge for the data owner after simply clicking “run”.</p>
<p>“Since its genesis in the mid 1990s, data mining has been thought of as encompassing two tasks:</p>
<ul>
<li><p>using data to test some pre-determined hypothesis, or</p></li>
<li><p>using data to determine the hypothesis in the first place.</p></li>
</ul>
<p>The full automation of both these tasks – hypothesising and then testing – leads to what is known as <em>automated discovery</em> or <em>machine learning</em>.” <sup>9</sup></p>
</section>
<section id="machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning">1.3.5 Machine learning:</h3>
<p>The notion of performance improving with experience is central to most, if not all, forms of machine learning. We will use the following general definition:</p>
<blockquote class="blockquote">
<p><em>Machine learning is the systematic study of algorithms and systems that improve their knowledge or performance with experience.</em> <sup>10</sup></p>
</blockquote>
<p>“The goal of machine learning is to <em>teach machines to carry out tasks by providing them with a couple of examples</em> (how to do or not do a task) &amp; <em>let an algorithm come up with the best rule set</em>. The pairs would be your training data, and the resulting rule set (also called model) could then be applied to future data that we have not yet seen.” <sup>11</sup></p>
<p>“It’s only over the past decade or so that the inherent multi-disciplinarity of machine learning has been recognized. It merges ideas from neuroscience and biology, statistics, mathematics, and physics to make machines learn. …..</p>
<p>Another thing that has driven the change in direction in machine learning research is data mining which looks at the extraction of useful information from massive data sets, and which requires efficient algorithms, putting <em>more of the emphasis back onto computer science</em>” <sup>12</sup></p>
</section>
<section id="predictive-analytics-13" class="level3">
<h3 class="anchored" data-anchor-id="predictive-analytics-13">1.3.6 Predictive analytics <sup>13</sup></h3>
<p>Data mining is the discovery of <strong>hidden patterns</strong> of data through machine learning — and sophisticated algorithms are the mining tools.</p>
<p>Predictive analytics is the process of refining that data resource, using business knowledge to extract hidden value from those newly discovered patterns.</p>
<p>Data mining + business knowledge = predictive analytics =&gt; value</p>
<section id="two-broad-identifiable-branches-to-predictive-analytics-14" class="level4">
<h4 class="anchored" data-anchor-id="two-broad-identifiable-branches-to-predictive-analytics-14">Two broad, identifiable branches to predictive analytics <sup>14</sup></h4>
<p>■ <em>Decision analytics</em> has to do with classifying (mainly) people into segments of interest to the analyst. This branch of analytics depends heavily on multivariate statistical analyses, such as cluster analysis and multidimensional scaling. Decision analytics also uses a method called logistic regression to deal with the special problems created by dependent variables that are binary or nominal, such as buys versus doesn’t buy and survives versus doesn’t survive.</p>
<p>■ <em>Predictive analytics</em> deals with forecasting, and often employs techniques that have been used for decades. Exponential smoothing (also termed exponentially weighted moving averages or EMWA) is one such technique, as is autoregression. Box-Jenkins analysis dates to the middle of the twentieth century and comprises the moving average and regression approaches to forecasting.</p>
<p>Of course, these two broad branches aren’t mutually exclusive. There’s not a clear dividing line between situations in which you would use one and not the other, although that’s often the case. But you can certainly find yourself asking questions such as these:</p>
<ul>
<li><p>I’ve classified my current database of prospects into likely buyers and likely non-buyers, according to demographics such as age, income, ZIP Code, and education level. Can I create a credible quarterly forecast of purchase volume if I apply the same classification criteria to a data set consisting of past prospects?</p></li>
<li><p>I’ve extracted two principal components from a set of variables that measure the weekly performance of several product lines over the past two years. How do I forecast the performance of the products for the next quarter using the principal components as the outcome measures?</p></li>
</ul>
<p>So, there can be overlap between decision analytics and predictive analytics. But not always— sometimes all you want to do is forecast, say, product revenue without first doing any classification or multivariate analysis. But at times you believe there’s a need to forecast the behavior of segments or of components that aren’t directly measurable. It’s in that sort of situation that the two broad branches, decision and predictive analytics, nourish one another.</p>
</section>
<section id="predictive-analytic-techniques-15" class="level4">
<h4 class="anchored" data-anchor-id="predictive-analytic-techniques-15">Predictive analytic techniques <sup>15</sup></h4>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/908c518eba0f61e6f51c70e9ca3b4cbf.jpg" class="img-fluid"></p>
<p>Predictive analytics is the process of using a set of sophisticated analytic tools to develop models and estimations of what the environment will do in the future. In addition to the preceding definition of predictive analytics, Gartner Research Director Gareth Herschel says this: “Predictive analytics helps connect data to effective action by drawing reliable conclusions about current conditions and future events.”</p>
</section>
<section id="predictive-analytics-is-data-driven16" class="level4">
<h4 class="anchored" data-anchor-id="predictive-analytics-is-data-driven16">Predictive analytics is data-driven<sup>16</sup></h4>
<p>Learning how to predict from data is sometimes called machine learning—but, it turns out, this is mostly an academic term you find used within research labs, conference papers, and university courses .In commercial, industrial, and government applications—in the real-world usage of machine learning to predict—it’s called something else:</p>
<p>Predictive analytics (PA)— Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.</p>
<p>Built upon computer science and statistics and bolstered by devoted conferences and university degree programs, PA has emerged as its own discipline. But, beyond a field of science, PA is a movement that exerts a forceful impact. Millions of decisions a day determine whom to call, mail, approve, test, diagnose, warn, investigate, incarcerate, set up on a date, and medicate. PA is the means to drive per-person decisions empirically, as guided by data. By answering this mountain of smaller questions, PA may in fact answer the biggest question of all:</p>
<p><em>How can we improve the effectiveness of all these massive functions across government, healthcare, business, nonprofit, and law enforcement work?</em></p>
<p>In this way, PA is a completely different animal from forecasting. Forecasting makes aggregate predictions on a macroscopic level.</p>
<p>Whereas forecasting estimates the total number of ice cream cones to be purchased next month in Nebraska, predictive technology tells you which individual Nebraskans are most likely to be seen with cone in hand.</p>
<p>PA leads within the growing trend to make decisions more “data driven,” relying less on one’s “gut” and more on hard, empirical evidence.</p>
<p>Enter this fact-based domain and you’ll be attacked by buzzwords, including analytics, big data, business intelligence, and data science. While PA fits underneath each of these umbrellas, these evocative terms refer more to the culture and general skill sets of technologists who do an assortment of creative, innovative things with data, rather than alluding to any specific technology or method.</p>
<p>These areas are broad; in some cases, they refer simply to standard Excel reports—that is, to things that are important and require a great deal of craft, but may not rely on science or sophisticated math. And so they are more subjectively defined.</p>
<p>Another term, data mining, is often used as a synonym for PA, but, as an evocative metaphor depicting “digging around” through data in one fashion or another, it is</p>
</section>
<section id="managing-risk-is-a-critical-aspect-of-decision-management-systems.-17" class="level4">
<h4 class="anchored" data-anchor-id="managing-risk-is-a-critical-aspect-of-decision-management-systems.-17">Managing risk is a critical aspect of Decision Management Systems. <sup>17</sup></h4>
<p>The first real commercial use of predictive analytics was to manage credit risk by predicting the likelihood that a consumer would miss a payment in the immediate future.</p>
<p>Suddenly there is all this data about who uses what power when. Using predictive analytics to find trends, segments with specific behaviors, and to predict how people might react to specific price changes will become the norm.</p>
<p>The process of building mathematical optimization models has similarities with its predictive analytics counterpart, but there are a few major differences worth highlighting to avoid confusion:</p>
<p>■ Although predictive models are generated by applying an algorithm to a data set, an optimization model is formulated by hand to represent a business problem by defining the decision variables, the objective, and the constraints.</p>
<p>■ Although the scope and input to a predictive model is often relatively small (such as information about a customer), the scope of an optimization model is usually a complex transaction or a set of transactions.</p>
<p>■ Predictive analytic models generally require access to large amounts of historical data that can be used to train the model. Optimization models can be run against historical data but do not require it.</p>
<p>■ Although invoking a predictive model in a Decision Service is relatively fast and simple—it simply involves evaluating a formula or interpreting a decision tree—solving an optimization model can consume significant time and memory, depending on the complexity of the model and size of the data. The optimization model must search a large set of possible actions to determine the one that best fits the constraints and goals.</p>
<p>Optimization is well established in supply chain problem domains where it is often used to define which products to make on which machines in a factory to maximize the value of products produced given restricted access to the various machines needed to make the products.</p>
<p>Similarly, your airplane seat, rental car, and hotel room are all likely to be priced using optimization technology.</p>
</section>
</section>
<section id="statistical-hypothesis-testing-18" class="level3">
<h3 class="anchored" data-anchor-id="statistical-hypothesis-testing-18">Statistical hypothesis testing <sup>18</sup></h3>
<p>A preliminary study may suggest that customers in the Northeast have a churn rate of 22.5%, whereas the nationwide average churn rate is only 15%. This may be just a chance fluctuation since the churn rate is not constant; it varies over regions and over time, so differences are to be expected. But the Northeast rate is one and a half times the U.S. average, which seems unusually high.</p>
<blockquote class="blockquote">
<p><em>What is the chance that this pattern or phenomenon is due to random variation? Statistical hypothesis testing is used to answer such questions.</em></p>
</blockquote>
</section>
<section id="data-science-19" class="level3">
<h3 class="anchored" data-anchor-id="data-science-19">Data science <sup>19</sup></h3>
<p>The statistician William S. Cleveland defined data science as an interdisciplinary field larger than statistics itself.</p>
<blockquote class="blockquote">
<p><em>We define data science as managing the process that can transform hypotheses and data into actionable predictions.</em></p>
</blockquote>
<p>Typical predictive analytic goals include predicting who will win an election, what products will sell well together, which loans will default, or which advertisements will be clicked on.</p>
<p>The data scientist is responsible for acquiring the data, managing the data, choosing the modeling technique, writing the code, and verifying the results.</p>
<section id="data-science-and-its-relationship-to-big-data-and-data-driven-decision-making-20" class="level4">
<h4 class="anchored" data-anchor-id="data-science-and-its-relationship-to-big-data-and-data-driven-decision-making-20">Data science and its relationship to Big Data and data-driven decision making <sup>20</sup></h4>
<blockquote class="blockquote">
<p><em>Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz.</em></p>
</blockquote>
<p>We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important</p>
<blockquote class="blockquote">
<ol type="i">
<li>to understand its relationships to these other important and closely related concepts,</li>
<li>to begin to understand what are the fundamental principles underlying data science.</li>
</ol>
</blockquote>
<p>At a high level, data science is a set of fundamental principles that support and guide the principled extraction of information and knowledge from data.</p>
<p>Data science is viewed as the connective tissue between data-processing technologies (including those for ‘‘big data’’) and data-driven decision making.</p>
<p>Probably the broadest business applications are in marketing for tasks such as targeted marketing, online advertising, and recommendations for cross-selling. Data science also is applied for general customer relationship management to analyze customer behavior in order to manage attrition and maximize expected customer value. The finance industry uses data science for credit scoring and trading and in operations via fraud detection and workforce management</p>
<p>A data-science perspective provides practitioners with structure and principles, which give the data scientist a framework to systematically treat problems of extracting useful knowledge from data.</p>
<p>Backed by the trillions of bytes’ worth of shopper history that is stored in Wal-Mart’s data warehouse, she felt that the company could ‘‘start predicting what’s going to happen, when Hurricane Charley struck, several weeks earlier, instead of waiting for it to happen,’’ as she put it.</p>
<p>The New York Times reported that: ‘‘.the experts mined the data and found that the stores would indeed need certain products— and not just the usual flashlights. ‘We didn’t know in the past that strawberry Pop-Tarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,’ Ms.&nbsp;Dillman said in a recent interview.’ And the pre-hurricane top-selling item was beer.’’’2</p>
<p>How should MegaTelCo decide on the set of customers to target to best reduce churn for a particular incentive budget?</p>
<p>One standard deviation higher on the DDD scale is associated with a 4–6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.</p>
<p>Our two example case studies illustrate two different sorts of decisions: (1) decisions for which ‘‘discoveries’’ need to be made within data, and (2) decisions that repeat, especially at massive scale, and so decision making can benefit from even small increases in accuracy based on data analysis. The Wal-Mart example above illustrates a type-1 problem. Linda Dillman would like to discover knowledge that will help Wal-Mart prepare for Hurricane Frances’s imminent arrival. Our churn example illustrates a type-2 DDD problem.</p>
<p>A large telecommunications company may have hundreds of millions of customers, each a candidate for defection. Tens of millions of customers have contracts expiring each month, so each one of them has an increased likelihood of defection in the near future. If we can improve our ability to estimate, for a given customer, how profitable it would be for us to focus on her, we can potentially reap large benefits by applying this ability to the millions of customers in the population. This same logic applies to many of the areas where we have seen the most intense application of data science and data mining:</p>
<p>Direct marketing, online advertising, credit scoring, financial trading, help-desk management, fraud detection, search ranking, product recommendation, and so on.</p>
<p>The use of big data technologies correlates with significant additional productivity growth. Specifically, one standard deviation higher utilization of big data technologies is associated with 1–3% higher productivity than the average firm; one standard deviation lower in terms of big data utilization is associated with 1–3% lower productivity. This leads to potentially very large productivity differences between the firms at the extremes.</p>
<p>Similarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the forefront, providing data-driven recommendations from massive data. There are other examples as well. Online advertisers must process extremely large volumes of data (billions of ad impressions per day is not unusual) and maintain a very high throughput (real-time bidding systems make decisions in tens of milliseconds).</p>
<p>Managers and line employees in other functional areas will only get the best from the company’s datascience resources if they have some basic understanding of the fundamental principles.</p>
<p>Facebook, Twitter, Yahoo, Google, Amazon along with many other ‘‘Digital 100’’ companies,5 have high valuations due primarily to data assets they are committed to capturing or creating.</p>
<p><strong>Fundamentals concepts of data science:</strong></p>
<ol type="1">
<li><p>Extracting useful knowledge from data to solve business problems can be treated systematically by <em>following a process with reasonably well-defined stages</em>. The Cross-Industry Standard Process for Data Mining (CRISP-DM)</p></li>
<li><p>Evaluating data-science results requires careful consideration of the <em>context in which they will be used</em>: For our churn-management example, how exactly are we going to use the patterns that are extracted from historical data? More generally, does the pattern lead to better decisions than some reasonable alternative?</p></li>
<li><p>The relationship between the business problem and the analytics solution often can be <em>decomposed into tractable subproblems</em> via the framework of analyzing expected value. We have many specific tools for estimating probabilities and values from data. For our churn example, should the <em>value</em> of the customer be taken into account in addition to the likelihood of leaving?</p></li>
<li><p>Information technology can be used to <em>find informative data items</em> from within a large body of data: In our churn example, a quantity of interest is the likelihood that a particular customer will leave after her contract expires. Before the contract expires, this would be an unknown quantity. However, there may be known data items (usage, service history, how many friends have canceled contracts) that correlate with our quantity of interest. This fundamental concept underlies a vast number of techniques for statistical analysis, predictive modeling, and other data mining.</p></li>
<li><p>Entities that are similar with respect to <em>known features or attributes often are similar with respect to unknown features</em> or attributes.</p></li>
<li><p>If you look too hard at a set of data, <em>you will find something</em>—but it might not generalize beyond the data you’re observing. This is referred to as ‘‘overfitting’’ a dataset.</p></li>
<li><p>To draw causal conclusions, one must pay very close attention to the presence of confounding factors, possibly unseen ones.</p></li>
</ol>
<p>For example, it is common to see job advertisements mentioning data-mining techniques (random forests, support vector machines), specific application areas (recommendation systems, ad placement optimization), alongside popular software tools for processing big data (SQL, Hadoop, MongoDB).</p>
</section>
</section>
</section>
<section id="where-is-the-concept-of-data-science-heading" class="level2">
<h2 class="anchored" data-anchor-id="where-is-the-concept-of-data-science-heading">1.4 Where is the concept of data science heading?</h2>
<p>“Google is not really a search company. It’s a machine-learning company,” says Matthew Zeiler, the CEO of visual search startup Clarifai, who worked on Google Brain during a pair of internships. He says that all of Google’s most-important projects—autonomous cars, advertising, Google Maps—stand to gain from this type of research. “Everything in the company is really driven by machine learning.” <sup>21</sup></p>
<section id="machine-learning-applications-for-data-center-efficiency-optimization-22" class="level4">
<h4 class="anchored" data-anchor-id="machine-learning-applications-for-data-center-efficiency-optimization-22">Machine learning applications for data center efficiency optimization <sup>22</sup></h4>
<p>“The sheer number of possible equipment combinations and their setpoint values makes it difficult to determine where the optimal efficiency lies,” Gao writes in the white paper on his initiative. “In a live DC, it is possible to meet the target setpoints through many possible combinations of hardware (mechanical and electrical equipment) and software (control strategies and setpoints). Testing each and every feature combination to maximize efficiency would be unfeasible given time constraints, frequent fluctuations in the IT load and weather conditions, as well as the need to maintain a stable DC environment.”</p>
<p>“Google’s Search engine has tried to approximate human intelligence by rapidly analyzing enormous amounts of data, but people like LeCun (from Facebook) aim to build massive “neutral networks” that actually mimic the way the brain works.”[^23]</p>
<p>“Vern Brownell, D-Wave’s CEO, talks about the company’s ongoing efforts to prove the potential of its hardware and its plans for the future … third category is in the broad area of machine learning, which we think is one of the most exciting things going on in computer science today.”<sup>23</sup></p>
<p>“In 2000, physicist Alexei Kitaev (then at Microsoft Research) proposed that a mysterious quasiparticle known as a Majorana could be used in quantum information processing, showing that Majoranas located at opposite ends of a quantum wire could effectively create a topologically protected qubit. Five years later, Das Sarma – along with Freedman and Chetan Nayak, Station Q’s other main leader – co-authored a paper suggesting an experimental proposal for creating a topologically protected qubit using something called the “fractional quantum Hall” system (twodimensional electron gas in a strong magnetic field) along with a similar quasiparticle. These important discoveries pointed to a promising new direction for protecting qubits, and therefore getting them to behave. After all, qubits working together in harmony is fundamental to getting them to compute.” <sup>24</sup></p>
<p>“Viv is not the only company competing for a share of those billions. The field of artificial intelligence has become the scene of a frantic corporate arms race, with Internet giants snapping up AI startups and talent. …Their goal is to build a new generation of AI that can process massive troves of data to predict and fulfill our desires.” <sup>25</sup></p>
<p>Google 3D phone/Amazon 3D phone. IBM Neural chip.</p>
</section>
</section>
<section id="some-of-the-current-trends." class="level2">
<h2 class="anchored" data-anchor-id="some-of-the-current-trends.">1.5 Some of the current trends.</h2>
<ul>
<li><p>July 2009 - <em>IBM</em> to buy <em>SPSS</em>(statistics software vendor) for $1.2 Billion to gain analytics</p></li>
<li><p>Mar 2011 - <em>Via Science</em> Acquires <em>Dataspora</em>, a pioneer in predictive analytics to leverage proprietary machine learning platform, REFS™, beyond healthcare and financial services.</p></li>
<li><p>Oct 2013 - <em>SAP</em> buys <em>KXEN</em>(statistics software vendor) to further predictive analytics</p></li>
<li><p>Feb 2014 – <em>Workday</em> acquires HR predictive analytics company <em>Identified</em></p></li>
<li><p>Mar 2014 - <em>Dell</em> acquires <em>StatSoft</em> (statistics software vendor) and the list of predictive platform vendors gets even shorter… by Simon Arkell, CEO, Predixion Software</p></li>
<li><p>Jun 2014 - <em>Nokia</em>’s HERE mapping service continued its buying spree with today’s acquisition of real-time predictive analytics firm <em>Medio Systems</em></p></li>
<li><p>Jul 2014 - <em>Twitter</em> acquires image search firm <em>Madbits</em> (uses deep learning techniques to understand the content of an image)</p></li>
</ul>
</section>
<section id="what-would-typical-application-areas-of-data-science-be" class="level2">
<h2 class="anchored" data-anchor-id="what-would-typical-application-areas-of-data-science-be">1.6 What would typical application areas of data science be?</h2>
<section id="microsoft-machine-learning-software" class="level4">
<h4 class="anchored" data-anchor-id="microsoft-machine-learning-software">Microsoft Machine Learning software</h4>
<p>OSISoft is working with Carnegie Mellon University on real time fault detection and the diagnosis of energy output variations across campus buildings. Machine learning is helping to mitigate issues in real time and to predictably optimize energy usage and cost.</p>
<p>GitHub: sebastianbk/BreastCancerNeuralNetwork Implementation of a Neural Network in .NET using the Diagnostic Wisconsin Breast Cancer Database. After completing the example with the Breast Cancer data set by coding it myself, I thought of using Azure Machine Learning to do the same job.</p>
<p><strong>Kaggle, the leading platform for predictive modeling competitions:</strong></p>
<ul>
<li><p><em>UPenn and Mayo Clinic’s Seizure Detection Challenge</em>: Detect seizures in intracranial EEG recordings</p></li>
<li><p><em>The Heritage Provider Network (HPN)</em>: The goal of the prize is to develop a predictive algorithm that can identify patients who will be admitted to a hospital within the next year, using historical claims data.</p></li>
<li><p><em>Dunnhumby</em>, a U.K. firm that does analytics for supermarket chains, was looking to build a model to predict when supermarket shoppers will next visit the store and how much they will spend.</p></li>
<li><p><em>BoehringerIngelheimBioSciences</em>: Predict a biological response of molecules from their chemical properties as optimally as this data allows, relate molecular information, to an actual biological response.</p></li>
<li><p><em>Jetpac is an online travel magazine startup</em>: Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as ‘good’.</p></li>
<li><p><em>AllState Insurance</em>: The goal of the Claim Prediction Challenge was to predict bodily injury liability, based solely on the characteristics of the insured vehicle.</p></li>
</ul>
</section>
<section id="ibm-life-sciences" class="level4">
<h4 class="anchored" data-anchor-id="ibm-life-sciences">IBM Life Sciences</h4>
<p>Ricardo Machado, (IBM, Brazil) published many papers on neural networks and a predictive expert system named <em>Next</em>. The power of this system stemmed from its ability to use “knowledge graphs” obtained from interviews with medical experts to form the basis of a model capable of altering these graphs when presented with data, thus transforming them into an artificial neural network. <em>Next</em> was successfully used to diagnose and classify kidney diseases.</p>
<p>Beatriz Leao (IBM, Brazil), developed a system called HYCONES, which also combined symbolic knowledge and neural networks. It was able to successfully detect and classify congenital heart diseases. The results of the work were published in M.D.&nbsp;Computing in 1994.</p>
</section>
<section id="stanford-machine-learning-projects-cs229" class="level4">
<h4 class="anchored" data-anchor-id="stanford-machine-learning-projects-cs229">Stanford Machine Learning Projects (CS229)</h4>
<ul>
<li><p>Characterizing and diagnosing hypertrophic cardiomyopathy from ECG data.</p></li>
<li><p>Electrical energy modeling in Y2E2 building based on distributed sensors information.</p></li>
<li><p>Predicting semantic features from CT images of liver lesions using deep learning.</p></li>
<li><p>Machine learning classification of kidney and lung cancer types.</p></li>
<li><p>Gaussian process based image segmentation and object detection in pathology slides.</p></li>
<li><p>Listen to your heart: stress prediction using consumer heart rate sensors.</p></li>
</ul>
</section>
</section>
</section>
<section id="analysis-techniques-tools" class="level1">
<h1>2. ANALYSIS TECHNIQUES &amp; TOOLS</h1>
<section id="top-ten-algorithms-in-data-mining-27" class="level2">
<h2 class="anchored" data-anchor-id="top-ten-algorithms-in-data-mining-27">2.1 Top ten algorithms in data mining <sup>26</sup></h2>
<ol type="1">
<li><p>C4.5</p></li>
<li><p><em>K</em>-Means</p></li>
<li><p>SVM: Support Vector Machines</p></li>
<li><p>Apriori</p></li>
<li><p>Expecation Maximization</p></li>
<li><p>PageRank</p></li>
<li><p>AdaBoost</p></li>
<li><p><em>k</em>NN: <em>k</em>-Nearest Neighbors.</p></li>
<li><p>Naïve Bayes</p></li>
<li><p>CART: Classification and Regression Trees</p></li>
</ol>
</section>
<section id="microsoft-sql-server-analysis-services-ssas-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="microsoft-sql-server-analysis-services-ssas-algorithms">2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Examples of tasks</strong></th>
<th><strong>Microsoft algorithms to use</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Predicting a discrete attribute</strong> - Flag the customers in a prospective buyers list as good or poor prospects. - Calculate the probability that a server will fail within the next 6 months.- Categorize patient outcomes and explore related factors.</td>
<td>* Decision Trees Algorithm * Naive Bayes Algorithm * Clustering Algorithm * Neural Network Algorithm</td>
</tr>
<tr class="even">
<td><strong>Predicting a continuous attribute</strong> - Forecast next year’s sales. - Predict site visitors given past historical and seasonal trends. - Generate a risk score given demographics.</td>
<td>* Decision Trees Algorithm * Time Series Algorithm * Linear Regression Algorithm</td>
</tr>
<tr class="odd">
<td><strong>Predicting a sequence</strong> - Perform clickstream analysis of a company’s Web site. - Analyze the factors leading to server failure. - Capture and analyze sequences of activities during outpatient visits, to formulate best practices around common activities.</td>
<td>* Sequence Clustering Algorithm</td>
</tr>
<tr class="even">
<td><strong>Finding groups of common items in transactions</strong> - Use market basket analysis to determine product placement. - Suggest additional products to a customer for purchase. - Analyze survey data from visitors to an event, to find which activities or booths were correlated, to plan future activities.</td>
<td>* Association Algorithm * Decision Trees Algorithm</td>
</tr>
<tr class="odd">
<td><strong>Finding groups of similar items</strong> - Create patient risk profiles groups based on attributes such as demographics and behaviors. - Analyze users by browsing and buying patterns. - Identify servers that have similar usage characteristics.</td>
<td>* Clustering Algorithm * Sequence Clustering Algorithm</td>
</tr>
</tbody>
</table>
</section>
<section id="madlib-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="madlib-algorithms">2.3 MADlib algorithms</h2>
<p>Key philosophies driving the architecture of MADlib are:</p>
<ul>
<li><p>Operate on the data locally—<strong>in database</strong>. Do not move it between multiple runtime environments unnecessarily.</p></li>
<li><p>Utilize best of breed database engines, but <strong>separate</strong> the machine learning logic from database specific implementation details.</p></li>
<li><p>Leverage MPP Share nothing technology, such as the Pivotal Greenplum Database, to provide parallelism and <strong>scalability</strong>.</p></li>
<li><p><strong>Open</strong> implementation maintaining active ties into ongoing academic research.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Technique</strong></th>
<th><strong>Example Use Cases</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>can be used to model a linear relationship of a scalar dependent variable to one or more explanatory independent variables.</td>
</tr>
<tr class="even">
<td>Latent Dirichlet Allocation</td>
<td>is a topic modeling function used to identify recurring themes in a large document corpus.</td>
</tr>
<tr class="odd">
<td>Summary Function</td>
<td>provides summary statistics for any data table. These statistics include statistics such as: number of distinct values, number of missing values, mean, variance, min, max, most frequent values, quantiles, etc.</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>can be used to predict a binary outcome of a dependent variable from one or more explanatory independent variables.</td>
</tr>
<tr class="odd">
<td>Elastic Net Regularization</td>
<td>is a regularization technique that can be implemented for either linear or logistic regression to help build a more robust model in the event of large numbers of explanatory independent variables.</td>
</tr>
<tr class="even">
<td>Principal Component Analysis</td>
<td>is a dimensional reduction technique that can be used to transform a high dimensional space into a lower dimensional space.</td>
</tr>
<tr class="odd">
<td>Apriori</td>
<td>is a technique for evaluating frequent item-sets, which allows analysis of what events tend to occur together. For instance what items customers frequently purchase in a single transaction.</td>
</tr>
<tr class="even">
<td>k-Means Clustering</td>
<td>is a clustering method used to identify regions of similarity within a dataset. It can be used for many types of analysis including customer segmentation analysis.</td>
</tr>
</tbody>
</table>
</section>
<section id="oracle-advanced-analytics" class="level2">
<h2 class="anchored" data-anchor-id="oracle-advanced-analytics">2.4 Oracle advanced analytics</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Technique</strong></th>
<th><strong>Applicability</strong></th>
<th><strong>Algorithms</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="./media/image9.jpg">./media/image9.jpg</a> Classification</td>
<td>Most commonly used technique for predicting a specific outcome such as response / no-response, high / medium / low-value customer, likely to buy / not buy.</td>
<td>* Logistic Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Naive Bayes —Fast, simple, commonly applicable * Support Vector Machine—Next generation, supports text and wide data. Decision Tree —Popular, provides human-readable rules.</td>
</tr>
<tr class="even">
<td><a href="./media/image10.jpg">./media/image10.jpg</a> Regression</td>
<td>Technique for predicting a continuous numerical outcome such as customer lifetime value, house value, process yield rates.</td>
<td>* Multiple Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Support Vector Machine</td>
</tr>
<tr class="odd">
<td><a href="./media/image11.jpg">./media/image11.jpg</a> Attribute Importance</td>
<td>Ranks attributes according to strength of relationship with target attribute. Use cases include finding factors most associated with customers who respond to an offer, factors most associated with healthy patients.</td>
<td>* Minimum Description – Considers each attribute as a simple predictive model of the target classLength—</td>
</tr>
<tr class="even">
<td><a href="./media/image12.jpg">./media/image12.jpg</a> Anomaly Detection</td>
<td>Identifies unusual or suspicious cases based on deviation from the norm. Common examples include health care fraud, expense report fraud, and tax compliance.</td>
<td>* One-Class Support Vector Machine — Trains on “normal” cases to flag unusual cases</td>
</tr>
<tr class="odd">
<td><a href="./media/image13.jpg">./media/image13.jpg</a> Clustering</td>
<td>Useful for exploring data and finding natural groupings. Members of a cluster are more like each other than they are like members of a different cluster. Common examples include finding new customer segments, and life sciences discovery.</td>
<td>* Enhanced K-Means—Supports text mining, hierarchical clustering, distance based * Orthogonal Partitioning Clustering— Hierarchical clustering, density based. * Expectation Maximization—Clustering technique that performs well in mixed data (dense and sparse) data mining problems.</td>
</tr>
<tr class="even">
<td><a href="./media/image14.jpg">./media/image14.jpg</a> Association</td>
<td>Finds rules associated with frequently co-occurring items, used for market basket analysis, cross-sell, root cause analysis. Useful for product bundling, instore placement, and defect analysis.</td>
<td>* Apriori—Industry standard for market basket analysis</td>
</tr>
<tr class="odd">
<td><a href="./media/image15.jpg">./media/image15.jpg</a> Feature Selection and Extraction</td>
<td>Produces new attributes as linear combination of existing attributes. Applicable for text data, latent semantic analysis, data compression, data decomposition and projection, and pattern recognition.</td>
<td>Non-negative Matrix Factorization— Next generation, maps the original data into the new set of attributes. * Principal Components Analysis (PCA)— creates new fewer composite attributes that represent all the attributes. * Singular Vector Decomposition— established feature extraction method that has a wide range of applications.</td>
</tr>
</tbody>
</table>
</section>
<section id="apache-mahout-for-hadoop" class="level2">
<h2 class="anchored" data-anchor-id="apache-mahout-for-hadoop">2.5 Apache Mahout for Hadoop</h2>
<p>The Apache Mahout project’s goal is to build a scalable machine learning library. There is some degree of overlap with big data analytics within a Hadoop</p>
<p>Mahout includes algorithms for clustering, classification and collaborative filtering. You can also find:</p>
<ol type="1">
<li><p>Matrix factorization based recommenders</p></li>
<li><p>K-Means, Fuzzy K-Means clustering</p></li>
<li><p>Latent Dirichlet Allocation</p></li>
<li><p>Singular Value Decomposition</p></li>
<li><p>Logistic regression classifier</p></li>
<li><p>(Complementary) Naive Bayes classifier</p></li>
<li><p>Random forest classifier</p></li>
</ol>
</section>
<section id="microsoft-azure-cloud-machine-learning-studio" class="level2">
<h2 class="anchored" data-anchor-id="microsoft-azure-cloud-machine-learning-studio">2.6 Microsoft Azure cloud machine learning studio</h2>
<p>There is a pool of VMs running machine learning algorithms using an orchestration engine, freeing the data scientist from moving data and moving to different services.</p>
<p>The ML Studio is targeting the emerging data scientists. You can train 10 models in minutes, not days. You can put a predictive model into production in minutes, not weeks or months. Some customers are reporting a 10X-100X in reduction in cost relative to SAS.</p>
<p>Employees can create their own <strong>workspaces</strong>, giving re-use and cross-teaming and sharing models with others.</p>
<p>The predictive models can be shared as a service across an enterprise leverage Azure as the <strong>public cloud back-end</strong>. For example, you can write JSON-based back ends that leverage your predictive models, allowing you to build decision making dashboards for your business.</p>
<p>Machine Learning algorithms are built to continually improve over time by leverage <strong>training sets</strong>. Training sets make it possible to continually improve the robustness of your predictive model.</p>
<p>The good news is that R is easily integrated into ML Studio. Right now, R is dominant in machine learning space.</p>
</section>
<section id="google-prediction-api" class="level2">
<h2 class="anchored" data-anchor-id="google-prediction-api">2.7 Google Prediction API</h2>
<p>Google’s cloud-based machine learning tools can help analyze your data to add the following features to your applications:</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/2b518674cc121ac2d8d3a372c458b7b5.jpg" class="img-fluid"></p>
<p>Customer sentiment analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/affb8303438400c6e1bffaaa17d3acfc.jpg" class="img-fluid"></p>
<p>Spam detection</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/0319b7f3396d5da0331e2db1b9ea8b27.jpg" class="img-fluid"></p>
<p>Message routing decisions</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/7f61a6e99693c838ff18523723e213b2.jpg" class="img-fluid"></p>
<p>Upsell opportunity analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/72186c0ef6af31d95093b7e6e820dcee.jpg" class="img-fluid"></p>
<p>Document and email classification</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/4d6d333f4a8f94358a8e6e734c18d231.jpg" class="img-fluid"></p>
<p>Diagnostics</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/9e22d36bd5b087be04b49beb6bfa711b.jpg" class="img-fluid"></p>
<p>Churn analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/b8f86c71b8d5cd1a53e04b69dcf0672d.jpg" class="img-fluid"></p>
<p>Suspicious activity identification</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/4f3eab4f9b3c79843ee03106315f36b9.jpg" class="img-fluid"></p>
<p>Recommendation systems</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/media/87cfd1e3c26d37315e249d627ff800cd.jpg" class="img-fluid"></p>
<p><em>And much more…</em></p>
</section>
<section id="implementation-options" class="level2">
<h2 class="anchored" data-anchor-id="implementation-options">2.8 Implementation options</h2>
<section id="hardware" class="level3">
<h3 class="anchored" data-anchor-id="hardware">2.8.1 Hardware</h3>
<p>Offline: Stand-Alone machine, private cloud.</p>
<p>Online: shared hosting, VPS, Virtual Machine</p>
<p>Hosting:1000/month</p>
<p>VPS: 2000-5000/month</p>
<p>Cloud: AzureML, Google Predictive API, Amazon, Oracle DM, ADAPA Cloud, FICO Cloud, SAS Cloud, Mathematica Cloud</p>
</section>
<section id="software" class="level3">
<h3 class="anchored" data-anchor-id="software">2.8.2 Software</h3>
<p>OS: Windows. Linux/UNIX:</p>
<p>Databases: SQL Server, Oracle DM, MySQL, MariaDB, Hadoop, NOSQL</p>
<p>Client End: Excel Add-ins</p>
<p>Analysis Packages: MS Excel, SAS, SPSS [ibm], Statistica [dell], STATA, KXEN [sap]</p>
<p>MADLIB ~ SQL library for databases</p>
<p>Languages: sql, python, perl, awk, sed, vba, R, C#, F#, julia</p>
<p>Graphics/Visualization: 2d/3d ggplot2</p>
<ul>
<li><p>Paid: SAS (best), IBM SPSS, Statistica, Stata, MS Excel (most versatile), MS SQL Server Analysis Services (Built-in Data mining Algorithms implemented as SQL based queries &amp; GUI tools), Oracle Data Mining (ODM), Predixion Tools.</p></li>
<li><p>OSS: WekaGUI (Machine Learning), KNIME, R, Python etc….</p></li>
</ul>
</section>
<section id="standards" class="level3">
<h3 class="anchored" data-anchor-id="standards">2.8.3 Standards</h3>
<p>CRISP-DM (Cross Industry Standard Process for Data Mining): Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment</p>
<p>SEMMA (Sample, Explore, Modify, Model and Assess) by SAS Inc.</p>
<p>PMML (Predictive Model Markup Language) – can exchange predictive models across software using XML. Zemetis, The Data Mining Group (dmg.org)</p>
<p>CONCLUSION</p>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>3. CONCLUSION</h1>
<section id="why-even-bother" class="level2">
<h2 class="anchored" data-anchor-id="why-even-bother">3.1 Why even bother?</h2>
<blockquote class="blockquote">
<p><em>CAPABILITY</em></p>
</blockquote>
</section>
<section id="future-trends" class="level2">
<h2 class="anchored" data-anchor-id="future-trends">3.2 Future trends?</h2>
<ul>
<li><p>The big players of the internet are investing in deep learning, AI &amp; quantum computing to deal with overflowing data.</p></li>
<li><p>All technologies considered hi-tech or cutting-edge are becoming commercialized for normal businesses.</p></li>
<li><p>Novell Computers or IBM Personal computers were very specialized services. But now are commoditized.</p></li>
<li><p>Now we have a situation where data sets are getting larger and the software to do advanced analysis &amp; create prediction models from the vast amounts of data is getting cheaper (SAS).</p></li>
<li><p>Smaller firms, individuals &amp; startups are getting a level playing field in terms of cost of operations, renting cloud, machines, people (expertise is flowing from research labs to engineering &amp; design depts. of companies.) etc…</p></li>
<li><p>“20-25% of outsourcing contracts now relating to Cloud; IT services companies that don’t invest in Cloud may be making a mistake, say experts”<sup>27</sup></p></li>
</ul>
</section>
<section id="business-opportunities-from-the-analysis-of-customer-data." class="level2">
<h2 class="anchored" data-anchor-id="business-opportunities-from-the-analysis-of-customer-data.">3.3 Business opportunities from the analysis of customer data.</h2>
<p>They include:</p>
<ul>
<li><p>Basic reporting &amp; analysis</p></li>
<li><p>Advanced/premium/value-added services</p></li>
<li><p>Engineering, quality control of production, manufacturing.</p></li>
<li><p>Wear/Tear, Performance of sensors / devices. • Optimization of resources for customers</p></li>
<li><p>Sell raw data to bigger firms.</p></li>
<li><p>Sell value added data to bigger firms.</p></li>
<li><p>Sell data services to bigger firms.</p></li>
<li><p>Get contracts from bigger firms.</p></li>
<li><p>Create a marketplace for sensors with advanced analysis in the commercial, industrial or consumer space.</p></li>
<li><p>Intellectual property creation</p></li>
</ul>
<p>For developing countries, it is a wide open market. They have basic electronic infrastructure. Most of the infrastructure in not “smart” or even standardized for advanced data analysis &amp; automated solutions. Less competition especially in the industrial, hardware, electronics, and engineering domain.</p>
<p>Data science in the enterprise</p>
<p>© 2014 by data science services</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>[^23] CADE METZ “<em>Facebook Taps ‘Deep Learning’ Giant for New AI Lab”</em> Wired Magazine (12.09.13) www.wired.com/2013/12/facebook-yann-lecun/</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>MIKE LOUKIDES <em>What is data science? The future belongs to the companies and people that turn data into products.</em> (June, 2010) radar.oreilly.com/2010/06/what-is-data-science.html↩︎</p></li>
<li id="fn2"><p>COLLEEN MCCUE <em>Data Mining and Predictive Analysis</em> (2007) Elsevier Inc1.↩︎</p></li>
<li id="fn3"><p>F. VAN DER HEIJDEN <em>Classification, parameter estimation and state estimation: an engineering approach using MATLAB</em> (2004) John Wiley &amp; Sons Ltd.↩︎</p></li>
<li id="fn4"><p>ANDREW R. WEBB AND KEITH D. COPSEY <em>Statistical Pattern Recognition, Third Edition</em> (2011) John Wiley &amp; Sons, Ltd↩︎</p></li>
<li id="fn5"><p>S.B. GREEN, N. J. SALKIND &amp; T. M. AKEY <em>Using SPSS for Windows and Macintosh: Analyzing and understanding data</em> (2008) Prentice Hall. New Jersey↩︎</p></li>
<li id="fn6"><p>KRZYSZTOF J. CIOS Data <em>Mining: A Knowledge Discovery Approach</em> (2007) Springer Science+Business Media, LLC↩︎</p></li>
<li id="fn7"><p>USAMA FAYYAD, GREGORY PIATETSKY-SHAPIRO AND PADHRAIC SMYTH <em>“From Data Mining to Knowledge Discovery in Databases”</em> AI Magazine (1996), vol.&nbsp;17 No.3, pp.37—54↩︎</p></li>
<li id="fn8"><p>KRZYSZTOF J. CIOS <em>Data Mining: A Knowledge Discovery Approach</em> (2007) Springer Science+Business Media, LLC↩︎</p></li>
<li id="fn9"><p>MOHAMED MEDHAT GABER (Editor) Scientific <em>Data Mining and Knowledge Discovery: Principles and Foundations</em> (2010) Springer-Verlag Berlin Heidelberg↩︎</p></li>
<li id="fn10"><p>PETER FLACH <em>Machine learning: The Art and Science of Algorithms that Make Sense of Data</em> (2012) Cambridge University Press, New York↩︎</p></li>
<li id="fn11"><p>WILLI RICHERT <em>Building Machine Learning Systems with Python</em> (2013) Packt Publishing↩︎</p></li>
<li id="fn12"><p>STEPHEN MARSLAND <em>Machine Learning: An Algorithmic Perspective</em> (2009) CRC Press↩︎</p></li>
<li id="fn13"><p>ANASSE BARI <em>Predictive Analytics for Dummies</em> (2013) Wiley↩︎</p></li>
<li id="fn14"><p>CONRAD CARLBERG <em>Predictive Analytics with Microsoft Excel</em> (2013) QUE↩︎</p></li>
<li id="fn15"><p>MICHAEL WESSLER <em>Predictive Analytics For Dummies, Alteryx Special Edition</em> (2014) Wiley↩︎</p></li>
<li id="fn16"><p>ERIC SIEGEL <em>Predictive Analytics: The Power to Predict who will click, buy, lie or die</em> (2013)↩︎</p></li>
<li id="fn17"><p>JAMES TAYLOR <em>Decision Management Systems - A Practical Guide to Using Business Rules and Predictive Analytics</em> (2012) IBM Press↩︎</p></li>
<li id="fn18"><p>FOSTER PROVOST AND TOM FAWCETT <em>Data Science for Business</em> (2013) O’Reilly Media, Inc↩︎</p></li>
<li id="fn19"><p>NINA ZUMEL <em>Practical Data Science with R</em> (2014) Manning Publications↩︎</p></li>
<li id="fn20"><p>FOSTER PROVOST AND TOM FAWCETT <em>“Data Science and its Relationship to Big Data and Data-Driven Decision Making”</em> Big Data (March 2013), 1(1): 51-59.↩︎</p></li>
<li id="fn21"><p>ROBERT MCMILLAN <em>“Inside the Artificial Brain That’s Remaking the Google Empire”</em> Wired Magazine (07.16.14) www.wired.com/2014/07/google_brain/↩︎</p></li>
<li id="fn22"><p>www.datacenterknowledge.com/archives/2014/05/28/google-using-machine-learning-boost-data-center-efficiency/2/↩︎</p></li>
<li id="fn23"><p>RACHEL COURTLAND <em>“D-Wave Aims to Bring Quantum Computing to the Cloud”</em> (9 Apr 2014) IEEE Spectrum http://spectrum.ieee.org/podcast/computing/hardware/dwave-aims-to-bring-quantum-computing-to-the-cloud↩︎</p></li>
<li id="fn24"><p>www.microsoft.com/en-us/news/stories/stationq/index.html↩︎</p></li>
<li id="fn25"><p>STEVEN LEVY <em>“Siri’s Inventors Are Building a Radical New AI That Does Anything You Ask”</em> Wired Magazine (08.12.14) www.wired.com/2014/08/viv/↩︎</p></li>
<li id="fn26"><p>XINDONG WU AND VIPIN KUMAR (eds.) <em>The Top Ten Algorithms in Data Mining</em> (2009) Chapman and Hall/CRC Press↩︎</p></li>
<li id="fn27"><p>ITIKA SHARMA PUNIT <em>“Cloud: The reality that enterprises cannot escape”</em> Business Standard Newspaper Bangalore Edition (July 7, 2014)↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>article</category>
  <category>datascience</category>
  <category>geospatial</category>
  <category>geoint</category>
  <category>coursera</category>
  <guid>https://fortunewalla.github.io/fw2/posts/12-briefoverviewprinciplesgeoint/</guid>
  <pubDate>Thu, 31 Jul 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Business opportunities from the analysis of customer data</title>
  <link>https://fortunewalla.github.io/fw2/posts/10-businessopportunities/</link>
  <description><![CDATA[ 




<p>DATA SCIENCE SERVICES</p>
<p>Business opportunities from the analysis of customer data</p>
<p>Data science in the enterprise</p>
<blockquote class="blockquote">
<p><strong>Fortune Walla</strong></p>
</blockquote>
<blockquote class="blockquote">
<p><strong>7/31/2014</strong></p>
</blockquote>
<blockquote class="blockquote">
<p>This document discusses the emergence of data science in the business sector. There are 3 main parts INTRODUCTION: Overview of the field of data science. ANALYSIS TECHNIQUES &amp; TOOLS: Tools used commercially. CONCLUSION: How can companies leverage data science in their business strategy &amp; development.</p>
</blockquote>
<section id="introduction" class="level1">
<h1>1. INTRODUCTION</h1>
<section id="data-as-a-commodity" class="level2">
<h2 class="anchored" data-anchor-id="data-as-a-commodity">1.1 Data as a commodity</h2>
<p>The whole notion of data and the use of data in organizations have changed. Especially for smaller organizations &amp; those that are not software companies.</p>
<p>The paradigm change that has occurred is that</p>
<ul>
<li><p>Data is now a commodity.</p></li>
<li><p>Value creation from data.</p></li>
<li><p>Creating new data products from existing data.</p></li>
</ul>
<blockquote class="blockquote">
<p><em>“A data application acquires its value from the data itself, and creates more data as a result. It’s not just an application with data; it’s a data product. Data science enables the creation of data products.”</em> <sup>1</sup></p>
</blockquote>
<p>So far all organizations were collecting data &amp; analyzing it to solve specific business/research problems.</p>
<ul>
<li><p>Here often only a part of the data was evaluated for specific business or research purposes.</p></li>
<li><p>Also people did not think about insights from the rest of the related but uncombined data?</p></li>
</ul>
</section>
<section id="what-has-caused-this-shift-in-the-value-of-data" class="level2">
<h2 class="anchored" data-anchor-id="what-has-caused-this-shift-in-the-value-of-data">1.2 What has caused this shift in the value of data?</h2>
<ul>
<li><p>My analogy would be DSP &amp; IC revolution</p></li>
<li><p>Events after Sep-11</p></li>
<li><p>S.M.A.C -&gt; Social Mobile Analytics Cloud</p></li>
</ul>
<p>This is where most companies missing the paradigm &amp; the opportunities. Most companies especially in the industrial &amp; technical sphere feel that they are not a data company but a hardware, embedded, biomedical, engineering instrumentation company.</p>
</section>
<section id="data-science-a-brave-new-world-2" class="level2">
<h2 class="anchored" data-anchor-id="data-science-a-brave-new-world-2">1.3 Data Science: a brave new world <sup>2</sup></h2>
<p>“Whether it is called data mining, predictive analytics, sense making, or knowledge discovery, the rapid development and increased availability of advanced computational techniques have changed our world in many ways.</p>
<p>There are very few, if any, electronic transactions that are not monitored, collected, aggregated, analyzed, and modeled. Data are collected about everything, from our financial activities to our shopping habits. Even casino gambling is being analyzed and modeled in an effort to characterize, predict, or modify behavior.”</p>
<section id="relation-between-the-subjects-3" class="level3">
<h3 class="anchored" data-anchor-id="relation-between-the-subjects-3">1.3.1 Relation between the subjects <sup>3</sup></h3>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/a58ee411f05ac990ba0a1222cf752b7b.jpg" class="img-fluid"></p>
</section>
<section id="statistical-pattern-recognition-4" class="level3">
<h3 class="anchored" data-anchor-id="statistical-pattern-recognition-4">1.3.2 Statistical pattern recognition <sup>4</sup></h3>
<p>Statistical pattern recognition is a term used to cover all stages of an investigation from problem formulation and data collection through to discrimination and classification, assessment of results and interpretation.</p>
<p>It developed significantly in the 1960s. It was very much an interdisciplinary subject. Approaches for analyzing such data include those for signal processing, filtering, data summarization, dimension reduction, variable selection, regression and classification.</p>
<p>The large number of applications ranging from the classical ones such as automatic character recognition and medical diagnosis to the more recent ones in <strong>data mining</strong> (such as credit scoring, consumer sales analysis and credit card transaction analysis) have attracted considerable research effort.</p>
<p>Within these areas significant progress has been made. These developments include, for example, kernel-based methods (including support vector machines) and Bayesian computational methods.</p>
<p>The term <strong>machine learning</strong> describes the study of machines that can adapt to their environment and learn from example. The machine learning emphasis is perhaps more on computationally intensive methods and less on a statistical approach.</p>
<p>Two complementary approaches to discrimination</p>
<ul>
<li><p>A decision theory approach based on calculation of probability density functions</p></li>
<li><p>The use of Bayes theorem and a discriminant function approach. (Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership. <sup>5</sup>)</p></li>
</ul>
<p>Two approaches to classification</p>
<ul>
<li><p>Both supervised (using class information to design a classifier – i.e.&nbsp;discrimination)</p></li>
<li><p>Unsupervised (allocating to groups without class information – i.e.&nbsp;clustering).</p></li>
</ul>
<p>A practical example of pattern recognition that is familiar to many people is classifying email messages (as spam/not spam) based upon message header, content and sender.</p>
</section>
<section id="knowledge-discovery-in-databases-kdd" class="level3">
<h3 class="anchored" data-anchor-id="knowledge-discovery-in-databases-kdd">1.3.3 Knowledge Discovery in Databases (KDD)</h3>
<section id="from-data-to-knowledge6" class="level4">
<h4 class="anchored" data-anchor-id="from-data-to-knowledge6">From Data to Knowledge:<sup>6</sup></h4>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/e28f667de4ecfb104d606eb212bfd452.jpg" class="img-fluid"></p>
<p>In short, KDA is a sequence of six steps, one of which is the data mining step concerned with building the data model. …from understanding of data and preprocessing to deployment of the results.</p>
</section>
<section id="overview-of-kdd-7" class="level4">
<h4 class="anchored" data-anchor-id="overview-of-kdd-7">Overview of KDD <sup>7</sup></h4>
<p>Historically, the notion of finding useful patterns in data has been given a variety of names, including data mining, knowledge extraction, information discovery, information harvesting, data archaeology, and data pattern processing.</p>
<p>The phrase <strong>knowledge discovery in databases</strong> was coined at the first KDD workshop in 1989 (Piatetsky-Shapiro 1991) to emphasize that knowledge is the end product of a data-driven discovery.</p>
<p>The interdisciplinary nature of KDD has evolved, and continues to evolve, from the intersection of research fields such as machine learning, pattern recognition, databases, and statistics.</p>
<p>Data mining is the application of specific algorithms for extracting patterns from data. The datamining component of KDD currently relies heavily on known techniques from machine learning, pattern recognition, and statistics to find patterns from data in the data-mining step of the KDD process.</p>
<p>The term data mining has mostly been used by statisticians, data analysts, and the management information systems (MIS) communities. It has also gained popularity in the database field.</p>
<p>Blind application of data-mining methods (rightly criticized as data dredging in the statistical literature) can be a dangerous activity, easily leading to the discovery of meaningless and invalid patterns.</p>
<p>A natural question is: <em>How is KDD different from pattern recognition or machine learning (and related fields)?</em></p>
<p>The answer is that these fields provide some of the data-mining methods that are used in the datamining step of the KDD process, including how the data are stored and accessed, how algorithms can be scaled to massive data sets ultimate and still run efficiently, how results can be interpreted and visualized, and how the overall man-machine interaction can usefully be modeled and supported.</p>
<p>The KDD process can be viewed as a multidisciplinary activity that encompasses techniques beyond the scope of any one particular discipline such as machine learning. Thus, for example, neural networks, although a powerful modeling tool, are relatively difficult to understand compared to decision trees.</p>
<p>KDD also emphasizes scaling and robustness properties of modeling algorithms for large noisy data sets.</p>
<p>Knowledge discovery from data is fundamentally a statistical endeavor. Statistics provides a language and framework for quantifying the uncertainty that results when one tries to infer general patterns from a particular sample of an overall population.</p>
<p>The term <em>data mining</em> has had negative connotations in statistics since the1960s when computerbased data analysis techniques were first introduced. The concern arose because if one searches long enough in any data set (even randomly generated data), one can find patterns that appear to be statistically significant but, in fact, are not.</p>
<p>KDD can also be viewed as encompassing a broader view of modeling than statistics. KDD aims to provide tools to automate (to the degree possible) the entire process of data analysis and the statistician’s “art” of hypothesis selection.</p>
<p>Especially database techniques for gaining efficient data access, grouping and ordering operations when accessing data, and optimizing queries constitute the basics for scaling algorithms to larger data sets.</p>
<p>Most data-mining algorithms from statistics, pattern recognition, and machine learning assume data are in the main memory and pay no attention to how the algorithm breaks down if only limited views of the data are possible.</p>
</section>
</section>
<section id="data-mining-8" class="level3">
<h3 class="anchored" data-anchor-id="data-mining-8">1.3.4 Data mining <sup>8</sup></h3>
<p>The aim of data mining is to <strong>make sense</strong> of <strong>large amounts</strong> of <strong>mostly unsupervised data</strong>, in <strong>some domain</strong>.</p>
<p>Businesses are the largest group of DM users, since they routinely collect massive amounts of data and have a vested interest in making sense of the data. Their goal is to make their companies more competitive and profitable.</p>
<p>In the above definition, the first key term is to <strong>make sense</strong>, which has different meanings depending on the user’s experience. Probably the most important requirement is that the discovered new knowledge needs to be understandable to data owners who want to use it to some advantage. The most convenient outcome by far would be knowledge or a model of the data that can be described in easy-to-understand terms, say, via production rules such as:</p>
<p>IF abnormality (obstruction) in coronary arteries</p>
<p>THEN coronary artery disease</p>
<p>In the example, the input data may be images of the heart and accompanying arteries. If the images are diagnosed by cardiologists as being normal or abnormal (with obstructed arteries), then such data are known as learning/training data. Some DM techniques generate models of the data in terms of production rules, and cardiologists may then analyze these and either accept or reject them (in case the rules do not agree with their domain knowledge).</p>
<p>Note, however, that cardiologists may not have used, or even known, some of the rules generated by DM techniques, even if the rules are correct (as determined by cardiologists after deeper examination), or as shown by a data miner to be performing well on new unseen data, known as test data.</p>
<p>We then come to the second requirement; the <strong>generated model needs to be valid</strong>. If, in our example, all the generated rules were already known to cardiologists, these rules would be considered trivial and of no interest, although the generation of the already-known rules validates the generated models and the DM methodology. However, in the latter case, the project results would be considered a failure by the cardiologists (data owners).</p>
<p>Thus, we come to the third requirement associated with <strong>making sense</strong>, namely, that the discovered knowledge must be novel. Let us suppose that the new knowledge about how to diagnose a patient had been discovered not in terms of production rules but by a different type of data model, say, a neural network. In this case, the new knowledge may or may not be acceptable to the cardiologists, since a neural network is a “black box” model that, in general, cannot be understood by humans. A trained neural network, however, might still be acceptable if it were proven to work well on hundreds of new cases. To illustrate the latter case, assume that the purpose of DM was to automate the analysis (prescreening) of heart images before a cardiologist would see a patient; in that case, a neural network model would be acceptable. We thus associate with the term making sense the fourth requirement, by requesting that the discovered knowledge be useful. This usefulness must hold true regardless of the type of model used (in our example, it was rules vs.&nbsp;neural networks).</p>
<p>The other key term in the definition is <strong>large amounts of data</strong>. DM is not about analyzing small data sets that can be easily dealt with using many standard techniques, or even manually. To give the reader a sense of the scale of data being collected that are good candidates for DM, let us look at the following examples:</p>
<ul>
<li><p>AT&amp;T handles over <strong>300 million calls daily</strong> to serve about 100 million customers and stores the information in a multiterabyte database.</p></li>
<li><p>Wal-Mart, in all its stores taken together handles about <strong>21 million transactions a day</strong>, and stores the information in a database of about a <strong>dozen terabytes</strong>.</p></li>
<li><p>NASA generates several <strong>gigabytes of data per hour</strong> through its Earth Observing System.</p></li>
<li><p>Oil companies like Mobil Oil store hundreds of <strong>terabytes</strong> of data about different aspects of oil exploration.</p></li>
<li><p>The Sloan Digital Sky Survey project will collect observational data of about <strong>40 terabytes</strong>.</p></li>
<li><p>Modern biology creates, in projects like the human genome and proteome, data measured in <strong>terabytes and petabytes</strong>. Although no data are publiclyd available,</p></li>
<li><p>Homeland Security in the U.S.A. is collecting <strong>petabytes of data</strong> on its own and other countries’ citizens.</p></li>
</ul>
<p>It is clear that none of the above databases can be analyzed by humans or even by the best algorithms (in terms of speed and memory requirements); these large amounts of data necessarily require the use of DM techniques to reduce the data in terms of both quantity and dimensionality.</p>
<p>The third key term in the above definition is mostly <strong>unsupervised data</strong>. It is much easier, and less expensive, to collect unsupervised data than supervised data. The reason is that with supervised data we must have known inputs corresponding to known outputs, as determined by domain experts. In our example, “input” images correspond to the “output” diagnosis of coronary artery disease (determined by cardiologists – a costly and error-prone process).</p>
<p>So what can be done if only unsupervised data are collected? To deal with the problem, one of the most difficult in DM, we need to use algorithms that are able to find “natural” groupings/clusters, relationships, and associations in the data. For example, if clusters can be found, they can possibly be labeled by domain experts. If we are able to do both, our unsupervised data becomes supervised, resulting in a much easier problem to deal with. Finding natural groupings or relationships in the data, however, is very difficult and remains an open research problem. Clustering is exacerbated by the fact that most clustering algorithms require the user a priori to specify (guess) the number of clusters in the data.</p>
<p>Similarly, the association-rule mining algorithms require the user to specify parameters that allow the generation of an appropriate number of high-quality associations. Another scenario exists when the available data are semisupervised, meaning that there are a few known training data pairs along with thousands of unsupervised data points. In our cardiology example, this situation would correspond to having thousands of images without diagnosis and only a few images that have been diagnosed. The question then becomes: Can these few data points help in the process of making sense of the entire data set?</p>
<p>Fortunately, there exist techniques of semi-supervised learning which take advantage of these few training data points.</p>
<p>By far the easiest scenario in DM is when all data points are fully supervised, since the majority of existing DM techniques are quite good at dealing with such data, with the possible exception of their scalability. A DM algorithm that works well on both small and large data is called scalable, but, unfortunately, few are.</p>
<p>The final key term in the definition <strong>is domain</strong>. The success of DM projects depends heavily on access to domain knowledge, and thus it is crucial for data miners to work very closely with domain experts/data owners. Discovering new knowledge from data is a process that is highly interactive (with domain experts) and iterative. We cannot simply take a successful DM system, built for some domain, and apply it to another domain and expect good results.</p>
<p>Another hundreds of available DM algorithms, such as clustering or machine learning, only small numbers of them are scalable to large data.</p>
<p><em>How does Data Mining Differ from Other Approaches?</em></p>
<p>Data mining came into existence in response to technological advances in many diverse disciplines. For instance, over the years computer engineering contributed significantly to the development of more powerful computers in terms of both speed and memory; computer science and mathematics continued to develop more and more efficient database architectures and search algorithms; and the combination of these disciplines helped to develop the World Wide Web.</p>
<p>All the data in the world are of no value without mechanisms to efficiently and effectively extract information and knowledge from them. Early pioneers such as U. Fayyad, H. Mannila, G. PiatetskyShapiro, G. Djorgovski, W. Frawley, P. Smith, and others recognized this urgent need, and the data mining field was born.</p>
<p>Data mining is not just an “umbrella” term coined for the purpose of making sense of data. The major distinguishing characteristic of DM is that it is data driven, as opposed to other methods that are often model driven.</p>
<p>In statistics, researchers frequently deal with the problem of finding the smallest data size that gives sufficiently confident estimates. In DM, we deal with the opposite problem, namely, data size is large and we are interested in building a data model that is small (not too complex) but still describes the data well.</p>
<p>Finding a good model of the data, which at the same time is easy to understand, is at the heart of DM. We need to keep in mind, however, that none of the generated models will be complete (using all the relevant variables/attributes of the data), and that almost always we will look for a compromise between model completeness and model. This approach is in accordance with Occam’s razor:</p>
<blockquote class="blockquote">
<p><strong>simpler models are preferred over more complex ones.</strong></p>
</blockquote>
<p>The users should understand that the application of even a very good tool to one’s data will most often not result in the generation of valuable knowledge for the data owner after simply clicking “run”.</p>
<p>“Since its genesis in the mid 1990s, data mining has been thought of as encompassing two tasks:</p>
<ul>
<li><p>using data to test some pre-determined hypothesis, or</p></li>
<li><p>using data to determine the hypothesis in the first place.</p></li>
</ul>
<p>The full automation of both these tasks – hypothesising and then testing – leads to what is known as <em>automated discovery</em> or <em>machine learning</em>.” <sup>9</sup></p>
</section>
<section id="machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="machine-learning">1.3.5 Machine learning:</h3>
<p>The notion of performance improving with experience is central to most, if not all, forms of machine learning. We will use the following general definition:</p>
<blockquote class="blockquote">
<p><em>Machine learning is the systematic study of algorithms and systems that improve their knowledge or performance with experience.</em> <sup>10</sup></p>
</blockquote>
<p>“The goal of machine learning is to <em>teach machines to carry out tasks by providing them with a couple of examples</em> (how to do or not do a task) &amp; <em>let an algorithm come up with the best rule set</em>. The pairs would be your training data, and the resulting rule set (also called model) could then be applied to future data that we have not yet seen.” <sup>11</sup></p>
<p>“It’s only over the past decade or so that the inherent multi-disciplinarity of machine learning has been recognized. It merges ideas from neuroscience and biology, statistics, mathematics, and physics to make machines learn. …..</p>
<p>Another thing that has driven the change in direction in machine learning research is data mining which looks at the extraction of useful information from massive data sets, and which requires efficient algorithms, putting <em>more of the emphasis back onto computer science</em>” <sup>12</sup></p>
</section>
<section id="predictive-analytics-13" class="level3">
<h3 class="anchored" data-anchor-id="predictive-analytics-13">1.3.6 Predictive analytics <sup>13</sup></h3>
<p>Data mining is the discovery of <strong>hidden patterns</strong> of data through machine learning — and sophisticated algorithms are the mining tools.</p>
<p>Predictive analytics is the process of refining that data resource, using business knowledge to extract hidden value from those newly discovered patterns.</p>
<p>Data mining + business knowledge = predictive analytics =&gt; value</p>
<section id="two-broad-identifiable-branches-to-predictive-analytics-14" class="level4">
<h4 class="anchored" data-anchor-id="two-broad-identifiable-branches-to-predictive-analytics-14">Two broad, identifiable branches to predictive analytics <sup>14</sup></h4>
<p>■ <em>Decision analytics</em> has to do with classifying (mainly) people into segments of interest to the analyst. This branch of analytics depends heavily on multivariate statistical analyses, such as cluster analysis and multidimensional scaling. Decision analytics also uses a method called logistic regression to deal with the special problems created by dependent variables that are binary or nominal, such as buys versus doesn’t buy and survives versus doesn’t survive.</p>
<p>■ <em>Predictive analytics</em> deals with forecasting, and often employs techniques that have been used for decades. Exponential smoothing (also termed exponentially weighted moving averages or EMWA) is one such technique, as is autoregression. Box-Jenkins analysis dates to the middle of the twentieth century and comprises the moving average and regression approaches to forecasting.</p>
<p>Of course, these two broad branches aren’t mutually exclusive. There’s not a clear dividing line between situations in which you would use one and not the other, although that’s often the case. But you can certainly find yourself asking questions such as these:</p>
<ul>
<li><p>I’ve classified my current database of prospects into likely buyers and likely non-buyers, according to demographics such as age, income, ZIP Code, and education level. Can I create a credible quarterly forecast of purchase volume if I apply the same classification criteria to a data set consisting of past prospects?</p></li>
<li><p>I’ve extracted two principal components from a set of variables that measure the weekly performance of several product lines over the past two years. How do I forecast the performance of the products for the next quarter using the principal components as the outcome measures?</p></li>
</ul>
<p>So, there can be overlap between decision analytics and predictive analytics. But not always— sometimes all you want to do is forecast, say, product revenue without first doing any classification or multivariate analysis. But at times you believe there’s a need to forecast the behavior of segments or of components that aren’t directly measurable. It’s in that sort of situation that the two broad branches, decision and predictive analytics, nourish one another.</p>
</section>
<section id="predictive-analytic-techniques-15" class="level4">
<h4 class="anchored" data-anchor-id="predictive-analytic-techniques-15">Predictive analytic techniques <sup>15</sup></h4>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/908c518eba0f61e6f51c70e9ca3b4cbf.jpg" class="img-fluid"></p>
<p>Predictive analytics is the process of using a set of sophisticated analytic tools to develop models and estimations of what the environment will do in the future. In addition to the preceding definition of predictive analytics, Gartner Research Director Gareth Herschel says this: “Predictive analytics helps connect data to effective action by drawing reliable conclusions about current conditions and future events.”</p>
</section>
<section id="predictive-analytics-is-data-driven16" class="level4">
<h4 class="anchored" data-anchor-id="predictive-analytics-is-data-driven16">Predictive analytics is data-driven<sup>16</sup></h4>
<p>Learning how to predict from data is sometimes called machine learning—but, it turns out, this is mostly an academic term you find used within research labs, conference papers, and university courses .In commercial, industrial, and government applications—in the real-world usage of machine learning to predict—it’s called something else:</p>
<p>Predictive analytics (PA)— Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.</p>
<p>Built upon computer science and statistics and bolstered by devoted conferences and university degree programs, PA has emerged as its own discipline. But, beyond a field of science, PA is a movement that exerts a forceful impact. Millions of decisions a day determine whom to call, mail, approve, test, diagnose, warn, investigate, incarcerate, set up on a date, and medicate. PA is the means to drive per-person decisions empirically, as guided by data. By answering this mountain of smaller questions, PA may in fact answer the biggest question of all:</p>
<p><em>How can we improve the effectiveness of all these massive functions across government, healthcare, business, nonprofit, and law enforcement work?</em></p>
<p>In this way, PA is a completely different animal from forecasting. Forecasting makes aggregate predictions on a macroscopic level.</p>
<p>Whereas forecasting estimates the total number of ice cream cones to be purchased next month in Nebraska, predictive technology tells you which individual Nebraskans are most likely to be seen with cone in hand.</p>
<p>PA leads within the growing trend to make decisions more “data driven,” relying less on one’s “gut” and more on hard, empirical evidence.</p>
<p>Enter this fact-based domain and you’ll be attacked by buzzwords, including analytics, big data, business intelligence, and data science. While PA fits underneath each of these umbrellas, these evocative terms refer more to the culture and general skill sets of technologists who do an assortment of creative, innovative things with data, rather than alluding to any specific technology or method.</p>
<p>These areas are broad; in some cases, they refer simply to standard Excel reports—that is, to things that are important and require a great deal of craft, but may not rely on science or sophisticated math. And so they are more subjectively defined.</p>
<p>Another term, data mining, is often used as a synonym for PA, but, as an evocative metaphor depicting “digging around” through data in one fashion or another, it is</p>
</section>
<section id="managing-risk-is-a-critical-aspect-of-decision-management-systems.-17" class="level4">
<h4 class="anchored" data-anchor-id="managing-risk-is-a-critical-aspect-of-decision-management-systems.-17">Managing risk is a critical aspect of Decision Management Systems. <sup>17</sup></h4>
<p>The first real commercial use of predictive analytics was to manage credit risk by predicting the likelihood that a consumer would miss a payment in the immediate future.</p>
<p>Suddenly there is all this data about who uses what power when. Using predictive analytics to find trends, segments with specific behaviors, and to predict how people might react to specific price changes will become the norm.</p>
<p>The process of building mathematical optimization models has similarities with its predictive analytics counterpart, but there are a few major differences worth highlighting to avoid confusion:</p>
<p>■ Although predictive models are generated by applying an algorithm to a data set, an optimization model is formulated by hand to represent a business problem by defining the decision variables, the objective, and the constraints.</p>
<p>■ Although the scope and input to a predictive model is often relatively small (such as information about a customer), the scope of an optimization model is usually a complex transaction or a set of transactions.</p>
<p>■ Predictive analytic models generally require access to large amounts of historical data that can be used to train the model. Optimization models can be run against historical data but do not require it.</p>
<p>■ Although invoking a predictive model in a Decision Service is relatively fast and simple—it simply involves evaluating a formula or interpreting a decision tree—solving an optimization model can consume significant time and memory, depending on the complexity of the model and size of the data. The optimization model must search a large set of possible actions to determine the one that best fits the constraints and goals.</p>
<p>Optimization is well established in supply chain problem domains where it is often used to define which products to make on which machines in a factory to maximize the value of products produced given restricted access to the various machines needed to make the products.</p>
<p>Similarly, your airplane seat, rental car, and hotel room are all likely to be priced using optimization technology.</p>
</section>
</section>
<section id="statistical-hypothesis-testing-18" class="level3">
<h3 class="anchored" data-anchor-id="statistical-hypothesis-testing-18">Statistical hypothesis testing <sup>18</sup></h3>
<p>A preliminary study may suggest that customers in the Northeast have a churn rate of 22.5%, whereas the nationwide average churn rate is only 15%. This may be just a chance fluctuation since the churn rate is not constant; it varies over regions and over time, so differences are to be expected. But the Northeast rate is one and a half times the U.S. average, which seems unusually high.</p>
<blockquote class="blockquote">
<p><em>What is the chance that this pattern or phenomenon is due to random variation? Statistical hypothesis testing is used to answer such questions.</em></p>
</blockquote>
</section>
<section id="data-science-19" class="level3">
<h3 class="anchored" data-anchor-id="data-science-19">Data science <sup>19</sup></h3>
<p>The statistician William S. Cleveland defined data science as an interdisciplinary field larger than statistics itself.</p>
<blockquote class="blockquote">
<p><em>We define data science as managing the process that can transform hypotheses and data into actionable predictions.</em></p>
</blockquote>
<p>Typical predictive analytic goals include predicting who will win an election, what products will sell well together, which loans will default, or which advertisements will be clicked on.</p>
<p>The data scientist is responsible for acquiring the data, managing the data, choosing the modeling technique, writing the code, and verifying the results.</p>
<section id="data-science-and-its-relationship-to-big-data-and-data-driven-decision-making-20" class="level4">
<h4 class="anchored" data-anchor-id="data-science-and-its-relationship-to-big-data-and-data-driven-decision-making-20">Data science and its relationship to Big Data and data-driven decision making <sup>20</sup></h4>
<blockquote class="blockquote">
<p><em>Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot—even ‘‘sexy’’—career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz.</em></p>
</blockquote>
<p>We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important</p>
<blockquote class="blockquote">
<ol type="i">
<li>to understand its relationships to these other important and closely related concepts,</li>
<li>to begin to understand what are the fundamental principles underlying data science.</li>
</ol>
</blockquote>
<p>At a high level, data science is a set of fundamental principles that support and guide the principled extraction of information and knowledge from data.</p>
<p>Data science is viewed as the connective tissue between data-processing technologies (including those for ‘‘big data’’) and data-driven decision making.</p>
<p>Probably the broadest business applications are in marketing for tasks such as targeted marketing, online advertising, and recommendations for cross-selling. Data science also is applied for general customer relationship management to analyze customer behavior in order to manage attrition and maximize expected customer value. The finance industry uses data science for credit scoring and trading and in operations via fraud detection and workforce management</p>
<p>A data-science perspective provides practitioners with structure and principles, which give the data scientist a framework to systematically treat problems of extracting useful knowledge from data.</p>
<p>Backed by the trillions of bytes’ worth of shopper history that is stored in Wal-Mart’s data warehouse, she felt that the company could ‘‘start predicting what’s going to happen, when Hurricane Charley struck, several weeks earlier, instead of waiting for it to happen,’’ as she put it.</p>
<p>The New York Times reported that: ‘‘.the experts mined the data and found that the stores would indeed need certain products— and not just the usual flashlights. ‘We didn’t know in the past that strawberry Pop-Tarts increase in sales, like seven times their normal sales rate, ahead of a hurricane,’ Ms.&nbsp;Dillman said in a recent interview.’ And the pre-hurricane top-selling item was beer.’’’2</p>
<p>How should MegaTelCo decide on the set of customers to target to best reduce churn for a particular incentive budget?</p>
<p>One standard deviation higher on the DDD scale is associated with a 4–6% increase in productivity. DDD also is correlated with higher return on assets, return on equity, asset utilization, and market value, and the relationship seems to be causal.</p>
<p>Our two example case studies illustrate two different sorts of decisions: (1) decisions for which ‘‘discoveries’’ need to be made within data, and (2) decisions that repeat, especially at massive scale, and so decision making can benefit from even small increases in accuracy based on data analysis. The Wal-Mart example above illustrates a type-1 problem. Linda Dillman would like to discover knowledge that will help Wal-Mart prepare for Hurricane Frances’s imminent arrival. Our churn example illustrates a type-2 DDD problem.</p>
<p>A large telecommunications company may have hundreds of millions of customers, each a candidate for defection. Tens of millions of customers have contracts expiring each month, so each one of them has an increased likelihood of defection in the near future. If we can improve our ability to estimate, for a given customer, how profitable it would be for us to focus on her, we can potentially reap large benefits by applying this ability to the millions of customers in the population. This same logic applies to many of the areas where we have seen the most intense application of data science and data mining:</p>
<p>Direct marketing, online advertising, credit scoring, financial trading, help-desk management, fraud detection, search ranking, product recommendation, and so on.</p>
<p>The use of big data technologies correlates with significant additional productivity growth. Specifically, one standard deviation higher utilization of big data technologies is associated with 1–3% higher productivity than the average firm; one standard deviation lower in terms of big data utilization is associated with 1–3% lower productivity. This leads to potentially very large productivity differences between the firms at the extremes.</p>
<p>Similarly, we see some companies already applying Big Data 2.0. Amazon again is a company at the forefront, providing data-driven recommendations from massive data. There are other examples as well. Online advertisers must process extremely large volumes of data (billions of ad impressions per day is not unusual) and maintain a very high throughput (real-time bidding systems make decisions in tens of milliseconds).</p>
<p>Managers and line employees in other functional areas will only get the best from the company’s datascience resources if they have some basic understanding of the fundamental principles.</p>
<p>Facebook, Twitter, Yahoo, Google, Amazon along with many other ‘‘Digital 100’’ companies,5 have high valuations due primarily to data assets they are committed to capturing or creating.</p>
<p><strong>Fundamentals concepts of data science:</strong></p>
<ol type="1">
<li><p>Extracting useful knowledge from data to solve business problems can be treated systematically by <em>following a process with reasonably well-defined stages</em>. The Cross-Industry Standard Process for Data Mining (CRISP-DM)</p></li>
<li><p>Evaluating data-science results requires careful consideration of the <em>context in which they will be used</em>: For our churn-management example, how exactly are we going to use the patterns that are extracted from historical data? More generally, does the pattern lead to better decisions than some reasonable alternative?</p></li>
<li><p>The relationship between the business problem and the analytics solution often can be <em>decomposed into tractable subproblems</em> via the framework of analyzing expected value. We have many specific tools for estimating probabilities and values from data. For our churn example, should the <em>value</em> of the customer be taken into account in addition to the likelihood of leaving?</p></li>
<li><p>Information technology can be used to <em>find informative data items</em> from within a large body of data: In our churn example, a quantity of interest is the likelihood that a particular customer will leave after her contract expires. Before the contract expires, this would be an unknown quantity. However, there may be known data items (usage, service history, how many friends have canceled contracts) that correlate with our quantity of interest. This fundamental concept underlies a vast number of techniques for statistical analysis, predictive modeling, and other data mining.</p></li>
<li><p>Entities that are similar with respect to <em>known features or attributes often are similar with respect to unknown features</em> or attributes.</p></li>
<li><p>If you look too hard at a set of data, <em>you will find something</em>—but it might not generalize beyond the data you’re observing. This is referred to as ‘‘overfitting’’ a dataset.</p></li>
<li><p>To draw causal conclusions, one must pay very close attention to the presence of confounding factors, possibly unseen ones.</p></li>
</ol>
<p>For example, it is common to see job advertisements mentioning data-mining techniques (random forests, support vector machines), specific application areas (recommendation systems, ad placement optimization), alongside popular software tools for processing big data (SQL, Hadoop, MongoDB).</p>
</section>
</section>
</section>
<section id="where-is-the-concept-of-data-science-heading" class="level2">
<h2 class="anchored" data-anchor-id="where-is-the-concept-of-data-science-heading">1.4 Where is the concept of data science heading?</h2>
<p>“Google is not really a search company. It’s a machine-learning company,” says Matthew Zeiler, the CEO of visual search startup Clarifai, who worked on Google Brain during a pair of internships. He says that all of Google’s most-important projects—autonomous cars, advertising, Google Maps—stand to gain from this type of research. “Everything in the company is really driven by machine learning.” <sup>21</sup></p>
<section id="machine-learning-applications-for-data-center-efficiency-optimization-22" class="level4">
<h4 class="anchored" data-anchor-id="machine-learning-applications-for-data-center-efficiency-optimization-22">Machine learning applications for data center efficiency optimization <sup>22</sup></h4>
<p>“The sheer number of possible equipment combinations and their setpoint values makes it difficult to determine where the optimal efficiency lies,” Gao writes in the white paper on his initiative. “In a live DC, it is possible to meet the target setpoints through many possible combinations of hardware (mechanical and electrical equipment) and software (control strategies and setpoints). Testing each and every feature combination to maximize efficiency would be unfeasible given time constraints, frequent fluctuations in the IT load and weather conditions, as well as the need to maintain a stable DC environment.”</p>
<p>“Google’s Search engine has tried to approximate human intelligence by rapidly analyzing enormous amounts of data, but people like LeCun (from Facebook) aim to build massive “neutral networks” that actually mimic the way the brain works.”[^23]</p>
<p>“Vern Brownell, D-Wave’s CEO, talks about the company’s ongoing efforts to prove the potential of its hardware and its plans for the future … third category is in the broad area of machine learning, which we think is one of the most exciting things going on in computer science today.”<sup>23</sup></p>
<p>“In 2000, physicist Alexei Kitaev (then at Microsoft Research) proposed that a mysterious quasiparticle known as a Majorana could be used in quantum information processing, showing that Majoranas located at opposite ends of a quantum wire could effectively create a topologically protected qubit. Five years later, Das Sarma – along with Freedman and Chetan Nayak, Station Q’s other main leader – co-authored a paper suggesting an experimental proposal for creating a topologically protected qubit using something called the “fractional quantum Hall” system (twodimensional electron gas in a strong magnetic field) along with a similar quasiparticle. These important discoveries pointed to a promising new direction for protecting qubits, and therefore getting them to behave. After all, qubits working together in harmony is fundamental to getting them to compute.” <sup>24</sup></p>
<p>“Viv is not the only company competing for a share of those billions. The field of artificial intelligence has become the scene of a frantic corporate arms race, with Internet giants snapping up AI startups and talent. …Their goal is to build a new generation of AI that can process massive troves of data to predict and fulfill our desires.” <sup>25</sup></p>
<p>Google 3D phone/Amazon 3D phone. IBM Neural chip.</p>
</section>
</section>
<section id="some-of-the-current-trends." class="level2">
<h2 class="anchored" data-anchor-id="some-of-the-current-trends.">1.5 Some of the current trends.</h2>
<ul>
<li><p>July 2009 - <em>IBM</em> to buy <em>SPSS</em>(statistics software vendor) for $1.2 Billion to gain analytics</p></li>
<li><p>Mar 2011 - <em>Via Science</em> Acquires <em>Dataspora</em>, a pioneer in predictive analytics to leverage proprietary machine learning platform, REFS™, beyond healthcare and financial services.</p></li>
<li><p>Oct 2013 - <em>SAP</em> buys <em>KXEN</em>(statistics software vendor) to further predictive analytics</p></li>
<li><p>Feb 2014 – <em>Workday</em> acquires HR predictive analytics company <em>Identified</em></p></li>
<li><p>Mar 2014 - <em>Dell</em> acquires <em>StatSoft</em> (statistics software vendor) and the list of predictive platform vendors gets even shorter… by Simon Arkell, CEO, Predixion Software</p></li>
<li><p>Jun 2014 - <em>Nokia</em>’s HERE mapping service continued its buying spree with today’s acquisition of real-time predictive analytics firm <em>Medio Systems</em></p></li>
<li><p>Jul 2014 - <em>Twitter</em> acquires image search firm <em>Madbits</em> (uses deep learning techniques to understand the content of an image)</p></li>
</ul>
</section>
<section id="what-would-typical-application-areas-of-data-science-be" class="level2">
<h2 class="anchored" data-anchor-id="what-would-typical-application-areas-of-data-science-be">1.6 What would typical application areas of data science be?</h2>
<section id="microsoft-machine-learning-software" class="level4">
<h4 class="anchored" data-anchor-id="microsoft-machine-learning-software">Microsoft Machine Learning software</h4>
<p>OSISoft is working with Carnegie Mellon University on real time fault detection and the diagnosis of energy output variations across campus buildings. Machine learning is helping to mitigate issues in real time and to predictably optimize energy usage and cost.</p>
<p>GitHub: sebastianbk/BreastCancerNeuralNetwork Implementation of a Neural Network in .NET using the Diagnostic Wisconsin Breast Cancer Database. After completing the example with the Breast Cancer data set by coding it myself, I thought of using Azure Machine Learning to do the same job.</p>
<p><strong>Kaggle, the leading platform for predictive modeling competitions:</strong></p>
<ul>
<li><p><em>UPenn and Mayo Clinic’s Seizure Detection Challenge</em>: Detect seizures in intracranial EEG recordings</p></li>
<li><p><em>The Heritage Provider Network (HPN)</em>: The goal of the prize is to develop a predictive algorithm that can identify patients who will be admitted to a hospital within the next year, using historical claims data.</p></li>
<li><p><em>Dunnhumby</em>, a U.K. firm that does analytics for supermarket chains, was looking to build a model to predict when supermarket shoppers will next visit the store and how much they will spend.</p></li>
<li><p><em>BoehringerIngelheimBioSciences</em>: Predict a biological response of molecules from their chemical properties as optimally as this data allows, relate molecular information, to an actual biological response.</p></li>
<li><p><em>Jetpac is an online travel magazine startup</em>: Given anonymized information on thousands of photo albums, predict whether a human evaluator would mark them as ‘good’.</p></li>
<li><p><em>AllState Insurance</em>: The goal of the Claim Prediction Challenge was to predict bodily injury liability, based solely on the characteristics of the insured vehicle.</p></li>
</ul>
</section>
<section id="ibm-life-sciences" class="level4">
<h4 class="anchored" data-anchor-id="ibm-life-sciences">IBM Life Sciences</h4>
<p>Ricardo Machado, (IBM, Brazil) published many papers on neural networks and a predictive expert system named <em>Next</em>. The power of this system stemmed from its ability to use “knowledge graphs” obtained from interviews with medical experts to form the basis of a model capable of altering these graphs when presented with data, thus transforming them into an artificial neural network. <em>Next</em> was successfully used to diagnose and classify kidney diseases.</p>
<p>Beatriz Leao (IBM, Brazil), developed a system called HYCONES, which also combined symbolic knowledge and neural networks. It was able to successfully detect and classify congenital heart diseases. The results of the work were published in M.D.&nbsp;Computing in 1994.</p>
</section>
<section id="stanford-machine-learning-projects-cs229" class="level4">
<h4 class="anchored" data-anchor-id="stanford-machine-learning-projects-cs229">Stanford Machine Learning Projects (CS229)</h4>
<ul>
<li><p>Characterizing and diagnosing hypertrophic cardiomyopathy from ECG data.</p></li>
<li><p>Electrical energy modeling in Y2E2 building based on distributed sensors information.</p></li>
<li><p>Predicting semantic features from CT images of liver lesions using deep learning.</p></li>
<li><p>Machine learning classification of kidney and lung cancer types.</p></li>
<li><p>Gaussian process based image segmentation and object detection in pathology slides.</p></li>
<li><p>Listen to your heart: stress prediction using consumer heart rate sensors.</p></li>
</ul>
</section>
</section>
</section>
<section id="analysis-techniques-tools" class="level1">
<h1>2. ANALYSIS TECHNIQUES &amp; TOOLS</h1>
<section id="top-ten-algorithms-in-data-mining-27" class="level2">
<h2 class="anchored" data-anchor-id="top-ten-algorithms-in-data-mining-27">2.1 Top ten algorithms in data mining <sup>26</sup></h2>
<ol type="1">
<li><p>C4.5</p></li>
<li><p><em>K</em>-Means</p></li>
<li><p>SVM: Support Vector Machines</p></li>
<li><p>Apriori</p></li>
<li><p>Expecation Maximization</p></li>
<li><p>PageRank</p></li>
<li><p>AdaBoost</p></li>
<li><p><em>k</em>NN: <em>k</em>-Nearest Neighbors.</p></li>
<li><p>Naïve Bayes</p></li>
<li><p>CART: Classification and Regression Trees</p></li>
</ol>
</section>
<section id="microsoft-sql-server-analysis-services-ssas-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="microsoft-sql-server-analysis-services-ssas-algorithms">2.2 Microsoft SQL Server Analysis Services (SSAS) algorithms</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 59%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Examples of tasks</strong></th>
<th><strong>Microsoft algorithms to use</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Predicting a discrete attribute</strong> - Flag the customers in a prospective buyers list as good or poor prospects. - Calculate the probability that a server will fail within the next 6 months.- Categorize patient outcomes and explore related factors.</td>
<td>* Decision Trees Algorithm * Naive Bayes Algorithm * Clustering Algorithm * Neural Network Algorithm</td>
</tr>
<tr class="even">
<td><strong>Predicting a continuous attribute</strong> - Forecast next year’s sales. - Predict site visitors given past historical and seasonal trends. - Generate a risk score given demographics.</td>
<td>* Decision Trees Algorithm * Time Series Algorithm * Linear Regression Algorithm</td>
</tr>
<tr class="odd">
<td><strong>Predicting a sequence</strong> - Perform clickstream analysis of a company’s Web site. - Analyze the factors leading to server failure. - Capture and analyze sequences of activities during outpatient visits, to formulate best practices around common activities.</td>
<td>* Sequence Clustering Algorithm</td>
</tr>
<tr class="even">
<td><strong>Finding groups of common items in transactions</strong> - Use market basket analysis to determine product placement. - Suggest additional products to a customer for purchase. - Analyze survey data from visitors to an event, to find which activities or booths were correlated, to plan future activities.</td>
<td>* Association Algorithm * Decision Trees Algorithm</td>
</tr>
<tr class="odd">
<td><strong>Finding groups of similar items</strong> - Create patient risk profiles groups based on attributes such as demographics and behaviors. - Analyze users by browsing and buying patterns. - Identify servers that have similar usage characteristics.</td>
<td>* Clustering Algorithm * Sequence Clustering Algorithm</td>
</tr>
</tbody>
</table>
</section>
<section id="madlib-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="madlib-algorithms">2.3 MADlib algorithms</h2>
<p>Key philosophies driving the architecture of MADlib are:</p>
<ul>
<li><p>Operate on the data locally—<strong>in database</strong>. Do not move it between multiple runtime environments unnecessarily.</p></li>
<li><p>Utilize best of breed database engines, but <strong>separate</strong> the machine learning logic from database specific implementation details.</p></li>
<li><p>Leverage MPP Share nothing technology, such as the Pivotal Greenplum Database, to provide parallelism and <strong>scalability</strong>.</p></li>
<li><p><strong>Open</strong> implementation maintaining active ties into ongoing academic research.</p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 12%">
<col style="width: 87%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Technique</strong></th>
<th><strong>Example Use Cases</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Linear Regression</td>
<td>can be used to model a linear relationship of a scalar dependent variable to one or more explanatory independent variables.</td>
</tr>
<tr class="even">
<td>Latent Dirichlet Allocation</td>
<td>is a topic modeling function used to identify recurring themes in a large document corpus.</td>
</tr>
<tr class="odd">
<td>Summary Function</td>
<td>provides summary statistics for any data table. These statistics include statistics such as: number of distinct values, number of missing values, mean, variance, min, max, most frequent values, quantiles, etc.</td>
</tr>
<tr class="even">
<td>Logistic Regression</td>
<td>can be used to predict a binary outcome of a dependent variable from one or more explanatory independent variables.</td>
</tr>
<tr class="odd">
<td>Elastic Net Regularization</td>
<td>is a regularization technique that can be implemented for either linear or logistic regression to help build a more robust model in the event of large numbers of explanatory independent variables.</td>
</tr>
<tr class="even">
<td>Principal Component Analysis</td>
<td>is a dimensional reduction technique that can be used to transform a high dimensional space into a lower dimensional space.</td>
</tr>
<tr class="odd">
<td>Apriori</td>
<td>is a technique for evaluating frequent item-sets, which allows analysis of what events tend to occur together. For instance what items customers frequently purchase in a single transaction.</td>
</tr>
<tr class="even">
<td>k-Means Clustering</td>
<td>is a clustering method used to identify regions of similarity within a dataset. It can be used for many types of analysis including customer segmentation analysis.</td>
</tr>
</tbody>
</table>
</section>
<section id="oracle-advanced-analytics" class="level2">
<h2 class="anchored" data-anchor-id="oracle-advanced-analytics">2.4 Oracle advanced analytics</h2>
<table class="caption-top table">
<thead>
<tr class="header">
<th><strong>Technique</strong></th>
<th><strong>Applicability</strong></th>
<th><strong>Algorithms</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="./media/image9.jpg">./media/image9.jpg</a> Classification</td>
<td>Most commonly used technique for predicting a specific outcome such as response / no-response, high / medium / low-value customer, likely to buy / not buy.</td>
<td>* Logistic Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Naive Bayes —Fast, simple, commonly applicable * Support Vector Machine—Next generation, supports text and wide data. Decision Tree —Popular, provides human-readable rules.</td>
</tr>
<tr class="even">
<td><a href="./media/image10.jpg">./media/image10.jpg</a> Regression</td>
<td>Technique for predicting a continuous numerical outcome such as customer lifetime value, house value, process yield rates.</td>
<td>* Multiple Regression—classic statistical technique but now available inside the Oracle Database and supports text and transactional data * Support Vector Machine</td>
</tr>
<tr class="odd">
<td><a href="./media/image11.jpg">./media/image11.jpg</a> Attribute Importance</td>
<td>Ranks attributes according to strength of relationship with target attribute. Use cases include finding factors most associated with customers who respond to an offer, factors most associated with healthy patients.</td>
<td>* Minimum Description – Considers each attribute as a simple predictive model of the target classLength—</td>
</tr>
<tr class="even">
<td><a href="./media/image12.jpg">./media/image12.jpg</a> Anomaly Detection</td>
<td>Identifies unusual or suspicious cases based on deviation from the norm. Common examples include health care fraud, expense report fraud, and tax compliance.</td>
<td>* One-Class Support Vector Machine — Trains on “normal” cases to flag unusual cases</td>
</tr>
<tr class="odd">
<td><a href="./media/image13.jpg">./media/image13.jpg</a> Clustering</td>
<td>Useful for exploring data and finding natural groupings. Members of a cluster are more like each other than they are like members of a different cluster. Common examples include finding new customer segments, and life sciences discovery.</td>
<td>* Enhanced K-Means—Supports text mining, hierarchical clustering, distance based * Orthogonal Partitioning Clustering— Hierarchical clustering, density based. * Expectation Maximization—Clustering technique that performs well in mixed data (dense and sparse) data mining problems.</td>
</tr>
<tr class="even">
<td><a href="./media/image14.jpg">./media/image14.jpg</a> Association</td>
<td>Finds rules associated with frequently co-occurring items, used for market basket analysis, cross-sell, root cause analysis. Useful for product bundling, instore placement, and defect analysis.</td>
<td>* Apriori—Industry standard for market basket analysis</td>
</tr>
<tr class="odd">
<td><a href="./media/image15.jpg">./media/image15.jpg</a> Feature Selection and Extraction</td>
<td>Produces new attributes as linear combination of existing attributes. Applicable for text data, latent semantic analysis, data compression, data decomposition and projection, and pattern recognition.</td>
<td>Non-negative Matrix Factorization— Next generation, maps the original data into the new set of attributes. * Principal Components Analysis (PCA)— creates new fewer composite attributes that represent all the attributes. * Singular Vector Decomposition— established feature extraction method that has a wide range of applications.</td>
</tr>
</tbody>
</table>
</section>
<section id="apache-mahout-for-hadoop" class="level2">
<h2 class="anchored" data-anchor-id="apache-mahout-for-hadoop">2.5 Apache Mahout for Hadoop</h2>
<p>The Apache Mahout project’s goal is to build a scalable machine learning library. There is some degree of overlap with big data analytics within a Hadoop</p>
<p>Mahout includes algorithms for clustering, classification and collaborative filtering. You can also find:</p>
<ol type="1">
<li><p>Matrix factorization based recommenders</p></li>
<li><p>K-Means, Fuzzy K-Means clustering</p></li>
<li><p>Latent Dirichlet Allocation</p></li>
<li><p>Singular Value Decomposition</p></li>
<li><p>Logistic regression classifier</p></li>
<li><p>(Complementary) Naive Bayes classifier</p></li>
<li><p>Random forest classifier</p></li>
</ol>
</section>
<section id="microsoft-azure-cloud-machine-learning-studio" class="level2">
<h2 class="anchored" data-anchor-id="microsoft-azure-cloud-machine-learning-studio">2.6 Microsoft Azure cloud machine learning studio</h2>
<p>There is a pool of VMs running machine learning algorithms using an orchestration engine, freeing the data scientist from moving data and moving to different services.</p>
<p>The ML Studio is targeting the emerging data scientists. You can train 10 models in minutes, not days. You can put a predictive model into production in minutes, not weeks or months. Some customers are reporting a 10X-100X in reduction in cost relative to SAS.</p>
<p>Employees can create their own <strong>workspaces</strong>, giving re-use and cross-teaming and sharing models with others.</p>
<p>The predictive models can be shared as a service across an enterprise leverage Azure as the <strong>public cloud back-end</strong>. For example, you can write JSON-based back ends that leverage your predictive models, allowing you to build decision making dashboards for your business.</p>
<p>Machine Learning algorithms are built to continually improve over time by leverage <strong>training sets</strong>. Training sets make it possible to continually improve the robustness of your predictive model.</p>
<p>The good news is that R is easily integrated into ML Studio. Right now, R is dominant in machine learning space.</p>
</section>
<section id="google-prediction-api" class="level2">
<h2 class="anchored" data-anchor-id="google-prediction-api">2.7 Google Prediction API</h2>
<p>Google’s cloud-based machine learning tools can help analyze your data to add the following features to your applications:</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/2b518674cc121ac2d8d3a372c458b7b5.jpg" class="img-fluid"></p>
<p>Customer sentiment analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/affb8303438400c6e1bffaaa17d3acfc.jpg" class="img-fluid"></p>
<p>Spam detection</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/0319b7f3396d5da0331e2db1b9ea8b27.jpg" class="img-fluid"></p>
<p>Message routing decisions</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/7f61a6e99693c838ff18523723e213b2.jpg" class="img-fluid"></p>
<p>Upsell opportunity analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/72186c0ef6af31d95093b7e6e820dcee.jpg" class="img-fluid"></p>
<p>Document and email classification</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/4d6d333f4a8f94358a8e6e734c18d231.jpg" class="img-fluid"></p>
<p>Diagnostics</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/9e22d36bd5b087be04b49beb6bfa711b.jpg" class="img-fluid"></p>
<p>Churn analysis</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/b8f86c71b8d5cd1a53e04b69dcf0672d.jpg" class="img-fluid"></p>
<p>Suspicious activity identification</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/4f3eab4f9b3c79843ee03106315f36b9.jpg" class="img-fluid"></p>
<p>Recommendation systems</p>
<p><img src="https://fortunewalla.github.io/fw2/posts/10-businessopportunities/media/87cfd1e3c26d37315e249d627ff800cd.jpg" class="img-fluid"></p>
<p><em>And much more…</em></p>
</section>
<section id="implementation-options" class="level2">
<h2 class="anchored" data-anchor-id="implementation-options">2.8 Implementation options</h2>
<section id="hardware" class="level3">
<h3 class="anchored" data-anchor-id="hardware">2.8.1 Hardware</h3>
<p>Offline: Stand-Alone machine, private cloud.</p>
<p>Online: shared hosting, VPS, Virtual Machine</p>
<p>Hosting:1000/month</p>
<p>VPS: 2000-5000/month</p>
<p>Cloud: AzureML, Google Predictive API, Amazon, Oracle DM, ADAPA Cloud, FICO Cloud, SAS Cloud, Mathematica Cloud</p>
</section>
<section id="software" class="level3">
<h3 class="anchored" data-anchor-id="software">2.8.2 Software</h3>
<p>OS: Windows. Linux/UNIX:</p>
<p>Databases: SQL Server, Oracle DM, MySQL, MariaDB, Hadoop, NOSQL</p>
<p>Client End: Excel Add-ins</p>
<p>Analysis Packages: MS Excel, SAS, SPSS [ibm], Statistica [dell], STATA, KXEN [sap]</p>
<p>MADLIB ~ SQL library for databases</p>
<p>Languages: sql, python, perl, awk, sed, vba, R, C#, F#, julia</p>
<p>Graphics/Visualization: 2d/3d ggplot2</p>
<ul>
<li><p>Paid: SAS (best), IBM SPSS, Statistica, Stata, MS Excel (most versatile), MS SQL Server Analysis Services (Built-in Data mining Algorithms implemented as SQL based queries &amp; GUI tools), Oracle Data Mining (ODM), Predixion Tools.</p></li>
<li><p>OSS: WekaGUI (Machine Learning), KNIME, R, Python etc….</p></li>
</ul>
</section>
<section id="standards" class="level3">
<h3 class="anchored" data-anchor-id="standards">2.8.3 Standards</h3>
<p>CRISP-DM (Cross Industry Standard Process for Data Mining): Business Understanding, Data Understanding, Data Preparation, Modeling, Evaluation, Deployment</p>
<p>SEMMA (Sample, Explore, Modify, Model and Assess) by SAS Inc.</p>
<p>PMML (Predictive Model Markup Language) – can exchange predictive models across software using XML. Zemetis, The Data Mining Group (dmg.org)</p>
<p>CONCLUSION</p>
</section>
</section>
</section>
<section id="conclusion" class="level1">
<h1>3. CONCLUSION</h1>
<section id="why-even-bother" class="level2">
<h2 class="anchored" data-anchor-id="why-even-bother">3.1 Why even bother?</h2>
<blockquote class="blockquote">
<p><em>CAPABILITY</em></p>
</blockquote>
</section>
<section id="future-trends" class="level2">
<h2 class="anchored" data-anchor-id="future-trends">3.2 Future trends?</h2>
<ul>
<li><p>The big players of the internet are investing in deep learning, AI &amp; quantum computing to deal with overflowing data.</p></li>
<li><p>All technologies considered hi-tech or cutting-edge are becoming commercialized for normal businesses.</p></li>
<li><p>Novell Computers or IBM Personal computers were very specialized services. But now are commoditized.</p></li>
<li><p>Now we have a situation where data sets are getting larger and the software to do advanced analysis &amp; create prediction models from the vast amounts of data is getting cheaper (SAS).</p></li>
<li><p>Smaller firms, individuals &amp; startups are getting a level playing field in terms of cost of operations, renting cloud, machines, people (expertise is flowing from research labs to engineering &amp; design depts. of companies.) etc…</p></li>
<li><p>“20-25% of outsourcing contracts now relating to Cloud; IT services companies that don’t invest in Cloud may be making a mistake, say experts”<sup>27</sup></p></li>
</ul>
</section>
<section id="business-opportunities-from-the-analysis-of-customer-data." class="level2">
<h2 class="anchored" data-anchor-id="business-opportunities-from-the-analysis-of-customer-data.">3.3 Business opportunities from the analysis of customer data.</h2>
<p>They include:</p>
<ul>
<li><p>Basic reporting &amp; analysis</p></li>
<li><p>Advanced/premium/value-added services</p></li>
<li><p>Engineering, quality control of production, manufacturing.</p></li>
<li><p>Wear/Tear, Performance of sensors / devices. • Optimization of resources for customers</p></li>
<li><p>Sell raw data to bigger firms.</p></li>
<li><p>Sell value added data to bigger firms.</p></li>
<li><p>Sell data services to bigger firms.</p></li>
<li><p>Get contracts from bigger firms.</p></li>
<li><p>Create a marketplace for sensors with advanced analysis in the commercial, industrial or consumer space.</p></li>
<li><p>Intellectual property creation</p></li>
</ul>
<p>For developing countries, it is a wide open market. They have basic electronic infrastructure. Most of the infrastructure in not “smart” or even standardized for advanced data analysis &amp; automated solutions. Less competition especially in the industrial, hardware, electronics, and engineering domain.</p>
<p>Data science in the enterprise</p>
<p>© 2014 by data science services</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>[^23] CADE METZ “<em>Facebook Taps ‘Deep Learning’ Giant for New AI Lab”</em> Wired Magazine (12.09.13) www.wired.com/2013/12/facebook-yann-lecun/</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>MIKE LOUKIDES <em>What is data science? The future belongs to the companies and people that turn data into products.</em> (June, 2010) radar.oreilly.com/2010/06/what-is-data-science.html↩︎</p></li>
<li id="fn2"><p>COLLEEN MCCUE <em>Data Mining and Predictive Analysis</em> (2007) Elsevier Inc1.↩︎</p></li>
<li id="fn3"><p>F. VAN DER HEIJDEN <em>Classification, parameter estimation and state estimation: an engineering approach using MATLAB</em> (2004) John Wiley &amp; Sons Ltd.↩︎</p></li>
<li id="fn4"><p>ANDREW R. WEBB AND KEITH D. COPSEY <em>Statistical Pattern Recognition, Third Edition</em> (2011) John Wiley &amp; Sons, Ltd↩︎</p></li>
<li id="fn5"><p>S.B. GREEN, N. J. SALKIND &amp; T. M. AKEY <em>Using SPSS for Windows and Macintosh: Analyzing and understanding data</em> (2008) Prentice Hall. New Jersey↩︎</p></li>
<li id="fn6"><p>KRZYSZTOF J. CIOS Data <em>Mining: A Knowledge Discovery Approach</em> (2007) Springer Science+Business Media, LLC↩︎</p></li>
<li id="fn7"><p>USAMA FAYYAD, GREGORY PIATETSKY-SHAPIRO AND PADHRAIC SMYTH <em>“From Data Mining to Knowledge Discovery in Databases”</em> AI Magazine (1996), vol.&nbsp;17 No.3, pp.37—54↩︎</p></li>
<li id="fn8"><p>KRZYSZTOF J. CIOS <em>Data Mining: A Knowledge Discovery Approach</em> (2007) Springer Science+Business Media, LLC↩︎</p></li>
<li id="fn9"><p>MOHAMED MEDHAT GABER (Editor) Scientific <em>Data Mining and Knowledge Discovery: Principles and Foundations</em> (2010) Springer-Verlag Berlin Heidelberg↩︎</p></li>
<li id="fn10"><p>PETER FLACH <em>Machine learning: The Art and Science of Algorithms that Make Sense of Data</em> (2012) Cambridge University Press, New York↩︎</p></li>
<li id="fn11"><p>WILLI RICHERT <em>Building Machine Learning Systems with Python</em> (2013) Packt Publishing↩︎</p></li>
<li id="fn12"><p>STEPHEN MARSLAND <em>Machine Learning: An Algorithmic Perspective</em> (2009) CRC Press↩︎</p></li>
<li id="fn13"><p>ANASSE BARI <em>Predictive Analytics for Dummies</em> (2013) Wiley↩︎</p></li>
<li id="fn14"><p>CONRAD CARLBERG <em>Predictive Analytics with Microsoft Excel</em> (2013) QUE↩︎</p></li>
<li id="fn15"><p>MICHAEL WESSLER <em>Predictive Analytics For Dummies, Alteryx Special Edition</em> (2014) Wiley↩︎</p></li>
<li id="fn16"><p>ERIC SIEGEL <em>Predictive Analytics: The Power to Predict who will click, buy, lie or die</em> (2013)↩︎</p></li>
<li id="fn17"><p>JAMES TAYLOR <em>Decision Management Systems - A Practical Guide to Using Business Rules and Predictive Analytics</em> (2012) IBM Press↩︎</p></li>
<li id="fn18"><p>FOSTER PROVOST AND TOM FAWCETT <em>Data Science for Business</em> (2013) O’Reilly Media, Inc↩︎</p></li>
<li id="fn19"><p>NINA ZUMEL <em>Practical Data Science with R</em> (2014) Manning Publications↩︎</p></li>
<li id="fn20"><p>FOSTER PROVOST AND TOM FAWCETT <em>“Data Science and its Relationship to Big Data and Data-Driven Decision Making”</em> Big Data (March 2013), 1(1): 51-59.↩︎</p></li>
<li id="fn21"><p>ROBERT MCMILLAN <em>“Inside the Artificial Brain That’s Remaking the Google Empire”</em> Wired Magazine (07.16.14) www.wired.com/2014/07/google_brain/↩︎</p></li>
<li id="fn22"><p>www.datacenterknowledge.com/archives/2014/05/28/google-using-machine-learning-boost-data-center-efficiency/2/↩︎</p></li>
<li id="fn23"><p>RACHEL COURTLAND <em>“D-Wave Aims to Bring Quantum Computing to the Cloud”</em> (9 Apr 2014) IEEE Spectrum http://spectrum.ieee.org/podcast/computing/hardware/dwave-aims-to-bring-quantum-computing-to-the-cloud↩︎</p></li>
<li id="fn24"><p>www.microsoft.com/en-us/news/stories/stationq/index.html↩︎</p></li>
<li id="fn25"><p>STEVEN LEVY <em>“Siri’s Inventors Are Building a Radical New AI That Does Anything You Ask”</em> Wired Magazine (08.12.14) www.wired.com/2014/08/viv/↩︎</p></li>
<li id="fn26"><p>XINDONG WU AND VIPIN KUMAR (eds.) <em>The Top Ten Algorithms in Data Mining</em> (2009) Chapman and Hall/CRC Press↩︎</p></li>
<li id="fn27"><p>ITIKA SHARMA PUNIT <em>“Cloud: The reality that enterprises cannot escape”</em> Business Standard Newspaper Bangalore Edition (July 7, 2014)↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>article</category>
  <category>product</category>
  <guid>https://fortunewalla.github.io/fw2/posts/10-businessopportunities/</guid>
  <pubDate>Thu, 31 Jul 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Connect to and query Microsoft SQL Server using SAS/ACCESS Interface to ODBC</title>
  <link>https://fortunewalla.github.io/fw2/posts/09-connectquerymssqlsasinterfaceodbc/</link>
  <description><![CDATA[ 




<p><em>Created on Sunday, May 18th, 2014 at 3:24 am</em></p>
<p>One of the important skills in SAS is being able to connect &amp; query a local or remote database, conduct data analysis in SAS &amp; write the new information back to the database. SAS provides access to a variety of databases as well as different ways of doing so. In this article, access to Microsoft SQL Server (MSSQL) using SAS/ACCESS ODBC interface is explained. If you are using SAS at an institute or organization, your Systems Administrator would have setup everything. The steps in this article might vary depending on the software configuration.</p>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p><strong>SAS methods for accessing relational database data [1]</strong></p>
<p>SAS/ACCESS Interface to Relational Databases is a family of interfaces each licensed separately with which you can interact with data in other vendor databases from within SAS. SAS/ACCESS provides these methods for accessing relational DBMS data.</p>
<ul>
<li><p>You can use the <strong>LIBNAME</strong> statement to assign SAS librefs to DBMS objects such as schemas and databases. After you associate a database with a libref, you can use a SAS two-level name to specify any table or view in the database. You can then work with the table or view as you would with a SAS data set.</p></li>
<li><p>You can use the <strong>SQL pass-through</strong> facility to interact with a data source using its native SQL syntax without leaving your SAS session. SQL statements are passed directly to the data source for processing.</p></li>
<li><p>You can use <strong>ACCESS</strong> and <strong>DBLOAD</strong> procedures for indirect access to DBMS data. Although SAS still supports these procedures for database systems and environments on which they were available for SAS 6, they are no longer the recommended method for accessing DBMS data.</p></li>
</ul>
<p>Not all SAS/ACCESS interfaces support all of these features. To determine which features are available in your environment, see SAS documentation.</p>
<p><strong>SAS/ACCESS interfaces to connect with MSSQL. [1]</strong> SAS provides many ways to establish a connection to a MSSQL database depending on your site license. Some of them are:</p>
<ul>
<li><p>SAS/ACCESS Interface to <strong>Microsoft SQL Server:</strong> This has been tested and certified against Data Direct Technologies Connect ODBC and Data Direct SequeLink ODBC products.</p></li>
<li><p>SAS/ACCESS Interface to <strong>ODBC</strong>, including ParAccel, Microsoft Parallel Data Warehouse, and more: Open database connectivity (ODBC) standards provide a common interface to a variety of data sources. The goal of ODBC is to enable access to data from any application, regardless of which DBMS handles the data.</p></li>
<li><p>SAS/ACCESS Interface to <strong>OLE DB:</strong> Microsoft OLE DB is an application programming interface (API) that provides access to data that can be in a database table, an e-mail file, a text file, or another type of file. This SAS/ACCESS interface accesses data from these sources through OLE DB data providers such as Microsoft Access, Microsoft SQL Server, and Oracle.</p></li>
</ul>
</section>
<section id="software-components-setup" class="level3">
<h3 class="anchored" data-anchor-id="software-components-setup">Software components setup:</h3>
<p><strong>1) ODBC DSN Source Setup:</strong> Please read the previous article about setting up MSSQL to accept remote connections. Make sure you setup the DSN to connect with SQL Server for your OS. Your institution/organization System Administrator should help you out with this. This is the procedure for Windows OS: <a href="http://broadwin.com/Manual/EngMan/23.3.3_Create_an_ODBC_DSN_to_SQL_Server.htm" class="uri">http://broadwin.com/Manual/EngMan/23.3.3_Create_an_ODBC_DSN_to_SQL_Server.htm</a></p>
<p><strong>2) SAS/ACCESS Interfaces availability:</strong> Before starting make sure your SAS installation contains the SAS/ACCESS Interface tools to connect with external databases.</p>
<p><strong>SOURCE CODE:</strong></p>
<pre class="sas"><code>Proc setinit noalias;
Run;</code></pre>
<p><strong>OUTPUT:</strong> Partial output shown below.</p>
<pre class="sas"><code>NOTE: PROCEDURE SETINIT used (Total process time):
real time 0.04 seconds
cpu time 0.00 seconds
Product Expiration Dates:
---SAS/ACCESS Interface to Microsoft SQL Server
14DEC2015
---SAS/ACCESS Interface to ODBC
14DEC2015
---SAS/ACCESS Interface to OLE DB
14DEC2015
---SAS/ACCESS Interface to Teradata
14DEC2015</code></pre>
<p>Ideally you should have Microsoft SQL Server, ODBC &amp; OLE DB. However, having only ODBC is enough, since most databases support ODBC</p>
</section>
<section id="sas-odbc-mssql-connection-examples2" class="level3">
<h3 class="anchored" data-anchor-id="sas-odbc-mssql-connection-examples2">SAS ODBC MSSQL connection examples:[2]</h3>
<p><strong>1) Setting up SAS ODBC MSSQL connection with prompt using LIBNAME statement.</strong> This uses the prompt method. The <code>SYSDBMSG</code> variable is used to write the connection details to the SAS LOG.</p>
<p><strong>SOURCE CODE:</strong></p>
<pre><code>libname sql odbc prompt;
%put %superq(sysdbmsg);</code></pre>
<ol type="a">
<li>You are prompted to select a DSN source.</li>
<li>You are prompted to enter the username &amp; password of the remote MSSQL connection.</li>
<li>If the connection is successful, the following message is displayed.</li>
</ol>
<p><strong>OUTPUT:</strong></p>
<pre><code>**SAS LOG:**
10 libname sql odbc prompt;
NOTE: Libref SQL was successfully assigned as follows:
Engine: ODBC
Physical Name: dbpcsql
11 %put %superq(sysdbmsg);
ODBC:
DSN=dbpcsql;Description=dbpcsql;UID=userName;PWD=passWord;APP=SAS 9.2 for Windows;WSID=DBPC;Network=DBMSSOCN</code></pre>
<p><strong>2) Setting up SAS ODBC MSSQL connection to <em>master</em> database using LIBNAME statement:</strong></p>
<p><strong>SOURCE CODE:</strong></p>
<pre><code>LIBNAME mssql odbc user='userName' password='passWord' datasrc=dbpcsql schema=dbo;

data new;
set mssql.spt_monitor;
run;

proc print data=mssql.spt_monitor;run;</code></pre>
<p><strong>OUTPUT:</strong></p>
<pre><code>**SAS LOG:**
1 LIBNAME mssql odbc user='userName' password=XXXXXX datasrc=dbpcsql schema=dbo;

NOTE: Libref MSSQL was successfully assigned as follows:
Engine: ODBC
Physical Name: dbpcsql
2 data new;
3 set mssql.spt_monitor;
4 run;

NOTE: There were 1 observations read from the data set
MSSQL.spt_monitor.
NOTE: The data set WORK.NEW has 1 observations and 11 variables.
NOTE: DATA statement used (Total process time):
real time 0.34 seconds
cpu time 0.03 seconds

5 proc print data=mssql.spt_monitor;run;
NOTE: Writing HTML Body file: sashtml.htm
NOTE: PROCEDURE PRINT used (Total process time):
real time 1.28 seconds
cpu time 0.28 seconds
**Results Viewer:sashtml**</code></pre>
<p>The SAS System</p>
<table class="caption-top table">
<colgroup>
<col style="width: 8%">
<col style="width: 12%">
<col style="width: 7%">
<col style="width: 6%">
<col style="width: 5%">
<col style="width: 9%">
<col style="width: 7%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 8%">
<col style="width: 9%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Obs</strong></th>
<th><strong>lastrun</strong></th>
<th><strong>cpu_busy</strong></th>
<th><strong>io_busy</strong></th>
<th><strong>idle</strong></th>
<th><strong>pack_received</strong></th>
<th><strong>pack_sent</strong></th>
<th><strong>connections</strong></th>
<th><strong>pack_errors</strong></th>
<th><strong>total_read</strong></th>
<th><strong>total_write</strong></th>
<th><strong>total_errors</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>02APR2010:17:34:58.817</td>
<td>9</td>
<td>7</td>
<td>792</td>
<td>28</td>
<td>28</td>
<td>14</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<pre><code>  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Obs**   **lastrun**              **cpu_busy**   **io_busy**   **idle**   **pack_received**   **pack_sent**   **connections**   **pack_errors**   **total_read**   **total_write**   **total_errors**
  --------- ------------------------ -------------- ------------- ---------- ------------------- --------------- ----------------- ----------------- ---------------- ----------------- ------------------
  **1**     02APR2010:17:34:58.817   9              7             792        28                  28              14                0                 0                0                 0
  --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

  : Page Layout

: Procedure Print: Data Set MSSQL.spt_monitor</code></pre>
<p><strong>3) Setting up SAS ODBC MSSQL connection to <em>AdventureWorksDW</em> database using LIBNAME statement:</strong></p>
<p><strong>SOURCE CODE:</strong></p>
<pre><code>LIBNAME mssqlaw odbc user='userName' password='passWord' datasrc=dbpcsql qualifier=AdventureWorksDW2008R2 schema=dbo;

proc print data=mssqlaw.DimCustomer(obs=15);run;</code></pre>
<p><strong>OUTPUT</strong></p>
<pre><code>**SAS LOG:**
7 LIBNAME mssqlaw odbc user='userName' password=XXXXXX datasrc=dbpcsql
7 ! qualifier=AdventureWorksDW2008R2 schema=dbo;
NOTE: Libref MSSQLAW was successfully assigned as follows:
Engine: ODBC
Physical Name: dbpcsql
8
9 proc print data=mssqlaw.DimCustomer(obs=15);run;

NOTE: PROCEDURE PRINT used (Total process time):
real time 0.37 seconds
cpu time 0.06 seconds
**Results Viewer:sashtml**</code></pre>
<p>The SAS System</p>
<div class="table-responsive">
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 1%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 3%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 5%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Obs</strong></th>
<th><strong>CustomerKey</strong></th>
<th><strong>GeographyKey</strong></th>
<th><strong>CustomerAlternateKey</strong></th>
<th><strong>Title</strong></th>
<th><strong>FirstName</strong></th>
<th><strong>MiddleName</strong></th>
<th><strong>LastName</strong></th>
<th><strong>NameStyle</strong></th>
<th><strong>BirthDate</strong></th>
<th><strong>MaritalStatus</strong></th>
<th><strong>Suffix</strong></th>
<th><strong>Gender</strong></th>
<th><strong>EmailAddress</strong></th>
<th><strong>YearlyIncome</strong></th>
<th><strong>TotalChildren</strong></th>
<th><strong>NumberChildrenAtHome</strong></th>
<th><strong>EnglishEducation</strong></th>
<th><strong>SpanishEducation</strong></th>
<th><strong>FrenchEducation</strong></th>
<th><strong>EnglishOccupation</strong></th>
<th><strong>SpanishOccupation</strong></th>
<th><strong>FrenchOccupation</strong></th>
<th><strong>HouseOwnerFlag</strong></th>
<th><strong>NumberCarsOwned</strong></th>
<th><strong>AddressLine1</strong></th>
<th><strong>AddressLine2</strong></th>
<th><strong>Phone</strong></th>
<th><strong>DateFirstPurchase</strong></th>
<th><strong>CommuteDistance</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>11000</td>
<td>26</td>
<td>AW00011000</td>
<td></td>
<td>Jon</td>
<td>V</td>
<td>Yang</td>
<td>0</td>
<td>1970-04-08</td>
<td>M</td>
<td></td>
<td>M</td>
<td>jon24@adventure-works.com</td>
<td>$90,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>0</td>
<td>3761 N. 14th St</td>
<td></td>
<td>1 (11) 500 555-0162</td>
<td>2005-07-22</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td>11001</td>
<td>37</td>
<td>AW00011001</td>
<td></td>
<td>Eugene</td>
<td>L</td>
<td>Huang</td>
<td>0</td>
<td>1969-05-14</td>
<td>S</td>
<td></td>
<td>M</td>
<td>eugene10@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>2243 W St.</td>
<td></td>
<td>1 (11) 500 555-0110</td>
<td>2005-07-18</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td>11002</td>
<td>31</td>
<td>AW00011002</td>
<td></td>
<td>Ruben</td>
<td></td>
<td>Torres</td>
<td>0</td>
<td>1969-08-12</td>
<td>M</td>
<td></td>
<td>M</td>
<td>ruben35@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>5844 Linden Land</td>
<td></td>
<td>1 (11) 500 555-0184</td>
<td>2005-07-10</td>
<td>2-5 Miles</td>
</tr>
<tr class="even">
<td><strong>4</strong></td>
<td>11003</td>
<td>11</td>
<td>AW00011003</td>
<td></td>
<td>Christy</td>
<td></td>
<td>Zhu</td>
<td>0</td>
<td>1972-02-15</td>
<td>S</td>
<td></td>
<td>F</td>
<td>christy12@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>1825 Village Pl.</td>
<td></td>
<td>1 (11) 500 555-0162</td>
<td>2005-07-01</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td><strong>5</strong></td>
<td>11004</td>
<td>19</td>
<td>AW00011004</td>
<td></td>
<td>Elizabeth</td>
<td></td>
<td>Johnson</td>
<td>0</td>
<td>1972-08-08</td>
<td>S</td>
<td></td>
<td>F</td>
<td>elizabeth5@adventure-works.com</td>
<td>$80,000.00</td>
<td>5</td>
<td>5</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>4</td>
<td>7553 Harness Circle</td>
<td></td>
<td>1 (11) 500 555-0131</td>
<td>2005-07-26</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td><strong>6</strong></td>
<td>11005</td>
<td>22</td>
<td>AW00011005</td>
<td></td>
<td>Julio</td>
<td></td>
<td>Ruiz</td>
<td>0</td>
<td>1969-08-05</td>
<td>S</td>
<td></td>
<td>M</td>
<td>julio1@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>7305 Humphrey Drive</td>
<td></td>
<td>1 (11) 500 555-0151</td>
<td>2005-07-02</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td><strong>7</strong></td>
<td>11006</td>
<td>8</td>
<td>AW00011006</td>
<td></td>
<td>Janet</td>
<td>G</td>
<td>Alvarez</td>
<td>0</td>
<td>1969-12-06</td>
<td>S</td>
<td></td>
<td>F</td>
<td>janet9@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>2612 Berry Dr</td>
<td></td>
<td>1 (11) 500 555-0184</td>
<td>2005-07-27</td>
<td>5-10 Miles</td>
</tr>
<tr class="even">
<td><strong>8</strong></td>
<td>11007</td>
<td>40</td>
<td>AW00011007</td>
<td></td>
<td>Marco</td>
<td></td>
<td>Mehta</td>
<td>0</td>
<td>1968-05-09</td>
<td>M</td>
<td></td>
<td>M</td>
<td>marco14@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>2</td>
<td>942 Brook Street</td>
<td></td>
<td>1 (11) 500 555-0126</td>
<td>2005-07-12</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td><strong>9</strong></td>
<td>11008</td>
<td>32</td>
<td>AW00011008</td>
<td></td>
<td>Rob</td>
<td></td>
<td>Verhoff</td>
<td>0</td>
<td>1968-07-07</td>
<td>S</td>
<td></td>
<td>F</td>
<td>rob4@adventure-works.com</td>
<td>$60,000.00</td>
<td>4</td>
<td>4</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>3</td>
<td>624 Peabody Road</td>
<td></td>
<td>1 (11) 500 555-0164</td>
<td>2005-07-28</td>
<td>10+ Miles</td>
</tr>
<tr class="even">
<td><strong>10</strong></td>
<td>11009</td>
<td>25</td>
<td>AW00011009</td>
<td></td>
<td>Shannon</td>
<td>C</td>
<td>Carlson</td>
<td>0</td>
<td>1968-04-01</td>
<td>S</td>
<td></td>
<td>M</td>
<td>shannon38@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>3839 Northgate Road</td>
<td></td>
<td>1 (11) 500 555-0110</td>
<td>2005-07-30</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td><strong>11</strong></td>
<td>11010</td>
<td>22</td>
<td>AW00011010</td>
<td></td>
<td>Jacquelyn</td>
<td>C</td>
<td>Suarez</td>
<td>0</td>
<td>1968-02-06</td>
<td>S</td>
<td></td>
<td>F</td>
<td>jacquelyn20@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>7800 Corrinne Court</td>
<td></td>
<td>1 (11) 500 555-0169</td>
<td>2005-07-17</td>
<td>5-10 Miles</td>
</tr>
<tr class="even">
<td><strong>12</strong></td>
<td>11011</td>
<td>22</td>
<td>AW00011011</td>
<td></td>
<td>Curtis</td>
<td></td>
<td>Lu</td>
<td>0</td>
<td>1967-11-04</td>
<td>M</td>
<td></td>
<td>M</td>
<td>curtis9@adventure-works.com</td>
<td>$60,000.00</td>
<td>4</td>
<td>4</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>4</td>
<td>1224 Shoenic</td>
<td></td>
<td>1 (11) 500 555-0117</td>
<td>2005-07-02</td>
<td>10+ Miles</td>
</tr>
<tr class="odd">
<td><strong>13</strong></td>
<td>11012</td>
<td>611</td>
<td>AW00011012</td>
<td></td>
<td>Lauren</td>
<td>M</td>
<td>Walker</td>
<td>0</td>
<td>1972-01-18</td>
<td>M</td>
<td></td>
<td>F</td>
<td>lauren41@adventure-works.com</td>
<td>$100,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>1</td>
<td>2</td>
<td>4785 Scott Street</td>
<td></td>
<td>717-555-0164</td>
<td>2007-09-17</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td><strong>14</strong></td>
<td>11013</td>
<td>543</td>
<td>AW00011013</td>
<td></td>
<td>Ian</td>
<td>M</td>
<td>Jenkins</td>
<td>0</td>
<td>1972-08-06</td>
<td>M</td>
<td></td>
<td>M</td>
<td>ian47@adventure-works.com</td>
<td>$100,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>1</td>
<td>3</td>
<td>7902 Hudson Ave.</td>
<td></td>
<td>817-555-0185</td>
<td>2007-10-15</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td><strong>15</strong></td>
<td>11014</td>
<td>634</td>
<td>AW00011014</td>
<td></td>
<td>Sydney</td>
<td></td>
<td>Bennett</td>
<td>0</td>
<td>1972-05-09</td>
<td>S</td>
<td></td>
<td>F</td>
<td>sydney23@adventure-works.com</td>
<td>$100,000.00</td>
<td>3</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>0</td>
<td>3</td>
<td>9011 Tank Drive</td>
<td></td>
<td>431-555-0156</td>
<td>2007-09-24</td>
<td>1-2 Miles</td>
</tr>
</tbody>
</table>
</div>
<pre><code>  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **Obs**   **CustomerKey**   **GeographyKey**   **CustomerAlternateKey**   **Title**   **FirstName**   **MiddleName**   **LastName**   **NameStyle**   **BirthDate**   **MaritalStatus**   **Suffix**   **Gender**   **EmailAddress**                  **YearlyIncome**   **TotalChildren**   **NumberChildrenAtHome**   **EnglishEducation**   **SpanishEducation**   **FrenchEducation**   **EnglishOccupation**   **SpanishOccupation**   **FrenchOccupation**   **HouseOwnerFlag**   **NumberCarsOwned**   **AddressLine1**   **AddressLine2**   **Phone**      **DateFirstPurchase**   **CommuteDistance**
  --------- ----------------- ------------------ -------------------------- ----------- --------------- ---------------- -------------- --------------- --------------- ------------------- ------------ ------------ --------------------------------- ------------------ ------------------- -------------------------- ---------------------- ---------------------- --------------------- ----------------------- ----------------------- ---------------------- -------------------- --------------------- ------------------ ------------------ -------------- ----------------------- ---------------------
  **1**     11000             26                 AW00011000                 &nbsp;           Jon             V                Yang           0               1970-04-08      M                   &nbsp;            M            jon24@adventure-works.com         \$90,000.00        2                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    0                     3761 N. 14th St    &nbsp;                  1 (11) 500     2005-07-22              1-2 Miles

  **2**     11001             37                 AW00011001                 &nbsp;           Eugene          L                Huang          0               1969-05-14      S                   &nbsp;            M            eugene10@adventure-works.com      \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     2243 W St.         &nbsp;                  1 (11) 500     2005-07-18              0-1 Miles

  **3**     11002             31                 AW00011002                 &nbsp;           Ruben           &nbsp;                Torres         0               1969-08-12      M                   &nbsp;            M            ruben35@adventure-works.com       \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     5844 Linden Land   &nbsp;                  1 (11) 500     2005-07-10              2-5 Miles

  **4**     11003             11                 AW00011003                 &nbsp;           Christy         &nbsp;                Zhu            0               1972-02-15      S                   &nbsp;            F            christy12@adventure-works.com     \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     1825 Village Pl.   &nbsp;                  1 (11) 500     2005-07-01              5-10 Miles

  **5**     11004             19                 AW00011004                 &nbsp;           Elizabeth       &nbsp;                Johnson        0               1972-08-08      S                   &nbsp;            F            elizabeth5@adventure-works.com    \$80,000.00        5                   5                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    4                     7553 Harness       &nbsp;                  1 (11) 500     2005-07-26              1-2 Miles

  **6**     11005             22                 AW00011005                 &nbsp;           Julio           &nbsp;                Ruiz           0               1969-08-05      S                   &nbsp;            M            julio1@adventure-works.com        \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     7305 Humphrey      &nbsp;                  1 (11) 500     2005-07-02              5-10 Miles

  **7**     11006             8                  AW00011006                 &nbsp;           Janet           G                Alvarez        0               1969-12-06      S                   &nbsp;            F            janet9@adventure-works.com        \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     2612 Berry Dr      &nbsp;                  1 (11) 500     2005-07-27              5-10 Miles

  **8**     11007             40                 AW00011007                 &nbsp;           Marco           &nbsp;                Mehta          0               1968-05-09      M                   &nbsp;            M            marco14@adventure-works.com       \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    2                     942 Brook Street   &nbsp;                  1 (11) 500     2005-07-12              0-1 Miles

  **9**     11008             32                 AW00011008                 &nbsp;           Rob             &nbsp;                Verhoff        0               1968-07-07      S                   &nbsp;            F            rob4@adventure-works.com          \$60,000.00        4                   4                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    3                     624 Peabody Road   &nbsp;                  1 (11) 500     2005-07-28              10+ Miles

  **10**    11009             25                 AW00011009                 &nbsp;           Shannon         C                Carlson        0               1968-04-01      S                   &nbsp;            M            shannon38@adventure-works.com     \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     3839 Northgate     &nbsp;                  1 (11) 500     2005-07-30              5-10 Miles

  **11**    11010             22                 AW00011010                 &nbsp;           Jacquelyn       C                Suarez         0               1968-02-06      S                   &nbsp;            F            jacquelyn20@adventure-works.com   \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     7800 Corrinne      &nbsp;                  1 (11) 500     2005-07-17              5-10 Miles

  **12**    11011             22                 AW00011011                 &nbsp;           Curtis          &nbsp;                Lu             0               1967-11-04      M                   &nbsp;            M            curtis9@adventure-works.com       \$60,000.00        4                   4                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    4                     1224 Shoenic       &nbsp;                  1 (11) 500     2005-07-02              10+ Miles

  **13**    11012             611                AW00011012                 &nbsp;           Lauren          M                Walker         0               1972-01-18      M                   &nbsp;            F            lauren41@adventure-works.com      \$100,000.00       2                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              1                    2                     4785 Scott Street  &nbsp;                  717-555-0164   2007-09-17              1-2 Miles

  **14**    11013             543                AW00011013                 &nbsp;           Ian             M                Jenkins        0               1972-08-06      M                   &nbsp;            M            ian47@adventure-works.com         \$100,000.00       2                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              1                    3                     7902 Hudson Ave.   &nbsp;                  817-555-0185   2007-10-15              0-1 Miles

  **15**    11014             634                AW00011014                 &nbsp;           Sydney          &nbsp;                Bennett        0               1972-05-09      S                   &nbsp;            F            sydney23@adventure-works.com      \$100,000.00       3                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              0                    3                     9011 Tank Drive    &nbsp;                  431-555-0156   2007-09-24              1-2 Miles
  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

  : Page Layout

: Procedure Print: Data Set MSSQLAW.DimCustomer</code></pre>
<p><strong>4) Setting up SAS ODBC MSSQL connection to <em>master</em> database using SQL pass-through method:</strong></p>
<p><strong>SOURCE CODE:</strong></p>
<pre><code>proc sql outobs=15;
connectto odbc as msql2 (user='userName' password='passWord' datasrc=dbpcsql );
select * from connection to msql2 (select * from master.dbo.spt_monitor);
disconnect from msql2;

quit;</code></pre>
<p><strong>OUTPUT:</strong></p>
<pre><code>**SAS LOG:**
17 proc sql outobs=15;
18 connect to odbc as msql2 (user='userName' password=XXXXXX datasrc=dbpcsql );
19 select * from connection to msql2 (select * from master.dbo.spt_monitor);
20 disconnect from msql2;
21 quit;
NOTE: PROCEDURE SQL used (Total process time):
real time 0.04 seconds
cpu time 0.01 seconds

**Results Viewer:sashtml**</code></pre>
<p>The SAS System</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 10%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 8%">
<col style="width: 9%">
<col style="width: 10%">
</colgroup>
<thead>
<tr class="header">
<th><strong>lastrun</strong></th>
<th><strong>cpu_busy</strong></th>
<th><strong>io_busy</strong></th>
<th><strong>idle</strong></th>
<th><strong>pack_received</strong></th>
<th><strong>pack_sent</strong></th>
<th><strong>connections</strong></th>
<th><strong>pack_errors</strong></th>
<th><strong>total_read</strong></th>
<th><strong>total_write</strong></th>
<th><strong>total_errors</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>02APR2010:17:34:58.817</td>
<td>9</td>
<td>7</td>
<td>792</td>
<td>28</td>
<td>28</td>
<td>14</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<pre><code>  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **lastrun**              **cpu_busy**   **io_busy**   **idle**   **pack_received**   **pack_sent**   **connections**   **pack_errors**   **total_read**   **total_write**   **total_errors**
  ------------------------ -------------- ------------- ---------- ------------------- --------------- ----------------- ----------------- ---------------- ----------------- ------------------
  02APR2010:17:34:58.817   9              7             792        28                  28              14                0                 0                0                 0
  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

  : Page Layout

: Procedure SQL: Query Results</code></pre>
<p><strong>5) Setting up SAS ODBC MSSQL connection to <em>AdventureWorksDW</em> database using SQL pass-through method:</strong> <strong>SOURCE CODE:</strong></p>
<pre><code>proc sql outobs=15;
connectto odbc as msql2 (user='userName' password='passWord' datasrc=dbpcsql );
select * from connection to msql2 (select * from AdventureWorksDW2008R2.dbo.DimCustomer);

disconnect from msql2;

quit;</code></pre>
<p><strong>OUTPUT:</strong></p>
<pre><code>**SAS LOG:**
22 proc sql outobs=15;
23 connect to odbc as msql3 (user='userName' password=XXXXXX datasrc=dbpcsql );
24 select * from connection to msql3 (select * from 
24 ! AdventureWorksDW2008R2.dbo.DimCustomer);
WARNING: Statement terminated early due to OUTOBS=15 option.
25 disconnect from msql3;
26 quit;

 NOTE: PROCEDURE SQL used (Total process time):
real time 0.35 seconds
cpu time 0.03 seconds
**Results Viewer:sashtml**</code></pre>
<p>The SAS System</p>
<div class="table-responsive">
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 1%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 3%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 6%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 4%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 3%">
<col style="width: 4%">
<col style="width: 3%">
</colgroup>
<thead>
<tr class="header">
<th><strong>CustomerKey</strong></th>
<th><strong>GeographyKey</strong></th>
<th><strong>CustomerAlternateKey</strong></th>
<th><strong>Title</strong></th>
<th><strong>FirstName</strong></th>
<th><strong>MiddleName</strong></th>
<th><strong>LastName</strong></th>
<th><strong>NameStyle</strong></th>
<th><strong>BirthDate</strong></th>
<th><strong>MaritalStatus</strong></th>
<th><strong>Suffix</strong></th>
<th><strong>Gender</strong></th>
<th><strong>EmailAddress</strong></th>
<th><strong>YearlyIncome</strong></th>
<th><strong>TotalChildren</strong></th>
<th><strong>NumberChildrenAtHome</strong></th>
<th><strong>EnglishEducation</strong></th>
<th><strong>SpanishEducation</strong></th>
<th><strong>FrenchEducation</strong></th>
<th><strong>EnglishOccupation</strong></th>
<th><strong>SpanishOccupation</strong></th>
<th><strong>FrenchOccupation</strong></th>
<th><strong>HouseOwnerFlag</strong></th>
<th><strong>NumberCarsOwned</strong></th>
<th><strong>AddressLine1</strong></th>
<th><strong>AddressLine2</strong></th>
<th><strong>Phone</strong></th>
<th><strong>DateFirstPurchase</strong></th>
<th><strong>CommuteDistance</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>11000</td>
<td>26</td>
<td>AW00011000</td>
<td></td>
<td>Jon</td>
<td>V</td>
<td>Yang</td>
<td>0</td>
<td>1970-04-08</td>
<td>M</td>
<td></td>
<td>M</td>
<td>jon24@adventure-works.com</td>
<td>$90,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>0</td>
<td>3761 N. 14th St</td>
<td></td>
<td>1 (11) 500 555-0162</td>
<td>2005-07-22</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td>11001</td>
<td>37</td>
<td>AW00011001</td>
<td></td>
<td>Eugene</td>
<td>L</td>
<td>Huang</td>
<td>0</td>
<td>1969-05-14</td>
<td>S</td>
<td></td>
<td>M</td>
<td>eugene10@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>2243 W St.</td>
<td></td>
<td>1 (11) 500 555-0110</td>
<td>2005-07-18</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td>11002</td>
<td>31</td>
<td>AW00011002</td>
<td></td>
<td>Ruben</td>
<td></td>
<td>Torres</td>
<td>0</td>
<td>1969-08-12</td>
<td>M</td>
<td></td>
<td>M</td>
<td>ruben35@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>5844 Linden Land</td>
<td></td>
<td>1 (11) 500 555-0184</td>
<td>2005-07-10</td>
<td>2-5 Miles</td>
</tr>
<tr class="even">
<td>11003</td>
<td>11</td>
<td>AW00011003</td>
<td></td>
<td>Christy</td>
<td></td>
<td>Zhu</td>
<td>0</td>
<td>1972-02-15</td>
<td>S</td>
<td></td>
<td>F</td>
<td>christy12@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>1825 Village Pl.</td>
<td></td>
<td>1 (11) 500 555-0162</td>
<td>2005-07-01</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td>11004</td>
<td>19</td>
<td>AW00011004</td>
<td></td>
<td>Elizabeth</td>
<td></td>
<td>Johnson</td>
<td>0</td>
<td>1972-08-08</td>
<td>S</td>
<td></td>
<td>F</td>
<td>elizabeth5@adventure-works.com</td>
<td>$80,000.00</td>
<td>5</td>
<td>5</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>4</td>
<td>7553 Harness Circle</td>
<td></td>
<td>1 (11) 500 555-0131</td>
<td>2005-07-26</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td>11005</td>
<td>22</td>
<td>AW00011005</td>
<td></td>
<td>Julio</td>
<td></td>
<td>Ruiz</td>
<td>0</td>
<td>1969-08-05</td>
<td>S</td>
<td></td>
<td>M</td>
<td>julio1@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>7305 Humphrey Drive</td>
<td></td>
<td>1 (11) 500 555-0151</td>
<td>2005-07-02</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td>11006</td>
<td>8</td>
<td>AW00011006</td>
<td></td>
<td>Janet</td>
<td>G</td>
<td>Alvarez</td>
<td>0</td>
<td>1969-12-06</td>
<td>S</td>
<td></td>
<td>F</td>
<td>janet9@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>1</td>
<td>2612 Berry Dr</td>
<td></td>
<td>1 (11) 500 555-0184</td>
<td>2005-07-27</td>
<td>5-10 Miles</td>
</tr>
<tr class="even">
<td>11007</td>
<td>40</td>
<td>AW00011007</td>
<td></td>
<td>Marco</td>
<td></td>
<td>Mehta</td>
<td>0</td>
<td>1968-05-09</td>
<td>M</td>
<td></td>
<td>M</td>
<td>marco14@adventure-works.com</td>
<td>$60,000.00</td>
<td>3</td>
<td>3</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>2</td>
<td>942 Brook Street</td>
<td></td>
<td>1 (11) 500 555-0126</td>
<td>2005-07-12</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td>11008</td>
<td>32</td>
<td>AW00011008</td>
<td></td>
<td>Rob</td>
<td></td>
<td>Verhoff</td>
<td>0</td>
<td>1968-07-07</td>
<td>S</td>
<td></td>
<td>F</td>
<td>rob4@adventure-works.com</td>
<td>$60,000.00</td>
<td>4</td>
<td>4</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>3</td>
<td>624 Peabody Road</td>
<td></td>
<td>1 (11) 500 555-0164</td>
<td>2005-07-28</td>
<td>10+ Miles</td>
</tr>
<tr class="even">
<td>11009</td>
<td>25</td>
<td>AW00011009</td>
<td></td>
<td>Shannon</td>
<td>C</td>
<td>Carlson</td>
<td>0</td>
<td>1968-04-01</td>
<td>S</td>
<td></td>
<td>M</td>
<td>shannon38@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>3839 Northgate Road</td>
<td></td>
<td>1 (11) 500 555-0110</td>
<td>2005-07-30</td>
<td>5-10 Miles</td>
</tr>
<tr class="odd">
<td>11010</td>
<td>22</td>
<td>AW00011010</td>
<td></td>
<td>Jacquelyn</td>
<td>C</td>
<td>Suarez</td>
<td>0</td>
<td>1968-02-06</td>
<td>S</td>
<td></td>
<td>F</td>
<td>jacquelyn20@adventure-works.com</td>
<td>$70,000.00</td>
<td>0</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>0</td>
<td>1</td>
<td>7800 Corrinne Court</td>
<td></td>
<td>1 (11) 500 555-0169</td>
<td>2005-07-17</td>
<td>5-10 Miles</td>
</tr>
<tr class="even">
<td>11011</td>
<td>22</td>
<td>AW00011011</td>
<td></td>
<td>Curtis</td>
<td></td>
<td>Lu</td>
<td>0</td>
<td>1967-11-04</td>
<td>M</td>
<td></td>
<td>M</td>
<td>curtis9@adventure-works.com</td>
<td>$60,000.00</td>
<td>4</td>
<td>4</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Professional</td>
<td>Profesional</td>
<td>Cadre</td>
<td>1</td>
<td>4</td>
<td>1224 Shoenic</td>
<td></td>
<td>1 (11) 500 555-0117</td>
<td>2005-07-02</td>
<td>10+ Miles</td>
</tr>
<tr class="odd">
<td>11012</td>
<td>611</td>
<td>AW00011012</td>
<td></td>
<td>Lauren</td>
<td>M</td>
<td>Walker</td>
<td>0</td>
<td>1972-01-18</td>
<td>M</td>
<td></td>
<td>F</td>
<td>lauren41@adventure-works.com</td>
<td>$100,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>1</td>
<td>2</td>
<td>4785 Scott Street</td>
<td></td>
<td>717-555-0164</td>
<td>2007-09-17</td>
<td>1-2 Miles</td>
</tr>
<tr class="even">
<td>11013</td>
<td>543</td>
<td>AW00011013</td>
<td></td>
<td>Ian</td>
<td>M</td>
<td>Jenkins</td>
<td>0</td>
<td>1972-08-06</td>
<td>M</td>
<td></td>
<td>M</td>
<td>ian47@adventure-works.com</td>
<td>$100,000.00</td>
<td>2</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>1</td>
<td>3</td>
<td>7902 Hudson Ave.</td>
<td></td>
<td>817-555-0185</td>
<td>2007-10-15</td>
<td>0-1 Miles</td>
</tr>
<tr class="odd">
<td>11014</td>
<td>634</td>
<td>AW00011014</td>
<td></td>
<td>Sydney</td>
<td></td>
<td>Bennett</td>
<td>0</td>
<td>1972-05-09</td>
<td>S</td>
<td></td>
<td>F</td>
<td>sydney23@adventure-works.com</td>
<td>$100,000.00</td>
<td>3</td>
<td>0</td>
<td>Bachelors</td>
<td>Licenciatura</td>
<td>Bac + 4</td>
<td>Management</td>
<td>GestiÃ³n</td>
<td>Direction</td>
<td>0</td>
<td>3</td>
<td>9011 Tank Drive</td>
<td></td>
<td>431-555-0156</td>
<td>2007-09-24</td>
<td>1-2 Miles</td>
</tr>
</tbody>
</table>
</div>
<pre><code>  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **CustomerKey**   **GeographyKey**   **CustomerAlternateKey**   **Title**   **FirstName**   **MiddleName**   **LastName**   **NameStyle**   **BirthDate**   **MaritalStatus**   **Suffix**   **Gender**   **EmailAddress**                  **YearlyIncome**   **TotalChildren**   **NumberChildrenAtHome**   **EnglishEducation**   **SpanishEducation**   **FrenchEducation**   **EnglishOccupation**   **SpanishOccupation**   **FrenchOccupation**   **HouseOwnerFlag**   **NumberCarsOwned**   **AddressLine1**   **AddressLine2**   **Phone**      **DateFirstPurchase**   **CommuteDistance**
  ----------------- ------------------ -------------------------- ----------- --------------- ---------------- -------------- --------------- --------------- ------------------- ------------ ------------ --------------------------------- ------------------ ------------------- -------------------------- ---------------------- ---------------------- --------------------- ----------------------- ----------------------- ---------------------- -------------------- --------------------- ------------------ ------------------ -------------- ----------------------- ---------------------
  11000             26                 AW00011000                 &nbsp;           Jon             V                Yang           0               1970-04-08      M                   &nbsp;            M            jon24@adventure-works.com         \$90,000.00        2                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    0                     3761 N. 14th St    &nbsp;                  1 (11) 500     2005-07-22              1-2 Miles
  11001             37                 AW00011001                 &nbsp;           Eugene          L                Huang          0               1969-05-14      S                   &nbsp;            M            eugene10@adventure-works.com      \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     2243 W St.         &nbsp;                  1 (11) 500     2005-07-18              0-1 Miles
  11002             31                 AW00011002                 &nbsp;           Ruben           &nbsp;                Torres         0               1969-08-12      M                   &nbsp;            M            ruben35@adventure-works.com       \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     5844 Linden Land   &nbsp;                  1 (11) 500     2005-07-10              2-5 Miles
  11003             11                 AW00011003                 &nbsp;           Christy         &nbsp;                Zhu            0               1972-02-15      S                   &nbsp;            F            christy12@adventure-works.com     \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     1825 Village Pl.   &nbsp;                  1 (11) 500     2005-07-01              5-10 Miles
  11004             19                 AW00011004                 &nbsp;           Elizabeth       &nbsp;                Johnson        0               1972-08-08      S                   &nbsp;            F            elizabeth5@adventure-works.com    \$80,000.00        5                   5                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    4                     7553 Harness       &nbsp;                  1 (11) 500     2005-07-26              1-2 Miles
  11005             22                 AW00011005                 &nbsp;           Julio           &nbsp;                Ruiz           0               1969-08-05      S                   &nbsp;            M            julio1@adventure-works.com        \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     7305 Humphrey      &nbsp;                  1 (11) 500     2005-07-02              5-10 Miles
  11006             8                  AW00011006                 &nbsp;           Janet           G                Alvarez        0               1969-12-06      S                   &nbsp;            F            janet9@adventure-works.com        \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    1                     2612 Berry Dr      &nbsp;                  1 (11) 500     2005-07-27              5-10 Miles
  11007             40                 AW00011007                 &nbsp;           Marco           &nbsp;                Mehta          0               1968-05-09      M                   &nbsp;            M            marco14@adventure-works.com       \$60,000.00        3                   3                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    2                     942 Brook Street   &nbsp;                  1 (11) 500     2005-07-12              0-1 Miles
  11008             32                 AW00011008                 &nbsp;           Rob             &nbsp;                Verhoff        0               1968-07-07      S                   &nbsp;            F            rob4@adventure-works.com          \$60,000.00        4                   4                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    3                     624 Peabody Road   &nbsp;                  1 (11) 500     2005-07-28              10+ Miles
  11009             25                 AW00011009                 &nbsp;           Shannon         C                Carlson        0               1968-04-01      S                   &nbsp;            M            shannon38@adventure-works.com     \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     3839 Northgate     &nbsp;                  1 (11) 500     2005-07-30              5-10 Miles
  11010             22                 AW00011010                 &nbsp;           Jacquelyn       C                Suarez         0               1968-02-06      S                   &nbsp;            F            jacquelyn20@adventure-works.com   \$70,000.00        0                   0                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  0                    1                     7800 Corrinne      &nbsp;                  1 (11) 500     2005-07-17              5-10 Miles
  11011             22                 AW00011011                 &nbsp;           Curtis          &nbsp;                Lu             0               1967-11-04      M                   &nbsp;            M            curtis9@adventure-works.com       \$60,000.00        4                   4                          Bachelors              Licenciatura           Bac + 4               Professional            Profesional             Cadre                  1                    4                     1224 Shoenic       &nbsp;                  1 (11) 500     2005-07-02              10+ Miles
  11012             611                AW00011012                 &nbsp;           Lauren          M                Walker         0               1972-01-18      M                   &nbsp;            F            lauren41@adventure-works.com      \$100,000.00       2                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              1                    2                     4785 Scott Street  &nbsp;                  717-555-0164   2007-09-17              1-2 Miles
  11013             543                AW00011013                 &nbsp;           Ian             M                Jenkins        0               1972-08-06      M                   &nbsp;            M            ian47@adventure-works.com         \$100,000.00       2                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              1                    3                     7902 Hudson Ave.   &nbsp;                  817-555-0185   2007-10-15              0-1 Miles
  11014             634                AW00011014                 &nbsp;           Sydney          &nbsp;                Bennett        0               1972-05-09      S                   &nbsp;            F            sydney23@adventure-works.com      \$100,000.00       3                   0                          Bachelors              Licenciatura           Bac + 4               Management              GestiÃ³n                Direction              0                    3                     9011 Tank Drive    &nbsp;                  431-555-0156   2007-09-24              1-2 Miles
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

  : Page Layout

: Procedure SQL: Query Results</code></pre>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary:</h2>
<p>The SAS/ACCESS interface component is used to connect to various kinds of databases. Three main ways of connecting SAS with MSSQL are using SAS/ACCESS interface for MS SQL Server, SAS/ACCESS interface for OLE DB &amp; SAS/ACCESS interface for ODBC. SAS allows data retrieval/storage using both LIBNAME statement and direct SQL Pass-Through statement method.</p>
<p>What is shown above is very elementary code just to get the process started. Students learning data science/analytics should try to use different combinations of SAS methods &amp; SAS interfaces to retrieve data from a database, manipulate it using SAS/SQL &amp; write the processed data sets back to the database.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References:</h2>
<ol type="1">
<li>SAS/ACCESS 9.2 for Relational Databases Reference, Fourth Edition. <a href="http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#titlepage.htm" class="uri">http://support.sas.com/documentation/cdl/en/acreldb/63647/HTML/default/viewer.htm#titlepage.htm</a><br>
</li>
<li>SAS 9.2 SQL Procedure User’s Guide. <a href="http://support.sas.com/documentation/cdl/en/sqlproc/62086/HTML/default/viewer.htm#titlepage.htm" class="uri">http://support.sas.com/documentation/cdl/en/sqlproc/62086/HTML/default/viewer.htm#titlepage.htm</a></li>
</ol>


</section>

 ]]></description>
  <category>article</category>
  <category>datascience</category>
  <category>sas</category>
  <category>odbc</category>
  <guid>https://fortunewalla.github.io/fw2/posts/09-connectquerymssqlsasinterfaceodbc/</guid>
  <pubDate>Sun, 18 May 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Project to implement remote PHP web client/ MSSQL database server network setup</title>
  <link>https://fortunewalla.github.io/fw2/posts/08-projecttoimplementremotedb/</link>
  <description><![CDATA[ 




<p><em>Posted on Friday, May 16th, 2014 at 2:43 pm</em></p>
<p>For students learning data science/analytics, accessing data sets stored from databases in remote servers is a necessary skill. People in corporations/institutions have Systems Administrators to set up most of the interfaces for such purposes. However knowing how such a process is setup is a good knowledge. Here PHP is used to demonstrate database access.</p>
<p>Although this project is not related to data science/analytics, it will give a good hands-on experience of client/server networking. These are mere guidelines and not details as specific options &amp; settings will depend on the version of the software &amp; components.</p>
<p>Pre-requisites: Fairly good understanding of computer hardware especially networking, Windows operating system, drivers &amp; software installation. Hardware/Networking</p>
<p>Physical: Atleast two computers with Ethernet/WiFi capability and a wireless/wired router.</p>
<p>Final Setup: Ability to create Workgroups &amp; shared folders. Also ability to ping &amp; telnet the remote computers/servers. Server Setup</p>
<p>There are many versions &amp; editions MSSQL depending on user requirements (The Developer Edition is best for learning &amp; academic use). For data mining features, SQL Server Analysis Services(SSAS) must be installed. To install OLTP/DW AdventureWorks sample databases, enable <code>FILESTREAM</code> and make sure the SQL Full-text Filter Daemon Launcher service is running.</p>
<ol type="1">
<li>If you have the space, do a full install of every feature. It will make it easier to troubleshoot problems.</li>
<li>Create a MSSQL user exclusively for remote access. <a href="http://technet.microsoft.com/en-us/library/aa337545.aspx" class="uri">http://technet.microsoft.com/en-us/library/aa337545.aspx</a></li>
<li>Allow remote connection to MSSQL. <a href="http://technet.microsoft.com/en-us/library/ms191464.aspx" class="uri">http://technet.microsoft.com/en-us/library/ms191464.aspx</a></li>
<li>Enable Shared Memory, TCP/IP, pipes <a href="http://technet.microsoft.com/en-us/library/ms181035.aspx" class="uri">http://technet.microsoft.com/en-us/library/ms181035.aspx</a></li>
<li>In the Windows Firewall, allow incoming/outgoing connections for port 1433 (default MSSQL port.) <a href="http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx" class="uri">http://blogs.msdn.com/b/walzenbach/archive/2010/04/14/how-to-enable-remote-connections-in-sql-server-2008.aspx</a></li>
</ol>
<p>Final Setup: From the Command Prompt in the client, execute ping dbpcip and telnet dbpcip 1433 where dbpcip is the IP address of MSSQL machine. If you can connect with both the commands, the client/server networking is probably working. Although accessing the datebase &amp; tables finally depends on the permissions assigned to the remote user. Client Setup</p>
<p>Keep these details in mind before starting installation. 1) Some of the types of PHP functions for access to MSSQL. This article only discusses <code>mssql_()</code> and <code>sqldrv_()</code>.</p>
<pre><code>`mssql_()` Supported by PHP 5.2 (php52) and lower. MSSQL Driver for PHP (SSDPHP) not required
`sqldrv_()` Supported by PHP 5.3 (php53) and higher. SSDPHP required.
`pdo_sqlsrv_()` Supported by php52 and higher. SSDPHP required.
`pdo_odbc_()` Supported by php51 and higher. SSDPHP not required.</code></pre>
<ol start="2" type="1">
<li><p>SQL Server Native Client (SSNC): This is needed by PHP to access MSSQL Server. SSNC2008: installs on winxp SSNC2012: does not install on winxp. php52: requires atleast SSNC2008 php53: requires atleast SSNC2012</p></li>
<li><p>Problem: php53 onwards requires SSNC2012, but SSNC2012 does not work on winxp. Also php53 does not support <code>mssql_()</code> functions.</p></li>
</ol>
<p>Solution: SSNC2008 works on winxp. Use php 5.2.13 to 5.2.17 as php52 requires SSNC2008. Also php 5.2.x supports <code>mssql_()</code>functions and sqldrv_() functions if you install the SSDPHP 2.0. Client software</p>
<ol type="1">
<li>Internet Information Services (IIS): Official Documentation: <a href="https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/iiiisin2.mspx?mfr=true" class="uri">https://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/iiiisin2.mspx?mfr=true</a></li>
</ol>
<p>Step-by-step illustrated guide: Follow until step 10 <a href="http://www.wikihow.com/Configure-IIS-for-Windows-XP-Pro" class="uri">http://www.wikihow.com/Configure-IIS-for-Windows-XP-Pro</a></p>
<p>You can test if it is working by opening the <a href="http://localhost/" class="uri">http://localhost/</a> address on the web browser. You should also test it on a remote computer using <a href="http://clientip/" class="uri">http://clientip/</a></p>
<p>Final Setup: Windows Version: XP IIS Version: 5.1</p>
<ol start="2" type="1">
<li>FastCGI:</li>
</ol>
<p>FastCGI helps IIS work better with PHP. Install the one which is compatible with your version of IIS &amp; OS. Official source: <a href="http://www.iis.net/downloads/microsoft/fastcgi-for-iis" class="uri">http://www.iis.net/downloads/microsoft/fastcgi-for-iis</a></p>
<p>Final Setup:fcgisetup_1.5_rtw_x86.msi</p>
<ol start="3" type="1">
<li>PHP installation:</li>
</ol>
<ol type="a">
<li>Steps to installing PHP with IIS are explained in &lt;www.php.net/manual/en/install.windows.iis6.php&gt;</li>
<li>5.2 and lower <code>mssql_()</code>documentation <a href="http://www.php.net/manual/en/book.mssql.php" class="uri">http://www.php.net/manual/en/book.mssql.php</a></li>
<li>5.3 and higher sqlsrv_() documentation <a href="http://www.php.net/manual/en/book.sqlsrv.php" class="uri">http://www.php.net/manual/en/book.sqlsrv.php</a></li>
<li>5.2 and higher pdo_sqlsrv() documentation <a href="http://www.php.net/manual/en/ref.pdo-sqlsrv.php" class="uri">http://www.php.net/manual/en/ref.pdo-sqlsrv.php</a></li>
<li>5.1 and higher pdo_odbc() documentation <a href="http://www.php.net/manual/en/ref.pdo-odbc.php" class="uri">http://www.php.net/manual/en/ref.pdo-odbc.php</a></li>
</ol>
<p>Final Setup: php-5.2.17-nts-Win32-VC6-x86.msi</p>
<ol start="4" type="1">
<li>MSSQL Driver for PHP:</li>
</ol>
<p>Official Documentation: <a href="http://technet.microsoft.com/en-us/library/dn425064%28v=sql.10%29.aspx" class="uri">http://technet.microsoft.com/en-us/library/dn425064%28v=sql.10%29.aspx</a></p>
<p>Installation help: <a href="http://www.iis.net/learn/application-frameworks/install-and-configure-php-on-iis/install-the-sql-server-driver-for-php" class="uri">http://www.iis.net/learn/application-frameworks/install-and-configure-php-on-iis/install-the-sql-server-driver-for-php</a></p>
<p>Final Setup: SQLSRV20.exe</p>
<ol start="5" type="1">
<li>SQL Server Native Client:</li>
</ol>
<p>Use this page to decide what components are needed for your setup. <a href="http://msdn.microsoft.com/en-us/library/cc296170%28SQL.105%29.aspx" class="uri">http://msdn.microsoft.com/en-us/library/cc296170%28SQL.105%29.aspx</a></p>
<p>Final Setup: sqlncli2k8r2x86.msi</p>
<ol start="6" type="1">
<li>Overview: The entire process can be very broadly summarised as:</li>
</ol>
<p>IIS -&gt; FastCGI -&gt; PHP 5.3 -&gt; sqlsrv_() -&gt; SSDPHP 3.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; sqlsrv_() -&gt; SSDPHP 2.0 -&gt; SSNC -&gt; MSSQL IIS -&gt; FastCGI -&gt; PHP 5.2 -&gt; <code>mssql_()</code>-&gt; No SSDPHP -&gt; SSNC -&gt; MSSQL Testing the final setup:</p>
<ol type="1">
<li>PHP System Information Test: If PHP is properly installed, the following code should execute:</li>
</ol>
<p>SOURCE CODE:</p>
<pre><code>&lt;?php echo phpinfo(); ?&gt;</code></pre>
<p>You should get PHP system information which includes information about msql_() and sqlsrv_() extensions. Only relevant partial output shown below.</p>
<p>OUTPUT:</p>
<pre><code>
    
        Registered PHP Streams 
            php, file, data, http, ftp, compress.zlib, compress.bzip2, 
        https, ftps, zip, sqlsrv   
    




cgi-fcgi


    
        Directive
            Local Value
            Master Value
    

    
        cgi.check_shebang_line
            1
            1
    

    
        cgi.fix_pathinfo
            1
            1
    

    
        cgi.force_redirect
            0
            0
    

    
        cgi.nph
            0
            0
    

    
        cgi.redirect_status_env
            no value
            no value
    

    
        cgi.rfc2616_headers
            0
            0
    

    
        fastcgi.impersonate
            1
            1
    

    
        fastcgi.logging
            0
            0
    



msql


    
        MSQL Support 
            enabled 
    

    
        Allow Persistent Links 
            yes 
    

    
        Persistent Links 
            0/unlimited 
    

    
        Total Links 
            0/unlimited 
    



mssql


    
        MSSQL Support
            enabled
    

    
        Active Persistent Links 
            0 
    

    
        Active Links 
            0 
    

    
        Library version 
            7.0 
    




    
        Directive
            Local Value
            Master Value
    

    
        mssql.allow_persistent
            On
            On
    

    
        mssql.batchsize
            0
            0
    

    
        mssql.compatability_mode
            Off
            Off
    

    
        mssql.connect_timeout
            5
            5
    

    
        mssql.datetimeconvert
            On
            On
    

    
        mssql.max_links
            Unlimited
            Unlimited
    

    
        mssql.max_persistent
            Unlimited
            Unlimited
    

    
        mssql.max_procs
            Unlimited
            Unlimited
    

    
        mssql.min_error_severity
            10
            10
    

    
        mssql.min_message_severity
            10
            10
    

    
        mssql.secure_connection
            Off
            Off
    

    
        mssql.textlimit
            Server default
            Server default
    

    
        mssql.textsize
            Server default
            Server default
    

    
        mssql.timeout
            60
            60
    



pdo_sqlsrv


    
        pdo_sqlsrv support
            enabled
    




    
        Directive
            Local Value
            Master Value
    

    
        pdo_sqlsrv.log_severity
            0
            0
    



sqlsrv


    
        sqlsrv support
            enabled
    




    
        Directive
            Local Value
            Master Value
    

    
        sqlsrv.LogSeverity
            0
            0
    

    
        sqlsrv.LogSubsystems
            0
            0
    

    
        sqlsrv.WarningsReturnAsErrors
            On
            On
    
</code></pre>
<ol start="2" type="1">
<li>Remote MSSQL Access Test: If phpinfo() indicates that all the drivers &amp; extensions are installed properly, then test to see if the database server &amp; client are properly registered.</li>
</ol>
<p>SOURCE CODE:</p>
<pre><code>
&lt;?php
$serverName = "dbpcip, 1433"; //serverName\instanceName, portNumber (default is 1433)
$connectionInfo = array( "Database"=&gt;"master", "UID"=&gt;"userName", "PWD"=&gt;"passWord");
$conn = sqlsrv_connect( $serverName, $connectionInfo);

if( $conn ) {
     echo "Connection established.
";
}else{
     echo "Connection could not be established.
";
     die( print_r( sqlsrv_errors(), true));
}

if( $client_info = sqlsrv_client_info( $conn)) {
    foreach( $client_info as $key =&gt; $value) {
        echo $key.": ".$value."
";
    }
} else {
    echo "Error in retrieving client info.
";
}


$server_info = sqlsrv_server_info( $conn);
if( $server_info )
{
    foreach( $server_info as $key =&gt; $value) {
       echo $key.": ".$value."
";
    }
} else {
      die( print_r( sqlsrv_errors(), true));
}
?&gt;</code></pre>
<p>OUTPUT:</p>
<pre><code>Connection established.


    
        Parameter
            Value
    

    
        DriverDllName:
            sqlncli10.dll
    

    
        DriverODBCVer:
            03.52
    

    
        DriverVer:
            10.50.1600
    

        
        ExtensionVer:
            2.0.1802.200
    

        
        SQLServerVersion:
            10.50.1600
    

        
        SQLServerName:
            DBPC</code></pre>
<ol start="3" type="1">
<li>Test of SQL Query: Once a connection is established, the final test is to see whether query execution is possible or not. This is a quick crude code. But the fact that it retrieves data from master.dbo.spt_monitor table shows the setup &amp; the connection works.</li>
</ol>
<p>SOURCE CODE:</p>
<pre><code>&lt;?php
$serverName = "dbpcip, 1433"; //serverName\instanceName, portNumber (default is 1433)
$connectionInfo = array( "Database"=&gt;"master", "UID"=&gt;"userName", "PWD"=&gt;"passWord");
$conn = sqlsrv_connect( $serverName, $connectionInfo);

error_reporting(-1);


if( $conn ) {
     echo "Connection established.
";
}else{
     echo "Connection could not be established.
";
     die( print_r( sqlsrv_errors(), true));
}
/* SQL Query */
$sql="select * from dbo.spt_monitor";
$results = sqlsrv_query( $conn, $sql );
if( $results === false) {
    die( print_r( sqlsrv_errors(), true) );
}
    echo "MSSQL master.dbo.spt_monitor TABLE
";
        echo "
            &lt;table border=1&gt;
            &lt;tr&gt;
                &lt;th&gt;cpu_busy&lt;/th&gt;
                &lt;th&gt;io_busy&lt;/th&gt;
                &lt;th&gt;idle&lt;/th&gt;
                &lt;th&gt;pack_received&lt;/th&gt;
                &lt;th&gt;pack_sent&lt;/th&gt;
                &lt;th&gt;connections&lt;/th&gt;
                &lt;th&gt;pack_errors&lt;/th&gt;
                &lt;th&gt;total_read&lt;/th&gt;
                &lt;th&gt;total_write&lt;/th&gt;
                &lt;th&gt;total_errors&lt;/th&gt;
             &lt;/tr&gt;";
    while ($row = sqlsrv_fetch_array($results))
    {
                $cpu_busy=$row[1];
                $io_busy=$row[2];
                $idle=$row[3];
                $pack_received=$row[4];
                $pack_sent=$row[5];
                $connections=$row[6];
                $pack_errors=$row[7];
                $total_read=$row[8];
                $total_write=$row[9];
                $total_errors=$row[10];
             
    echo "
            &lt;tr&gt;
                &lt;td&gt;$cpu_busy&lt;/td&gt;;
                &lt;td&gt;$io_busy&lt;/td&gt;;
                &lt;td&gt;$idle&lt;/td&gt;;
                &lt;td&gt;$pack_received&lt;/td&gt;;
                &lt;td&gt;$pack_sent&lt;/td&gt;;
                &lt;td&gt;$connections&lt;/td&gt;;
                &lt;td&gt;$pack_errors&lt;/td&gt;;
                &lt;td&gt;$total_read&lt;/td&gt;;
                &lt;td&gt;$total_write&lt;/td&gt;;
                &lt;td&gt;$total_errors&lt;/td&gt;;
            &lt;/tr&gt;";
    }
    echo "&lt;/table&gt;";
    ?&gt;</code></pre>
<p>OUTPUT:</p>
<pre><code>Connection established.

MSSQL master.dbo.spt_monitor TABLE

; ; ; ; ; ; ; ; ; ;

            

                
                    cpu_busy
                        io_busy
                        idle
                        pack_received
                        pack_sent
                        connections
                        pack_errors
                        total_read
                        total_write
                        total_errors
                

                
                    9
                        7
                        792
                        28
                        28
                        14
                        0
                        0
                        0
                        0
                </code></pre>
<ol start="4" type="1">
<li>Incorrect SSDPHP version:</li>
</ol>
<p>Problem: If you use php53/SSDPHP 3.0 code but use SSNC2008 instead of SSNC 2012, you get an error message.</p>
<p>Could not connect. Array ( [0] =&gt; Array ( [0] =&gt; IMSSP [SQLSTATE] =&gt; IMSSP [1] =&gt; -49 [code] =&gt; -49 [2] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: <a href="http://go.microsoft.com/fwlink/?LinkId=163712" class="uri">http://go.microsoft.com/fwlink/?LinkId=163712</a> [message] =&gt; This extension requires the Microsoft SQL Server 2012 Native Client. Access the following URL to download the Microsoft SQL Server 2012 Native Client ODBC driver for x86: <a href="http://go.microsoft.com/fwlink/?LinkId=163712" class="uri">http://go.microsoft.com/fwlink/?LinkId=163712</a> ) [1] =&gt; Array ( [0] =&gt; IM002 [SQLSTATE] =&gt; IM002 [1] =&gt; 0 [code] =&gt; 0 [2] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified [message] =&gt; [Microsoft][ODBC Driver Manager] Data source name not found and no default driver specified ) )</p>
<p>Solution: Install php52/SSDPHP 2.0 &amp; SSNC2008</p>
<p><strong>Summary:</strong></p>
<p>Although this project takes a lot of time to setup, troubleshoot &amp; tweak all the settings, the end result is a client/server setup one can experiment with. Once the database server is setup properly, various types of software can be configured to retrieve/store data. PHP can be used to create web applications for doing data analysis using MSSQL.</p>
<p>To access data from MSSQL using php52, <code>mssql_()</code> functions is used. But php53 onwards uses only <code>sqldrv_()</code> functions to access MSSQL. Hence it is better to learn <code>sqldrv_()</code> functions. However knowing how to use the older functions will help when dealing with legacy systems having php52.</p>



 ]]></description>
  <category>article</category>
  <category>datascience</category>
  <category>php</category>
  <category>mysql</category>
  <guid>https://fortunewalla.github.io/fw2/posts/08-projecttoimplementremotedb/</guid>
  <pubDate>Fri, 16 May 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Software choices to implement a remote database client/server network setup</title>
  <dc:creator>Fortune Walla</dc:creator>
  <link>https://fortunewalla.github.io/fw2/posts/07-softwarechoicesremotedb/</link>
  <description><![CDATA[ 




<p><em>Posted on Thursday, May 8th, 2014 at 2:25 am</em></p>
<p>Interacting with a database on a network from statistical/reporting software like MS Excel, MS Access, SAS, R, Python, etc… is a great way to learn how data might be retrieved, analysed &amp; stored remotely. Web scripting languages like ASPX, PHP also allow remote database interactions for data analysis using Web based applications.</p>
<p>However, a local database with local software using only individual local files pretty much provides the same experience as using them on a network. But the aim here is to simulate a corporate/research environment where all the software &amp; data resources are spread throughout the network. The aim is not to become an expert in the technologies but to know just enough to use them to do data science/analytics.</p>
<p>Hence the philosophy is to use minimum hardware/software resources to be able to study &amp; learn quickly and efficiently. The problem is that each person has a different configuration of client/server and hardware/software components &amp; usually one person’s solution will not necessarily work for the other. The choices available to you will be different from the choices presented here.</p>
<p><strong>Server decision process:</strong></p>
<ul>
<li><p>Server database: MS SQL Server (MSSQL) vs.&nbsp;Mysql Database (mysql)</p>
<p>Decision: MSSQL</p>
<p>Reason: MSSQL has built-in data mining through SQL Server Analysis Services (SSAS). Also MSSQL2008 offers more data mining options when compared to MSSQL2005. There is more information online about MSSQL data mining features than about mysql data mining features. Furthermore, MS Excel can perform data mining directly on spreadsheets using SSAS via an add-in</p></li>
<li><p>Server OS: Windows XP (winxp) vs.&nbsp;Windows Vista/7 (win7)</p></li>
</ul>
<pre><code>Decision: win7

Reason: win7 has similar features to Windows Server 2008 &amp; handles networking better than winxp. Winxp would be sufficient but one needs to install additional software components such as dotnet packages, VC++ run-time packages, deal with permissions/networking issues etc…</code></pre>
<p>Once you have the server &amp; software setup for networking &amp; remote access, almost any hardware/software combination can be used as a client.</p>
<p><strong>Client decision process:</strong></p>
<ul>
<li>Client OS: Windows vs.&nbsp;GNU/Linux</li>
</ul>
<p>Decision: Windows</p>
<p>Reason: MS Excel and MS Access are used widely in the business/research world. Hence Windows XP was chosen. Also MS Excel supports many add-ins for statistical analysis, data mining &amp; visualization. Although software such as R, Python, WEKA etc… would integrate better with GNU/Linux.</p>
<ul>
<li>Client web server: MS Internet Information Services (IIS) vs.&nbsp;Apache Web Server (apache).</li>
</ul>
<pre><code>Decision: IIS

Reason: A student of data science/analytics does not need advanced features of a web server. A basic version of IIS comes built-in with winxp. I am guessing it is enough for the purpose of learning. Apache is robust but requires more configuration than IIS.</code></pre>
<ul>
<li>Client web language: ASPX vs.&nbsp;PHP</li>
</ul>
<pre><code>Decision:PHP

Reason: PHP is easy to configure for IIS &amp; light on system resources. ASPX would be the preferred choice for use with IIS and MSSQL, but requires more configuration &amp; systems resources for Visual Studio 2010 development environment. Students can look at the PHPStats project https://github.com/mcordingley/PHPStats for using statistical functions to do data analysis on the web.</code></pre>
<ul>
<li>OS for Web Server: Client OS vs.&nbsp;Server OS</li>
</ul>
<pre><code>Decision: Client OS

Reason: Decision to install IIS/PHP on Client OS was primarily done for two reasons i) To isolate the database from any instability that might result from the Web application environment ii) To simulate an “Internet” where the database &amp; web server reside on different machines.

Misc. client software: MS Excel, MS Access, R, Python. It is advisable to try the database access features of many software for learning &amp; practice.</code></pre>
<p><strong>Summary:</strong></p>
<p>Final server setup: SQL Server 2008 on Windows 7.</p>
<p>Final client setup: PHP using IIS on Windows XP.</p>
<p>The main idea is to simulate a corporate/research data science/analytics environment where data &amp; software resources are spread throughout the network. The design choices are made to use minimum resources. This enables students to learn the basics efficiently without having to worry about the advanced features. The implementation of these choices will be discussed in another post.</p>



 ]]></description>
  <category>article</category>
  <category>datascience</category>
  <category>sas</category>
  <guid>https://fortunewalla.github.io/fw2/posts/07-softwarechoicesremotedb/</guid>
  <pubDate>Thu, 08 May 2014 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Referring to a SAS data set with its full filesystem path</title>
  <link>https://fortunewalla.github.io/fw2/posts/06-referringsasdatasetfullpath/</link>
  <description><![CDATA[ 




<p><em>Posted on Friday, April 4th, 2014 at 3:14 am</em></p>
<p>The standard way of referring to a SAS data set is through its library reference. i.e.&nbsp;<code>LIBREF.DATASETNAME</code>. However SAS also allows accessing the data set using the full filesystem path. Although not very useful, it does show the versatility that SAS allows the user. It can be used in situations where the data set needs to be accessed without defining a library name reference to access it.</p>
<pre><code>proc print data=sasuser.admit;
run;</code></pre>
<p>Using the full filesystem path we get:</p>
<pre><code>proc print data='C:\Documents and Settings\sasuser\My Documents\My SAS Files\9.1\admit.sas7bdat'; 
run;</code></pre>



 ]]></description>
  <category>article</category>
  <category>datascience</category>
  <category>sas</category>
  <guid>https://fortunewalla.github.io/fw2/posts/06-referringsasdatasetfullpath/</guid>
  <pubDate>Fri, 04 Apr 2014 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
